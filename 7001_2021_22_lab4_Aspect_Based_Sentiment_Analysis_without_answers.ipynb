{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hExKCzh6doIW"
   },
   "source": [
    "# Lab 4 - Aspect-Based Sentiment Analysis\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HixoFOoCIJ7V"
   },
   "source": [
    "In this session, we demonstrate how to deal with the aspect-based sentiment analysis (ABSA). You can find the whole task description from (https://aclanthology.org/D19-1654.pdf).\n",
    "This task provides a review text dataset with aspect.\n",
    "Given a review and an aspect, we need to classify the sentiment conveyed towards that aspect on a  three-point scale:   POSITIVE, NEUTRAL, and NEGATIVE.\n",
    "This is a multi-class classification task, and it needs to analyze the text and its aspect. \n",
    "\n",
    "Same as before, we are going to use Keras Sequential API in this session. The Sequential API allows you to make models layer-by-layer. You could modify the previous models to fit in the new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3828,
     "status": "ok",
     "timestamp": 1647368511595,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "m8fpBfhBpupy"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.layers import Lambda, GlobalAveragePooling1D, Dense, Embedding\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import LSTM, RNN, Dropout, Input, LeakyReLU, Bidirectional,Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqvPQvgvPv1W"
   },
   "source": [
    "### Downloading and preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EundMtGPpCdf"
   },
   "source": [
    "Unlike the IMDB dataset that is included and preprocessed by the Keras, the dataset we will be using is the aspect-term sentiment analysis (ATSA) dataset, which consists of 5297 labeled reviews. These are split into 4,297 reviews for training and 500 reviews for testing and validation, respectively. \n",
    "\n",
    "For ATSA, the annotators extract aspect terms in the sentences and label the sentiment polarities with respect to the  aspect  terms.   The  sentences  that  consist  of only one aspect term or multiple aspects with the same  sentiment  polarities  are  deleted.  ATSA also provides the start and end positions in a sentence for each aspect term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 667,
     "status": "ok",
     "timestamp": 1647368512260,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "P27sPFg7f0CJ"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def downloadfile(url):\n",
    "  rq = requests.get(url)\n",
    "  open(url.split('/')[-1], 'wb').write(rq.content)\n",
    "downloadfile('https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data/MAMS-ATSA/raw/train.xml')\n",
    "downloadfile('https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data/MAMS-ATSA/raw/val.xml')\n",
    "downloadfile('https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data/MAMS-ATSA/raw/test.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 649,
     "status": "ok",
     "timestamp": 1647368512907,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "X3Voc6S_gT2X",
    "outputId": "68e67a76-211a-4397-be74-9b3f7e775c60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries: 11186\n",
      "Test entries: 1336\n"
     ]
    }
   ],
   "source": [
    "# The code is modified from https://raw.githubusercontent.com/siat-nlp/MAMS-for-ABSA/master/data_process/utils.py\n",
    "from xml.etree.ElementTree import parse\n",
    "\n",
    "def parse_sentence_term(path, lowercase=False):\n",
    "    tree = parse(path)\n",
    "    sentences = tree.getroot()\n",
    "    data = []\n",
    "    split_char = '__split__'\n",
    "    for sentence in sentences:\n",
    "        text = sentence.find('text')\n",
    "        if text is None:\n",
    "            continue\n",
    "        text = text.text\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        aspectTerms = sentence.find('aspectTerms')\n",
    "        if aspectTerms is None:\n",
    "            continue\n",
    "        for aspectTerm in aspectTerms:\n",
    "            term = aspectTerm.get('term')\n",
    "            if lowercase:\n",
    "                term = term.lower()\n",
    "            polarity = aspectTerm.get('polarity')\n",
    "            start = aspectTerm.get('from')\n",
    "            end = aspectTerm.get('to')\n",
    "            piece = [text , term,  polarity , start , end]\n",
    "            data.append(piece)\n",
    "    return data\n",
    "train = parse_sentence_term(\"train.xml\",True)\n",
    "val = parse_sentence_term(\"val.xml\",True)\n",
    "test = parse_sentence_term(\"test.xml\",True)\n",
    "\n",
    "print(\"Training entries: {}\".format(len(train)))\n",
    "print(\"Test entries: {}\".format(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6U4iCV9-rmay"
   },
   "source": [
    "We now can start playing around with the data, letâ€™s first see some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1647368512908,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "h-gjWRAuqg5s",
    "outputId": "71338aa0-5505-42b8-d541-22b886aec2dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE \t ASPECT \t LABEL \t ASPECT-START-INDEX \t ASPECT-END-INDEX\n",
      "['the decor is not special at all but their food and amazing prices make up for it.', 'decor', 'negative', '4', '9']\n",
      "['the decor is not special at all but their food and amazing prices make up for it.', 'food', 'positive', '42', '46']\n",
      "['the decor is not special at all but their food and amazing prices make up for it.', 'prices', 'positive', '59', '65']\n",
      "['when tables opened up, the manager sat another party before us.', 'tables', 'neutral', '5', '11']\n",
      "['when tables opened up, the manager sat another party before us.', 'manager', 'negative', '27', '34']\n"
     ]
    }
   ],
   "source": [
    "print(\"SENTENCE \\t ASPECT \\t LABEL \\t ASPECT-START-INDEX \\t ASPECT-END-INDEX\")\n",
    "print(train[0])\n",
    "print(train[1])\n",
    "print(train[2])\n",
    "print(train[3])\n",
    "print(train[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTRZrpcyr-4x"
   },
   "source": [
    "We could use this dataset to try an \"unknown aspect\" task, if we assume that the ASPECT, LABEL and START/END-INDEX fields are what the model must predict. But here we will attempt a simpler \"known aspect\" task: we will assume that we know ASPECT and START/END-INDEX and the model must just predict the LABEL for a given combination of aspect and sentence.\n",
    "\n",
    "First, build a vocabulary based on the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 851,
     "status": "ok",
     "timestamp": 1647368513756,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "79Ev72Kgq4XL",
    "outputId": "9827f4b0-7ed8-4c14-b044-f0ead30c3ae9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7894\n",
      "7898\n",
      "{'<PAD>': 0, '<START>': 1, '<UNK>': 2, '<EOS>': 3, 'bleeding': 4, 'bubbling': 5, 'styrofoam': 6, 'chutney': 7, 'bottega': 8, 'saturday': 9, 'freshness': 10, 'mickey': 11, 'wedding': 12, 'lately': 13, 'bad': 14, 'frightful': 15, 'scintillating': 16, \"where's\": 17, 'porch': 18, 'drew': 19, 'coming': 20, 'steer': 21, 'accomodating': 22, 'lifts': 23, 'judge': 24, 'persian': 25, 'spider': 26, 'bustling': 27, 'gratinee': 28, 'several': 29, 'questioned': 30, 'impressed': 31, 'jo': 32, 'encroach': 33, 'entreee': 34, 'manager': 35, 'state': 36, 'demi': 37, 'stretched': 38, 'tandoor': 39, 'shallot': 40, 'nyc': 41, 'helpers': 42, 'untidy': 43, 'immense': 44, 'dropped': 45, 'clogged': 46, 'busboy': 47, 'throwing': 48, 'sleek': 49, 'luger': 50, \"we've\": 51, 'ingredients': 52, 'steps': 53, 'surprize': 54, 'rabbit': 55, 'volocety': 56, 'purity': 57, 'comprise': 58, \"'off\": 59, 'sceney': 60, 'heartfelt': 61, 'personally': 62, 'apologized': 63, 'opera': 64, 'deserve': 65, 'trencherman': 66, 'forty': 67, 'luxe': 68, 'ofcourse': 69, 'waitress': 70, 'feat': 71, 'gross': 72, 'equally': 73, 'kimow': 74, 'agonizingly': 75, 'martini': 76, 'boasts': 77, 'pommes': 78, 'soem': 79, 'hefty': 80, 'meager': 81, 'sampled': 82, 'flooring': 83, 'anchovy': 84, 'dissapointing': 85, 'dead': 86, 'total': 87, 'hardly': 88, 'shoved': 89, 'tv': 90, '02': 91, 'downfall': 92, \"we're\": 93, 'understood': 94, 'suspect': 95, 'tell': 96, 'grate': 97, 'added': 98, 'habit': 99, 'massive': 100, 'beverages': 101, 'decorated': 102, 'determining': 103, '22': 104, 'positive': 105, 'highlighted': 106, 'par': 107, 'although': 108, 'filthy': 109, 'very': 110, 'hide': 111, 'reached': 112, 'stretch': 113, 'porn': 114, 'taramasolata': 115, 'down': 116, \"o'\": 117, 'vongerichten': 118, 'plastic': 119, 'blue': 120, 'gimmick': 121, 'guzzling': 122, 'chalkboard': 123, 'director': 124, 'sambosas': 125, 'duty': 126, 'completed': 127, 'careful': 128, 'flexibility': 129, '70s': 130, 'horrific': 131, 'neapolitan': 132, 'actually': 133, 'inspired': 134, '28': 135, 'wife': 136, 'blunt': 137, 'caring': 138, 'blaring': 139, 'halves': 140, 'minute': 141, 'yoguart': 142, 'cakes': 143, 'buffett': 144, 'direction': 145, 'bite': 146, \"'em\": 147, 'ugh': 148, 'touch': 149, 'returned': 150, 'bucket': 151, 'april': 152, 'incident': 153, 'heads': 154, 'll': 155, 'trundle': 156, 'tunes': 157, 'outlandish': 158, '33': 159, 'simplicity': 160, 'chopsticks': 161, 'tostades': 162, 'changing': 163, 'downstair': 164, 'vindaloo': 165, 'habtchi': 166, 'throw': 167, 'insult': 168, 'brulee': 169, 'wrong': 170, 'bolognese': 171, 'tips': 172, 'mashed': 173, 'atmosphere': 174, 'crouton': 175, 'goods': 176, 'venture': 177, 'sweetner': 178, 'arrived': 179, 'ski': 180, 'softie': 181, 'weathered': 182, 'napolean': 183, 'liking': 184, 'recruiting': 185, 'assaulted': 186, 'scallop': 187, 'vintage': 188, 'accomodate': 189, 'cosmopolitan': 190, 'adults': 191, 'emphasis': 192, 'luke': 193, 'trees': 194, 'overwelming': 195, 'mon': 196, 'paratha': 197, 'section': 198, \"eatery's\": 199, 'faded': 200, 'sexy': 201, 'cured': 202, 'du': 203, 'olympic': 204, 'coolade': 205, 'early': 206, 'luring': 207, 'boast': 208, 'liver': 209, 'offend': 210, 'oddest': 211, 'humor': 212, 'sample': 213, 'space': 214, 'mother': 215, 'jambalaya': 216, 'bartenders': 217, \"'manager'\": 218, 'carpaccio': 219, 'polpettini': 220, 'quieter': 221, 'coma': 222, 'shrift': 223, 'winter': 224, 'kearney': 225, 'lied': 226, 'overlooked': 227, 'date': 228, 'accident': 229, 'nicest': 230, 'meaning': 231, 'absence': 232, 'residing': 233, 'brunch': 234, 'understandable': 235, 'bruises': 236, 'layer': 237, 'banania': 238, 'locations': 239, 'inform': 240, 'nowhere': 241, 'orange': 242, 'slab': 243, 'instance': 244, 'bath': 245, 'keep': 246, 'hunks': 247, 'female': 248, 'interestingly': 249, 'toro': 250, \"waitstaff's\": 251, 'parade': 252, 'jeollado': 253, 'versed': 254, 'reacted': 255, 'actual': 256, 'consider': 257, 'scoop': 258, 'stumbled': 259, 'frustrating': 260, 'separated': 261, 'm': 262, 'sci': 263, 'specials': 264, 'fails': 265, 'acrylic': 266, 'special': 267, 'dairy': 268, 'binge': 269, 'matriarchal': 270, 'coal': 271, 'eben': 272, 'snooby': 273, 'make': 274, 'feed': 275, 'pomodoro': 276, 'coupled': 277, 'morels': 278, 'tipping': 279, 'yesterday': 280, 'blanche': 281, 'focused': 282, 'thinner': 283, '39': 284, 'filipino': 285, 'boi': 286, \"crave's\": 287, 'comped': 288, 'un': 289, 'worn': 290, 'beforehand': 291, 'walled': 292, 'crisped': 293, 'building': 294, 'thn': 295, 'resembles': 296, 'mixed': 297, 'flame': 298, 'pricy': 299, 'burnt': 300, 'personalized': 301, 'halfway': 302, 'quibbling': 303, 'lot': 304, 'cards': 305, 'joke': 306, 'broken': 307, '68': 308, 'dramatic': 309, 'mezzes': 310, 'cuisine': 311, 'congratulated': 312, 'olds': 313, 'originality': 314, 'doubts': 315, 'patty': 316, \"boylan's\": 317, 'ended': 318, 'ok': 319, 'televisions': 320, 'mozz': 321, 'ditmars': 322, 'declaring': 323, 'witnessed': 324, 'goosey': 325, \"ta'im\": 326, 'tu': 327, '1hour': 328, 'blanb': 329, 'taste': 330, 'instead': 331, 'any': 332, 'tale': 333, 'emptyits': 334, \"maitre'd\": 335, 'feedback': 336, 'affect': 337, 'drama': 338, 'glanced': 339, 'york': 340, 'he': 341, 'fills': 342, 'collaboration': 343, 'dylan': 344, 'learn': 345, 'support': 346, 'reservation': 347, 'compnay': 348, 'lotus': 349, 'suede': 350, 'rich': 351, 'nbr': 352, 'wheel': 353, 'errors': 354, 'dessrt': 355, 'portrait': 356, 'potent': 357, 'even': 358, 'cared': 359, 'sparkling': 360, 'stuff': 361, 'margs': 362, 'understanding': 363, 'aperitif': 364, 'apetizers': 365, \"'the\": 366, 'glasswine': 367, 'trimmings': 368, 'bagel': 369, 'billed': 370, 'welcomes': 371, 'random': 372, 'litter': 373, 'map': 374, 'tha': 375, 'talents': 376, 'frog': 377, 'yukon': 378, 'compelling': 379, 'irish': 380, 'alcoholic': 381, 'homogeneous': 382, \"fabulous'\": 383, 'inconvenient': 384, 'achieving': 385, 'highs': 386, 'parties': 387, 'deafening': 388, 'musical': 389, 'walnuts': 390, 'monsieur': 391, 'suggestion': 392, 'energetic': 393, 'serve': 394, 'electric': 395, 'childhood': 396, 'badly': 397, 'exceptional': 398, 'brownstone': 399, 'dollars': 400, 'btw': 401, 'ceiling': 402, 'prematurely': 403, 'dory': 404, 'off': 405, 'observed': 406, 'focuses': 407, 'girlfriends': 408, 'nightclubs': 409, 'fan': 410, 'suggests': 411, 'specializes': 412, 'fusion': 413, \"table's\": 414, 'chips': 415, 'intrusive': 416, 'translation': 417, 'union': 418, 'exception': 419, 'subpar': 420, 'paneer': 421, 'beleive': 422, 'cancelled': 423, 'turbot': 424, 'meatball': 425, 'cabernet': 426, 'courses': 427, 'bridge': 428, 'exit': 429, 'schlag': 430, \"teddy's\": 431, 'loaf': 432, 'hi': 433, 'managment': 434, 'skylight': 435, 'young': 436, 'creams': 437, 'cruda': 438, 'recognized': 439, 'brie': 440, 'beets': 441, \"i've\": 442, 'turnover': 443, 'verbatim': 444, 'allows': 445, 'virgnin': 446, \"entree's\": 447, 'jello': 448, 'lump': 449, 'dancefloor': 450, 'pretty': 451, 'occurance': 452, 'definitly': 453, 'dictate': 454, 'banquette': 455, 'salon': 456, 'roll': 457, 'letter': 458, 'becasue': 459, 'sweet': 460, \"owners'\": 461, 'poorly': 462, 'supplement': 463, 'music': 464, 'curtly': 465, 'carciofi': 466, 'cranapple': 467, 'wintery': 468, 'thick': 469, \"c'mon\": 470, 'twists': 471, 'art': 472, 'omelette': 473, 'seasons': 474, 'chickens': 475, 'confections': 476, 'intervals': 477, 'sandwhiches': 478, 'effort': 479, 'fact': 480, 'western': 481, 'pre': 482, 'shrugged': 483, 'quater': 484, 'chefs': 485, '7pm': 486, 'accessible': 487, 'makhni': 488, 'diets': 489, 'knock': 490, 'red': 491, 'guess': 492, 'maniccoti': 493, \"pony's\": 494, 'griddle': 495, 'staurday': 496, 'damn': 497, 'prepares': 498, 'accents': 499, 'alessandro': 500, 'plate': 501, 'vinagrette': 502, 'seviche': 503, 'secure': 504, 'hated': 505, 'guy': 506, 'hai': 507, 'necessary': 508, 'overall': 509, 'gang': 510, 'houston': 511, \"one's\": 512, 'rolled': 513, 'consistantly': 514, 'nabe': 515, 'breath': 516, 'untill': 517, 'one': 518, 'need': 519, 'alternate': 520, 'long': 521, 'runniest': 522, 'veggies': 523, 'lucy': 524, 'perfect': 525, 'courteous': 526, 'misleading': 527, 'constructed': 528, 'mango': 529, 'teachers': 530, 'bridges': 531, 'royal': 532, 'store': 533, 'eat': 534, 'antipasto': 535, \"menu's\": 536, 'mugs': 537, 'freshest': 538, 'imagine': 539, 'proof': 540, 'otside': 541, 'valentines': 542, 'clips': 543, 'kudos': 544, 'pluck': 545, 'status': 546, \"shuoldn't\": 547, 'eating': 548, 'unhelpful': 549, 'greenwich': 550, 'gringos': 551, 'helpul': 552, 'miami': 553, 'sometimes': 554, 'cashier': 555, 'firees': 556, 'require': 557, 'crayons': 558, 'asthma': 559, 'counts': 560, 'japanese': 561, 'filter': 562, 'regrettably': 563, 'sign': 564, 'astonished': 565, 'reversed': 566, 'translated': 567, 'important': 568, 'spiffs': 569, 'warm': 570, 'bagels': 571, 'calories': 572, 'steel': 573, 'motherland': 574, 'involving': 575, 'bar': 576, 'upping': 577, 'cognac': 578, 'pricing': 579, 'abundantly': 580, 'outshine': 581, 'celebrity': 582, 'moradas': 583, 'crutches': 584, 'include': 585, 'chalked': 586, 'enormous': 587, 'hen': 588, 'fee': 589, 'adn': 590, 'menus': 591, 'disapointing': 592, 'screen': 593, 'grain': 594, 'presentation': 595, 'cigars': 596, 'positioned': 597, 'messes': 598, 'hooted': 599, 'beef': 600, 'saving': 601, 'crepe': 602, 'apricots': 603, 'comming': 604, 'trim': 605, 'characters': 606, 'incorrectly': 607, 'job': 608, 'asian': 609, 'problem': 610, 'kiwitini': 611, 'distrupted': 612, 'cacciatore': 613, 'lentil': 614, 'packs': 615, 'tacky': 616, 'lead': 617, 'picturesque': 618, 'carrot': 619, 'dixon': 620, 'fernando': 621, 'spotty': 622, 'clerck': 623, '130': 624, 'beans': 625, 'occasion': 626, 'rehearsal': 627, 'stroll': 628, 'colonies': 629, 'bustles': 630, 'mousse': 631, 'celebrites': 632, 'terrine': 633, 'puts': 634, 'passes': 635, 'whereas': 636, 'wiped': 637, 'ping': 638, 'foodthe': 639, 'cups': 640, 'p': 641, 'originally': 642, 'shuck': 643, 'brasserie': 644, 'watermelon': 645, 'noisily': 646, 'cloyingly': 647, 'kunz': 648, 'mural': 649, 'wobbly': 650, 'secretly': 651, 'milkshakes': 652, 'cola': 653, 'bulgogi': 654, 'expertise': 655, 'b41': 656, 'downstairs': 657, 'sweetened': 658, 'overcook': 659, 'brazilian': 660, 'please': 661, 'slowly': 662, 'glorified': 663, \"calf's\": 664, 'having': 665, 'canbe': 666, 'tepid': 667, 'tabbouleh': 668, \"restaurant's\": 669, 'profanities': 670, \"'80s\": 671, 'ingredient': 672, 'peel': 673, 'so': 674, 'establishments': 675, 'superfluously': 676, 'relally': 677, 'attain': 678, 'phenomonal': 679, 'exist': 680, 'salads': 681, 'ues': 682, 'hood': 683, 'devine': 684, 'fogged': 685, 'broght': 686, 'skills': 687, 'cake': 688, 'complete': 689, 'waterfall': 690, 'immaculate': 691, 'radius': 692, 'nudged': 693, 'someth': 694, 'required': 695, 'cheek': 696, 'macadamia': 697, 'provide': 698, 'hunk': 699, 'decadent': 700, 'avail': 701, 'coleslaw': 702, '00': 703, 'than': 704, 'sacrifice': 705, 'type': 706, 'greatest': 707, '15th': 708, 'room': 709, 'creating': 710, 'viet': 711, 'passion': 712, 'lives': 713, 'torellini': 714, '6pm': 715, 'atmostphere': 716, 'boiled': 717, 'explanation': 718, 'chocolaty': 719, 'touted': 720, 'pared': 721, 'omakase': 722, 'omikase': 723, 'miniblueberrrymuffins': 724, 'opened': 725, 'lucky': 726, \"irving's\": 727, 'wraps': 728, 'settle': 729, 'everytime': 730, 'advanced': 731, 'chimed': 732, 'doughnuts': 733, 'contain': 734, 'fun': 735, 'check': 736, 'tuesdays': 737, 'opinion': 738, 'agreeing': 739, 'busser': 740, 'listening': 741, '137': 742, 'doorstep': 743, 'really': 744, 'four': 745, 'possibly': 746, 'retaurant': 747, 'twenties': 748, 'bogarted': 749, 'adequately': 750, 'skewers': 751, 'tortellini': 752, 'pages': 753, 'granola': 754, 'shaped': 755, 'hanging': 756, 'sorrel': 757, 'accordingly': 758, 'festooned': 759, 'companion': 760, 'painted': 761, \"comp'ed\": 762, 'lighting': 763, 'tableclothes': 764, 'skilled': 765, 'descript': 766, 'simultaneously': 767, 'jerk': 768, 'talkative': 769, 'marginally': 770, 'remainder': 771, 'spiced': 772, 'earlier': 773, 'areos': 774, \"ol'\": 775, '500': 776, 'conversation': 777, 'cleaned': 778, 'converasation': 779, 'shoestring': 780, 'benches': 781, 'in': 782, 'supremely': 783, 'silvery': 784, 'bonehead': 785, 'gooey': 786, '23': 787, 'barely': 788, 'tiiiiiiiny': 789, 'satisfied': 790, 'shabby': 791, 'excellent': 792, 'needs': 793, \"shouldn't\": 794, 'nephew': 795, 'brings': 796, 'wait': 797, \"ehrlich's\": 798, 'agrees': 799, 'saffron': 800, 'complemented': 801, 'parm': 802, 'surprises': 803, 'inedible': 804, 'went': 805, \"springtime's\": 806, 'negetive': 807, 'exotic': 808, 'signs': 809, 'licking': 810, '2004': 811, 'luches': 812, 'yogurt': 813, 'tiled': 814, 'fruits': 815, 'likelemon': 816, 'stories': 817, 'lunch': 818, 'tumblers': 819, 'raspberry': 820, 'arches': 821, 'slowed': 822, 'sansachun': 823, 'singing': 824, 'potli': 825, 'moniker': 826, 'jumped': 827, 'welcoming': 828, '8pm': 829, 'automatic': 830, 'wearing': 831, 'scattershot': 832, 'ar': 833, 'cheddar': 834, 'takes': 835, 'feast': 836, 'canard': 837, 'distracted': 838, 'fabulous': 839, 'match': 840, 'declined': 841, 'horse': 842, 'overcome': 843, 'stare': 844, 'lax': 845, 'metro': 846, 'moroe': 847, 'st': 848, 'forced': 849, 'yam': 850, 'respond': 851, 'dim': 852, 'uninteresting': 853, 'exciting': 854, 'introduction': 855, 'c': 856, 'introduce': 857, 'being': 858, 'bronx': 859, 'term': 860, 'crusty': 861, 'gay': 862, 'newport': 863, 'baggato': 864, 'idea': 865, 'afterall': 866, 'soon': 867, 'southern': 868, 'daily': 869, 'sinful': 870, 'swimming': 871, 'seemingly': 872, 'themed': 873, 'soory': 874, 'palace': 875, 'swank': 876, 'refill': 877, 'tom': 878, 'right': 879, 'sourdough': 880, 'knowledge': 881, 'telltale': 882, 'frustrated': 883, 'crabmeat': 884, 'heck': 885, 'endive': 886, 'ghanouj': 887, \"rec'd\": 888, 'houses': 889, 'shops': 890, 'hired': 891, 'brilliant': 892, 'xo': 893, \"companion's\": 894, 'asia': 895, 'post': 896, 'diamond': 897, 'rapid': 898, 'reject': 899, 'fill': 900, 'somelier': 901, 'streaked': 902, 'cursed': 903, 'swear': 904, 'upbeat': 905, 'hats': 906, 'melt': 907, 'stew': 908, 'excelent': 909, 'discount': 910, '3times': 911, 'nouvo': 912, 'revival': 913, 'trivia': 914, 'scones': 915, 'half': 916, \"valentine's\": 917, 'improvements': 918, 'satisfying': 919, \"doesn't\": 920, 'havent': 921, 'bill': 922, 'calm': 923, 'soupy': 924, 'dancers': 925, 'bizarrely': 926, 'grouchy': 927, 'maintenance': 928, 'protocol': 929, 'patrons': 930, 'hustle': 931, 'scrawled': 932, 'pierogies': 933, 'spray': 934, 'pleasing': 935, 'recourse': 936, 'scores': 937, 'venison': 938, \"'put\": 939, 'express': 940, 'dill': 941, 'knives': 942, 'spread': 943, 'accent': 944, 'hoot': 945, 'mole': 946, 'decoration': 947, 'pleasant': 948, 'rank': 949, 'messy': 950, 'million': 951, 'genuinely': 952, 'bottom': 953, 'rava': 954, 'knows': 955, 'hit': 956, \"harry's\": 957, 'insincere': 958, 'alfama': 959, 'points': 960, 'upheld': 961, 'soup': 962, '4th': 963, 'lollies': 964, 'lacks': 965, 'riesling': 966, 'bay': 967, 'brigade': 968, 'recommend': 969, 'easter': 970, 'got': 971, 'pleasure': 972, 'volumous': 973, 'press': 974, 'curd': 975, 'extreme': 976, 'plump': 977, 'langostine': 978, 'uses': 979, 'spans': 980, 'uninformed': 981, 'believes': 982, 'doughy': 983, 'knowledgeable': 984, 'herself': 985, 'informing': 986, 'overseeing': 987, 'spilled': 988, 'lampshades': 989, 'hugely': 990, 'restuarant': 991, 'del': 992, 'weather': 993, 'polite': 994, 'mercy': 995, 'scribbing': 996, 'outer': 997, 'roast': 998, 'licious': 999, 'la': 1000, 'flay': 1001, 'visual': 1002, 'bruschetta': 1003, 'beg': 1004, 'thinks': 1005, 'asks': 1006, 'go': 1007, 'frite': 1008, 'foodies': 1009, 'succinct': 1010, 'tower': 1011, 'charm': 1012, 'threw': 1013, 'arragance': 1014, 'atomsphere': 1015, 'slope': 1016, 'trippy': 1017, 'sampling': 1018, 'madras': 1019, 'figs': 1020, 'lent': 1021, 'lackluster': 1022, 'recherche': 1023, 'complements': 1024, 'scallion': 1025, 'decided': 1026, 'itsuperb': 1027, 'chatter': 1028, 'mozarella': 1029, 'for': 1030, 'definite': 1031, 'chatted': 1032, 'causing': 1033, 'book': 1034, 'tremendous': 1035, 'mussels': 1036, 'may': 1037, 'asking': 1038, \"mars'\": 1039, 'imitation': 1040, 'narrow': 1041, 'new': 1042, \"dog's\": 1043, \"schiller's\": 1044, 'shochu': 1045, 'interminable': 1046, '07': 1047, 'messed': 1048, 'definatly': 1049, 'angolo': 1050, 'expecially': 1051, 'smallish': 1052, 'visit': 1053, 'formality': 1054, 'pint': 1055, 'same': 1056, \"we'd\": 1057, 'crumbled': 1058, 'venezuelan': 1059, 'livers': 1060, 'suit': 1061, 'meats': 1062, 'enquire': 1063, 'astounding': 1064, 'fifteen': 1065, 'poorest': 1066, 'woods': 1067, 'gap': 1068, 'painting': 1069, 'trattoria': 1070, 'radish': 1071, 'pricier': 1072, 'maki': 1073, 'shout': 1074, 'spreads': 1075, 'mini': 1076, 'lamb': 1077, 'loungier': 1078, 'buddha': 1079, 'ply': 1080, 'sans': 1081, 'offer': 1082, '14': 1083, 'greeting': 1084, 'translate': 1085, 'brunches': 1086, 'companionship': 1087, 'maple': 1088, 'deviled': 1089, 'shellfish': 1090, 'kindly': 1091, 'reasonable': 1092, 'brussels': 1093, 'everything': 1094, 'chianti': 1095, 'helping': 1096, 'clay': 1097, '105': 1098, 'since': 1099, 'restauarnt': 1100, 'claims': 1101, 'duck': 1102, 'fellow': 1103, 'entered': 1104, 'latkes': 1105, 'jungle': 1106, 'sumptous': 1107, 'zen': 1108, 'apathetic': 1109, 'slots': 1110, 'find': 1111, 'trust': 1112, 'scale': 1113, 'juice': 1114, 'tells': 1115, 'barked': 1116, 'critical': 1117, 'handsome': 1118, \"glass'\": 1119, 'unpretentious': 1120, 'spicy': 1121, 'plainly': 1122, 'eric': 1123, 'raves': 1124, 'elaborate': 1125, 'n': 1126, 'fare': 1127, 'greens': 1128, 'line': 1129, 'robata': 1130, 'hinges': 1131, 'taxtip': 1132, 'snow': 1133, 'yuppies': 1134, 'kept': 1135, 'aloo': 1136, 'renovated': 1137, 'whip': 1138, 'smoke': 1139, 'touting': 1140, 'figureheads': 1141, 'kids': 1142, \"po'\": 1143, 'hawaiian': 1144, 'gave': 1145, 'dean': 1146, 'mouthwatering': 1147, 'speak': 1148, 'din': 1149, 'air': 1150, 'egg': 1151, 'brazil': 1152, \"valenti's\": 1153, 'fat': 1154, 'puercos': 1155, 'clue': 1156, 'buffet': 1157, 'garlic': 1158, 'allowing': 1159, 'rep': 1160, 'luckly': 1161, 'fridge': 1162, 'brightly': 1163, '12': 1164, 'tone': 1165, 'lan': 1166, 'american': 1167, 'quesadilla': 1168, 'wd': 1169, 'six': 1170, 'printed': 1171, 'unbeknownst': 1172, 'greater': 1173, 'spend': 1174, 'evokes': 1175, 'anyway': 1176, 'simply': 1177, 'serves': 1178, 'guestlisted': 1179, 'arrive': 1180, 'behind': 1181, 'sports': 1182, 'hovering': 1183, 'noodle': 1184, '15': 1185, 'unwanted': 1186, 'farthest': 1187, 'pistachios': 1188, 'deliverymen': 1189, 'rows': 1190, 'tsty': 1191, 'excruciatingly': 1192, 'prize': 1193, 'jitters': 1194, 'plum': 1195, 'springy': 1196, 'shad': 1197, 'tarte': 1198, 'empanadas': 1199, 'sub': 1200, 'treasure': 1201, 'fault': 1202, 'minion': 1203, 'donuts': 1204, 'goong': 1205, 'peppery': 1206, 'splattered': 1207, 'fuss': 1208, \"ploughman's\": 1209, 'granite': 1210, 'compensate': 1211, 'flower': 1212, 'european': 1213, 'fattier': 1214, 'proved': 1215, 'recommended': 1216, 'impossibly': 1217, 'ting': 1218, 'ben': 1219, 'pepperoni': 1220, 'smoky': 1221, 'tahini': 1222, '15pm': 1223, 'seats': 1224, 'bygone': 1225, 'admitting': 1226, 'greenpoint': 1227, 'mayonaisse': 1228, 'chance': 1229, 'meat': 1230, 'speedily': 1231, 'fattouch': 1232, 'nobu': 1233, 'mozzerella': 1234, 'experience': 1235, 'breathe': 1236, 'sawdust': 1237, 'coats': 1238, 'month': 1239, 'deciding': 1240, 'developed': 1241, 'results': 1242, '2002': 1243, 'towels': 1244, 'dine': 1245, 'lag': 1246, 'intro': 1247, 'firelight': 1248, 'prefer': 1249, 'circle': 1250, 'exhileratingly': 1251, 'fishes': 1252, 'inexplicably': 1253, 'coupon': 1254, 'side': 1255, 'loaded': 1256, 'skillets': 1257, 'head': 1258, 'beach': 1259, 'members': 1260, 'trendy': 1261, 'entrees': 1262, 'delicacies': 1263, 'house': 1264, 'seemed': 1265, 'bird': 1266, 'biscuit': 1267, 'father': 1268, 'describe': 1269, \"family's\": 1270, 'igloo': 1271, 'tribute': 1272, 'bombay': 1273, 'thrity': 1274, 'aplogized': 1275, 'loooooong': 1276, 'nop': 1277, 'jokes': 1278, 'pretention': 1279, 'an': 1280, 'against': 1281, 'juicy': 1282, 'slipped': 1283, 'retake': 1284, 'nao': 1285, 'topped': 1286, 'weekday': 1287, 'revelations': 1288, 'leche': 1289, 'belgian': 1290, 'tolerable': 1291, 'flavors': 1292, 'crucial': 1293, 'facts': 1294, 'sichuan': 1295, 'diff': 1296, 'types': 1297, 'mitzvas': 1298, 'automatically': 1299, \"father's\": 1300, 'gold': 1301, 'adamant': 1302, \"drinkin'\": 1303, 'stops': 1304, 'savvy': 1305, 'steamboat': 1306, 'penny': 1307, 'roaster': 1308, 'accompanied': 1309, 'appreciated': 1310, 'chix': 1311, 'heaping': 1312, 'wowing': 1313, 'catalana': 1314, 'questioning': 1315, 'remain': 1316, 'assortment': 1317, 'outposts': 1318, 'soiled': 1319, 'whisk': 1320, 'apart': 1321, 'last': 1322, 'bohemian': 1323, 'snail': 1324, 'cassata': 1325, 'removed': 1326, 'tira': 1327, 'tight': 1328, 'names': 1329, 'paa': 1330, \"'97\": 1331, 'aside': 1332, 'certain': 1333, 'alcohol': 1334, 'discounting': 1335, 'wash': 1336, 'boot': 1337, 'mention': 1338, 'taxed': 1339, 'mariachi': 1340, 'killer': 1341, 'lip': 1342, 'serving': 1343, 'constant': 1344, 'bring': 1345, 'stale': 1346, 'plane': 1347, 'school': 1348, '1940s': 1349, 'pistacchio': 1350, 'respectful': 1351, 'queens': 1352, 'anyone': 1353, 'cheesecake': 1354, 'nonetheless': 1355, 'speakers': 1356, 'recent': 1357, 'humus': 1358, 'lasagna': 1359, 'uniformly': 1360, 'kid': 1361, 'ways': 1362, 'concerns': 1363, 'raisin': 1364, 'fromaround': 1365, 'called': 1366, 'succeeds': 1367, '25min': 1368, 'three': 1369, 'overcompensate': 1370, 'chancho': 1371, 'daring': 1372, 'astoria': 1373, 'tres': 1374, 'flavorful': 1375, 'phony': 1376, 'teeny': 1377, 'incomplete': 1378, 'sinuses': 1379, 'maitre': 1380, 'mis': 1381, 'trek': 1382, 'patties': 1383, 'nastier': 1384, 'notified': 1385, 'cheezy': 1386, 'switch': 1387, 'machine': 1388, 'stays': 1389, 'finest': 1390, 'chill': 1391, 'magnificent': 1392, 'damper': 1393, 'apizz': 1394, 'caters': 1395, 'disgracefully': 1396, 'refilling': 1397, 'unoccupied': 1398, 'explains': 1399, 'bears': 1400, 'sharp': 1401, 'sapphire': 1402, 'melding': 1403, 'easier': 1404, 'inferior': 1405, 'country': 1406, '35pp': 1407, 'afterwards': 1408, 'merengue': 1409, 'hostess': 1410, 'values': 1411, 'minimum': 1412, 'speaker': 1413, 'reserved': 1414, 'exotica': 1415, 'saks': 1416, 'breathing': 1417, 'miss': 1418, 'liitle': 1419, 'furthermore': 1420, 'arroganrt': 1421, 'heady': 1422, 'shouting': 1423, 'hustled': 1424, 'east': 1425, 'pony': 1426, 'buco': 1427, 'reappearing': 1428, 'overfilled': 1429, 'hurt': 1430, 'nights': 1431, 'loungy': 1432, 'fi': 1433, 'lean': 1434, 'expletive': 1435, 'ferreting': 1436, 'fastly': 1437, \"d'oevre\": 1438, 'hill': 1439, 'musice': 1440, 'spring': 1441, 'merely': 1442, 'addressing': 1443, 'cyrillic': 1444, 'faves': 1445, 'doubt': 1446, 'diverse': 1447, 'frankie': 1448, 'how': 1449, 'clean': 1450, 'caramelized': 1451, 'gingseng': 1452, 'joy': 1453, 'beignets': 1454, 'additional': 1455, 'hard': 1456, 'arousing': 1457, 'reshaking': 1458, 'wherebv': 1459, 'pregnant': 1460, 'slam': 1461, 'steaks': 1462, 'draw': 1463, 'bulky': 1464, 'il': 1465, 'retro': 1466, 'opulent': 1467, 'babe': 1468, 'hipster': 1469, 'vacuum': 1470, 'mesquite': 1471, 'ultimate': 1472, 'jersey': 1473, 'coke': 1474, 'sweat': 1475, 'average': 1476, 'steepest': 1477, 'riedel': 1478, 'yama': 1479, 'chocolate': 1480, 'blackboard': 1481, 'crabcakes': 1482, 'employees': 1483, \"'absolutely\": 1484, 'zeppoles': 1485, 'punch': 1486, 'curtain': 1487, 'lady': 1488, 'excessive': 1489, 'devi': 1490, 'sink': 1491, 'fried': 1492, 'da': 1493, 'yorkshires': 1494, 'inch': 1495, 'baku': 1496, 'rhubarb': 1497, 'friench': 1498, 'starters': 1499, 'kimchi': 1500, 'preserve': 1501, 'point': 1502, 'steaming': 1503, 'scorned': 1504, 'ducking': 1505, 'forking': 1506, \"hubby's\": 1507, 'delectably': 1508, 'standards': 1509, 'beet': 1510, 'feels': 1511, 'damned': 1512, 'uninterested': 1513, 'minimal': 1514, 'sips': 1515, 'cabin': 1516, 'lone': 1517, 'anything': 1518, 'orderes': 1519, 'handcut': 1520, 'teaspoon': 1521, 'thailand': 1522, 'erk': 1523, 'swapping': 1524, 'consist': 1525, 'visitors': 1526, 'pistachio': 1527, \"diner's\": 1528, 'unusual': 1529, 'stinky': 1530, 'fluent': 1531, 'pizzaria': 1532, 'mmmm': 1533, 'city': 1534, 'blown': 1535, 'readily': 1536, 'sweaty': 1537, '8ish': 1538, 'ensuing': 1539, 'superfluous': 1540, 'purpose': 1541, 'tarts': 1542, 'if': 1543, 'identical': 1544, 'posters': 1545, 'appetites': 1546, 'brule': 1547, 'unapologetic': 1548, 'salted': 1549, 'means': 1550, 'fraiche': 1551, 'requests': 1552, 'comply': 1553, 'textures': 1554, 'carnitas': 1555, 'overstuffed': 1556, 'classic': 1557, 'collection': 1558, 'forget': 1559, 'obnoxious': 1560, 'best': 1561, 'alarmingly': 1562, 'large': 1563, 'sea': 1564, 'cuz': 1565, 'busier': 1566, 'disappointment': 1567, 'absentminded': 1568, 'screaming': 1569, 'fingers': 1570, 'rubbery': 1571, 'peanuts': 1572, 'jalipeno': 1573, 'doused': 1574, \"peoples'\": 1575, 'sin': 1576, 'brusselsprouts': 1577, 'district': 1578, 'nis': 1579, 'parmigiana': 1580, 'floor': 1581, 'uncle': 1582, 'harry': 1583, 'metallica': 1584, 'per': 1585, 'chorizo': 1586, 'opted': 1587, 'ballroom': 1588, 'clearing': 1589, 'linguine': 1590, 'moderate': 1591, 'upstairs': 1592, \"food'\": 1593, 'car': 1594, 'trays': 1595, 'proxmitity': 1596, 'without': 1597, 'sq': 1598, 'client': 1599, 'pen': 1600, 'straight': 1601, 'inches': 1602, 'tastier': 1603, 'subtle': 1604, 'intimidate': 1605, \"you'd\": 1606, 'enough': 1607, 'sure': 1608, 'abruptly': 1609, 'max': 1610, 'overpower': 1611, 'mediorce': 1612, 'creative': 1613, 'flaw': 1614, 'smile': 1615, 'colored': 1616, 'alleviate': 1617, 'previous': 1618, 'enlivened': 1619, 'dish': 1620, \"d's\": 1621, 'compromise': 1622, 'intimidating': 1623, 'gallante': 1624, 'emphasizes': 1625, 'pairs': 1626, 'barbecued': 1627, 'have': 1628, 'banquettes': 1629, 'understaffing': 1630, 'anyhting': 1631, 'living': 1632, 'fireplace': 1633, 'bedford': 1634, 'rescue': 1635, \"else's\": 1636, 'factors': 1637, 'decorative': 1638, 'lounge': 1639, 'rizzo': 1640, 'gossipping': 1641, 'north': 1642, 'bisquits': 1643, 'alter': 1644, 'tandoori': 1645, 'eaten': 1646, 'concern': 1647, '19': 1648, 'whick': 1649, 'portabello': 1650, 'corona': 1651, 'parish': 1652, 'allergies': 1653, 'would': 1654, 'caviar': 1655, \"kid's\": 1656, 'clutching': 1657, 'environment': 1658, 'artichoke': 1659, \"jeffery's\": 1660, 'year': 1661, 'locker': 1662, 'stool': 1663, 'letting': 1664, 'sicilian': 1665, 'marinera': 1666, 'intricately': 1667, 'fix': 1668, 'general': 1669, 'thais': 1670, 'impose': 1671, 'glazed': 1672, 'specified': 1673, 'manage': 1674, 'could': 1675, 'bottino': 1676, 'trip': 1677, 'depending': 1678, 'steamed': 1679, 'forgot': 1680, 'regaled': 1681, 'including': 1682, 'almost': 1683, 'begging': 1684, 'cater': 1685, 'quail': 1686, 'serverd': 1687, 'brooklyn': 1688, 'nori': 1689, 'coated': 1690, 'self': 1691, 'stronger': 1692, 'calves': 1693, 'ruined': 1694, 'swarming': 1695, 'straining': 1696, 'expirience': 1697, 'bub': 1698, 'morsels': 1699, 'skate': 1700, 'postive': 1701, 'beurre': 1702, 'obscene': 1703, 'muffins': 1704, 'tickets': 1705, '125pp': 1706, 'homefried': 1707, 'thereof': 1708, 'pudding': 1709, 'free': 1710, 'spinich': 1711, 'vip': 1712, 'squab': 1713, 'describing': 1714, 'showstoppers': 1715, 'outing': 1716, 'main': 1717, 'misu': 1718, 'ever': 1719, 'quacamole': 1720, 'affordable': 1721, 'expect': 1722, \"wouldn't\": 1723, 'bit': 1724, 'won': 1725, 'ubiquitious': 1726, 'shlog': 1727, 'making': 1728, 'scrumptious': 1729, 'cracker': 1730, 'sunken': 1731, 'evey': 1732, 'personality': 1733, 'anthipathy': 1734, 'impress': 1735, 'drool': 1736, 'nibble': 1737, 'raz': 1738, 'aggressing': 1739, 'flatbreads': 1740, 'scared': 1741, 'suffered': 1742, 'john': 1743, 'telephono': 1744, \"sarge's\": 1745, 'inordinate': 1746, 'stripe': 1747, 'cheeses': 1748, 'california': 1749, '2000': 1750, 'iutside': 1751, 'elevated': 1752, 'clubs': 1753, 'sucked': 1754, 'cheesy': 1755, 'detiorated': 1756, 'tartar': 1757, 'unhealthy': 1758, 'servers': 1759, 'complaining': 1760, '1950s': 1761, 'sahbu': 1762, 'varied': 1763, 'argentine': 1764, 'eel': 1765, 'below': 1766, 'cotes': 1767, 'leches': 1768, 'underwhelmed': 1769, 'counterpoint': 1770, \"'meduim\": 1771, 'town': 1772, 'together': 1773, 'chef': 1774, 'shamelessly': 1775, '30': 1776, \"grandfather's\": 1777, 'friendly': 1778, 'smiles': 1779, 'nothing': 1780, 'materialized': 1781, 'americain': 1782, 'bouncer': 1783, 'smoothie': 1784, 'cultures': 1785, 'mistakenly': 1786, '05': 1787, 'burning': 1788, 'globes': 1789, 'eats': 1790, 'confused': 1791, 'sweater': 1792, 'plentiful': 1793, 'jazz': 1794, 'pear': 1795, 'meatzza': 1796, \"'hits'\": 1797, 'scented': 1798, 'background': 1799, 'periods': 1800, 'minutes': 1801, 'poured': 1802, 'distinctive': 1803, 'guided': 1804, 'bonito': 1805, 'kalbi': 1806, 'gnocchi': 1807, 'sooo': 1808, 'anywhere': 1809, 'chick': 1810, 'entrance': 1811, 'letdown': 1812, 'partnered': 1813, 'extend': 1814, 'today': 1815, 'raver': 1816, 'concept': 1817, 'tomatoes': 1818, 'hillarious': 1819, 'owners': 1820, 'stella': 1821, 'mint': 1822, 'casual': 1823, 'chilled': 1824, 'soul': 1825, 'corridor': 1826, 'fed': 1827, 'returning': 1828, 'petite': 1829, '36': 1830, 'executed': 1831, 'char': 1832, 'joked': 1833, 'famine': 1834, 'brocolli': 1835, \"hour's\": 1836, 'snap': 1837, 'reservations': 1838, 'sampler': 1839, '21': 1840, 'carafe': 1841, \"city's\": 1842, 'thatched': 1843, 'growing': 1844, 'night': 1845, 'incorporate': 1846, 'architect': 1847, 'stemming': 1848, 'app': 1849, 'quiet': 1850, 'turning': 1851, 'view': 1852, 'manner': 1853, 'critic': 1854, 'melts': 1855, 'like': 1856, 'chapathi': 1857, 'tri': 1858, 'pricey': 1859, 'appealing': 1860, 'swiss': 1861, 'gummy': 1862, 'amateurs': 1863, \"he's\": 1864, \"food's\": 1865, \"romy's\": 1866, 'progressively': 1867, 'adding': 1868, 'pointed': 1869, 'spoon': 1870, 'caravan': 1871, 'adjustments': 1872, 'earthy': 1873, 'unimpressive': 1874, 'superiority': 1875, 'min': 1876, 'pancake': 1877, 'regarding': 1878, 'phenomenol': 1879, 'shooters': 1880, 'money': 1881, \"elvie's\": 1882, 'deli': 1883, 'fajitahs': 1884, 'truffles': 1885, 'bright': 1886, 'oranges': 1887, 'v8': 1888, 'steamy': 1889, 'tinga': 1890, \"artisanal's\": 1891, 'broth': 1892, 'brownie': 1893, 'reflects': 1894, 'chops': 1895, 'gone': 1896, 'mountain': 1897, 'mariano': 1898, 'transition': 1899, 'thai': 1900, 'iced': 1901, 'unrecognizable': 1902, 'ziti': 1903, 'middle': 1904, 'altogether': 1905, 'perfection': 1906, 'stoke': 1907, 'weird': 1908, '30minutes': 1909, 'le': 1910, 'siberia': 1911, 'artificial': 1912, \"john's\": 1913, 'repoured': 1914, 'lo': 1915, 'saketini': 1916, 'listed': 1917, 'condiment': 1918, 'no': 1919, 'zodiac': 1920, 'erase': 1921, 'myers': 1922, 'vegetables': 1923, 'bringing': 1924, 'mix': 1925, 'forward': 1926, 'crackers': 1927, 'issue': 1928, 'spice': 1929, 'naps': 1930, 'slight': 1931, 'overconceived': 1932, 'sold': 1933, 'private': 1934, 'rock': 1935, 'impeccable': 1936, 'solid': 1937, 'inviting': 1938, 'skip': 1939, 'mysteriously': 1940, 'adorns': 1941, 'talked': 1942, 'upsold': 1943, 'industrial': 1944, 'dimly': 1945, 'buddies': 1946, 'farm': 1947, 'peter': 1948, 'pop': 1949, 'fireplaces': 1950, '1hr': 1951, 'foot': 1952, 'seedy': 1953, 'keer': 1954, 'vacation': 1955, 'plantain': 1956, 'replace': 1957, 'distinguish': 1958, 'spacious': 1959, 'spacey': 1960, 'dosa': 1961, 'considerable': 1962, 'blt': 1963, \"what's\": 1964, 'flavored': 1965, 'unfortunate': 1966, \"zagat's\": 1967, 'chet': 1968, 'salty': 1969, 'customary': 1970, 'smores': 1971, 'prepped': 1972, 'increasingly': 1973, '45': 1974, \"host's\": 1975, 'bean': 1976, 'tatare': 1977, 'savory': 1978, 'curries': 1979, 'limited': 1980, 'steep': 1981, 'deak': 1982, '24': 1983, 'hoegardden': 1984, 'bountiful': 1985, 'scott': 1986, 'gather': 1987, 'definately': 1988, 'starburst': 1989, 'pubby': 1990, 'combination': 1991, 'grabbed': 1992, 'silly': 1993, 'rough': 1994, 'hunt': 1995, 'prosciutto': 1996, 'ky': 1997, 'farmer': 1998, 'b': 1999, 'stormed': 2000, 'hautest': 2001, 'fool': 2002, 'undoubtedly': 2003, 'sear': 2004, 'taken': 2005, 'infinitely': 2006, 'hottest': 2007, 'plethora': 2008, '330': 2009, \"can't\": 2010, 'familar': 2011, 'roommate': 2012, 'content': 2013, 'chat': 2014, 'warmly': 2015, 'novices': 2016, 'summer': 2017, 'changes': 2018, 'typical': 2019, 'upright': 2020, 'liners': 2021, 'brighter': 2022, 'delivered': 2023, 'containing': 2024, 'filet': 2025, 'sensational': 2026, 'expensive': 2027, 'server': 2028, 'goodness': 2029, 'merguez': 2030, 'mechanical': 2031, 'eztra': 2032, 'purposes': 2033, \"d'\": 2034, 'ruins': 2035, \"wasn't\": 2036, 'naughtily': 2037, 'talented': 2038, 'seasoned': 2039, \"'parisian'\": 2040, 'confounded': 2041, 'considerate': 2042, 'facade': 2043, 'tenerloin': 2044, 'cloth': 2045, 'lofty': 2046, 'grub': 2047, 'skelletons': 2048, 'rosas': 2049, 'slammed': 2050, 'rarely': 2051, 'undecorated': 2052, 'codes': 2053, 'notice': 2054, 'trimmed': 2055, 'hiding': 2056, 'mirrors': 2057, 'laughed': 2058, 'usually': 2059, 'disdainful': 2060, 'indication': 2061, 'buffalo': 2062, 'beverage': 2063, 'months': 2064, 'stressed': 2065, 'concoctions': 2066, 'pool': 2067, 'tax': 2068, 'hosts': 2069, 'delivery': 2070, 'ricotta': 2071, 'okonomiyaki': 2072, 'fixe': 2073, 'unworth': 2074, 'r': 2075, 'disappearing': 2076, 'safe': 2077, 'deep': 2078, 'known': 2079, 'newyork': 2080, 'mansion': 2081, 'drip': 2082, 'given': 2083, 'straw': 2084, 'cheaper': 2085, 'basket': 2086, 'nourishment': 2087, 'unmistakable': 2088, 'unlike': 2089, 'faith': 2090, \"couldnt'\": 2091, 'polished': 2092, 'avoid': 2093, 'fashioned': 2094, \"staffs'\": 2095, 'pecan': 2096, 'sushiden': 2097, 'neglected': 2098, 'te': 2099, 'wallpapered': 2100, 'generous': 2101, 'tirade': 2102, 'appropriate': 2103, 'extremele': 2104, 'instad': 2105, 'rack': 2106, \"rice's\": 2107, 'reasonably': 2108, 'wny': 2109, 'pakoras': 2110, 'coriander': 2111, 'sheet': 2112, 'menu': 2113, 'fattiest': 2114, 'drive': 2115, 'cheese': 2116, 'textbook': 2117, 'misplaced': 2118, \"'tango\": 2119, 'james': 2120, 'facing': 2121, 'tues': 2122, 'champaigne': 2123, 'ranges': 2124, 'worse': 2125, 'cons': 2126, 'profession': 2127, 'hoisin': 2128, 'problems': 2129, 'spaghettini': 2130, 'sets': 2131, 'freebies': 2132, 'responded': 2133, 'reemerge': 2134, 'heavy': 2135, 'staple': 2136, 'spare': 2137, 'justin': 2138, '6': 2139, 'ornate': 2140, 'smiling': 2141, 'goat': 2142, 'cantonese': 2143, 'landmarks': 2144, 'potatoes': 2145, 'hallway': 2146, 'subway': 2147, 'paninis': 2148, 'dozen': 2149, 'everyday': 2150, 'torches': 2151, 'distance': 2152, 'calculator': 2153, 'bizarre': 2154, 'lest': 2155, 'refined': 2156, 'tucked': 2157, 'lugers': 2158, 'subtly': 2159, 'tap': 2160, 'publications': 2161, 'coffees': 2162, 'attendant': 2163, 'immigrants': 2164, 'suspects': 2165, 'cool': 2166, 'preparing': 2167, 'location': 2168, 'agressive': 2169, 'artistes': 2170, 'assorted': 2171, 'round': 2172, 'porcelain': 2173, \"quartino's\": 2174, 'vibe': 2175, 'atlantic': 2176, 'included': 2177, 'sogginess': 2178, 'versace': 2179, 'substitution': 2180, 'been': 2181, 'wild': 2182, 'drinks': 2183, 'souvenirs': 2184, 'burst': 2185, 'talentless': 2186, 'pints': 2187, 'resonable': 2188, 'licorice': 2189, 'hours': 2190, 'livelier': 2191, 'came': 2192, 'scrape': 2193, 'crowed': 2194, 'memory': 2195, 'anna': 2196, 'partying': 2197, 'affair': 2198, 'chris': 2199, 'deliberately': 2200, 'overlooking': 2201, 'ourself': 2202, 'slant': 2203, 'chheesy': 2204, 'ridiculously': 2205, 'tarragon': 2206, 'encompass': 2207, 'sherry': 2208, 'aztec': 2209, 'south': 2210, 'beginning': 2211, 'v': 2212, 'avg': 2213, 'sustaining': 2214, 'appetizes': 2215, 'ponzu': 2216, 'cappuccino': 2217, 'spears': 2218, '00am': 2219, 'molases': 2220, 'comment': 2221, 'gallery': 2222, 'overshadowed': 2223, 'sister': 2224, 'autrocious': 2225, \"farmers'\": 2226, 'carry': 2227, 'noodles': 2228, 'aspecting': 2229, 'brown': 2230, 'first': 2231, 'haughty': 2232, 'treasured': 2233, 'seemly': 2234, 'babies': 2235, 'dogs': 2236, 'galley': 2237, 'desert': 2238, 'description': 2239, 'supplied': 2240, 'beautifully': 2241, 'sole': 2242, 'unhappy': 2243, 'orders': 2244, 'areas': 2245, 'creates': 2246, 'co': 2247, 'gabfest': 2248, 'leading': 2249, 'sebastian': 2250, 'chicken': 2251, 'law': 2252, 'french': 2253, \"i'm\": 2254, 'compared': 2255, 'assistance': 2256, 'overlap': 2257, 'heated': 2258, 'freind': 2259, 'information': 2260, 'fortune': 2261, 'brook': 2262, 'acapulco': 2263, 'tarter': 2264, 'kicking': 2265, 'austrian': 2266, 'purchases': 2267, 'dissappointing': 2268, 'lengthier': 2269, 'ending': 2270, 'their': 2271, 'tearing': 2272, 'functional': 2273, 'accused': 2274, 'written': 2275, 'laden': 2276, 'fig': 2277, 'following': 2278, 'peppers': 2279, 'none': 2280, \"yuo're\": 2281, 'himself': 2282, 'remedy': 2283, 'romance': 2284, 'cache': 2285, 'ineffective': 2286, 'theres': 2287, 'aptz': 2288, 'only': 2289, 'directed': 2290, 'divided': 2291, 'crushed': 2292, 'inexcusably': 2293, 'meditation': 2294, 'choori': 2295, 'urinals': 2296, 'collar': 2297, 'pastry': 2298, 'soot': 2299, 'carrying': 2300, 'descent': 2301, 'fav': 2302, 'soufle': 2303, 'arrangment': 2304, 'smoked': 2305, '60': 2306, 'unforgettable': 2307, 'sky': 2308, 'lemonade': 2309, 'sheen': 2310, 'frenzied': 2311, 'overpaying': 2312, 'displeased': 2313, 'pancottas': 2314, 'legit': 2315, 'falafel': 2316, 'predominantly': 2317, 'vacumed': 2318, 'elegantly': 2319, '89': 2320, 'board': 2321, 'kimchee': 2322, 'vinyl': 2323, 'joints': 2324, '13': 2325, 'chose': 2326, 'think': 2327, 'greeeted': 2328, 'involved': 2329, 'friendlier': 2330, 'hijinks': 2331, 'almond': 2332, 'garlicky': 2333, 'surface': 2334, 'specialty': 2335, 'reads': 2336, 'doing': 2337, 'offsprings': 2338, 'orechiette': 2339, 'undercooked': 2340, 'cooked': 2341, \"strip's\": 2342, 'frites': 2343, 'five': 2344, 'departure': 2345, 'control': 2346, 'poking': 2347, 'hunan': 2348, 'cooler': 2349, 'world': 2350, 'logistics': 2351, 'strictly': 2352, 'less': 2353, 'markets': 2354, 'whites': 2355, 'patroon': 2356, 'glow': 2357, 'onesself': 2358, 'endure': 2359, 'contents': 2360, 'italy': 2361, 'turn': 2362, 'not': 2363, 'these': 2364, 'addition': 2365, 'pie': 2366, 'divine': 2367, 'honey': 2368, 'working': 2369, 'leaves': 2370, 'suppose': 2371, 'advantage': 2372, 'uncork': 2373, 'banquets': 2374, 'regardless': 2375, 'nuisance': 2376, 'regular': 2377, 'reference': 2378, 'cuban': 2379, 'gf': 2380, 'seeps': 2381, 'accompaning': 2382, 'going': 2383, 'pepperoncino': 2384, 'harlem': 2385, 'over': 2386, 'pita': 2387, 'freeze': 2388, 'queso': 2389, 'next': 2390, 'hugging': 2391, 'hearts': 2392, 'curry': 2393, 'pig': 2394, 'ripe': 2395, 'virtually': 2396, 'shot': 2397, 'experiences': 2398, 'late': 2399, 'e': 2400, 'nang': 2401, 'brougt': 2402, 'caprese': 2403, 'mac': 2404, 'encrusted': 2405, 'coldly': 2406, 'disorganized': 2407, 'account': 2408, 'starts': 2409, 'babbo': 2410, 'described': 2411, 'covered': 2412, 'depicting': 2413, 'african': 2414, 'homey': 2415, 'ethiopian': 2416, 'goodyear': 2417, 'recieving': 2418, 'lancaster': 2419, 'lick': 2420, 'layout': 2421, 'paris': 2422, 'protein': 2423, 'forever': 2424, 'junior': 2425, 'rushed': 2426, 'try': 2427, 'accepted': 2428, 'across': 2429, 'centric': 2430, 'intentions': 2431, 'communal': 2432, 'pooris': 2433, 'pond': 2434, 'allabaster': 2435, 'turnip': 2436, 'feeling': 2437, 'walks': 2438, 'allowed': 2439, 'seeing': 2440, 'anytime': 2441, 'dutiful': 2442, 'steinhof': 2443, 'candle': 2444, 'notes': 2445, 'kahlua': 2446, 'contains': 2447, 'river': 2448, 'venus': 2449, 'sunday': 2450, 'decore': 2451, 'celebrating': 2452, 'addictive': 2453, 'welcome': 2454, 'appetite': 2455, 'implies': 2456, 'thurs': 2457, 'suckling': 2458, 'ode': 2459, 'upper': 2460, 'upscaled': 2461, 'sage': 2462, 'undesirable': 2463, 'likewise': 2464, 'meh': 2465, 'rest': 2466, 'shaved': 2467, 'delay': 2468, 'happen': 2469, 'amount': 2470, 'finishing': 2471, 'pushed': 2472, \"'big'\": 2473, 'visits': 2474, 'drove': 2475, 'probably': 2476, 'authentic': 2477, 'caliber': 2478, 'norm': 2479, 'closing': 2480, 'wallet': 2481, 'finer': 2482, 'titanic': 2483, 'richard': 2484, 'bartendars': 2485, \"hasn't\": 2486, 'gras': 2487, 'chopped': 2488, 'isolated': 2489, 'shockingly': 2490, 'salt': 2491, 'omelets': 2492, 'adorably': 2493, 'final': 2494, 'lured': 2495, 'days': 2496, \"'frozen\": 2497, 'publishers': 2498, 'frowns': 2499, 'santo': 2500, 'tataki': 2501, 'speaking': 2502, 'flavor': 2503, 'street': 2504, 'leisurely': 2505, 'occupy': 2506, 'ambiance': 2507, 'tray': 2508, \"that's\": 2509, 'snide': 2510, 'wierd': 2511, 'wanna': 2512, 'musts': 2513, 'atop': 2514, 'refreshingly': 2515, 'pumkin': 2516, 'talking': 2517, 'aloof': 2518, \"group's\": 2519, 'attracts': 2520, 'boring': 2521, 'bobby': 2522, 'eerything': 2523, 'truly': 2524, 'disturbing': 2525, 'trucker': 2526, 'amaze': 2527, 'begins': 2528, 'acted': 2529, 'lazy': 2530, 'quickest': 2531, 'costumer': 2532, 'cramped': 2533, 'backed': 2534, 'civilized': 2535, 'hire': 2536, 'ephemera': 2537, 'cinnamon': 2538, 'guests': 2539, 'wines': 2540, 'philippe': 2541, 'apparent': 2542, 'monitoring': 2543, 'tacos': 2544, 'seinfeld': 2545, 'pain': 2546, 'cupcakes': 2547, 'begin': 2548, 'bargain': 2549, 'seaport': 2550, 'businessmen': 2551, 'satisfy': 2552, 'fantastic': 2553, 'terrace': 2554, 'baguettes': 2555, 'coconout': 2556, \"'loud'\": 2557, 'experienced': 2558, 'washed': 2559, 'proportion': 2560, 'posted': 2561, '95': 2562, 'nighttime': 2563, 'geta': 2564, 'fennel': 2565, 'health': 2566, 'piccolo': 2567, 'magic': 2568, 'mezze': 2569, 'overcrowded': 2570, 'o': 2571, 'helpfull': 2572, 'son': 2573, 'seperatley': 2574, 'heartedly': 2575, 'remembered': 2576, 'watching': 2577, '9': 2578, 'delayed': 2579, 'casablanca': 2580, \"tony's\": 2581, 'sees': 2582, 'conventional': 2583, 'residents': 2584, 'trading': 2585, 'deeply': 2586, 'briskly': 2587, 'karl': 2588, 'situation': 2589, 'substituted': 2590, '50': 2591, 'hunched': 2592, 'kiev': 2593, 'lit': 2594, 'temperature': 2595, 'oysters': 2596, '4am': 2597, 'napoleon': 2598, 'kitsch': 2599, 'overkill': 2600, 'refried': 2601, 'spectacular': 2602, 'hole': 2603, 'swiped': 2604, 'afford': 2605, 'varies': 2606, 'thoroughly': 2607, 'authenticity': 2608, 'yen': 2609, 'beers': 2610, 'endured': 2611, 'management': 2612, 'sticky': 2613, 'calf': 2614, 'bunny': 2615, 'says': 2616, 'pernod': 2617, 'priced': 2618, 'ribbon': 2619, 'homemade': 2620, 'larger': 2621, 'they': 2622, 'g': 2623, 'jack': 2624, 'largest': 2625, 'feel': 2626, 'ominous': 2627, 'clams': 2628, 'queixo': 2629, 'shanghainese': 2630, 'annoyed': 2631, 'battered': 2632, 'amongst': 2633, 'page': 2634, 'cava': 2635, 'outlet': 2636, 'market': 2637, 'placed': 2638, 'comp': 2639, 'grizzle': 2640, 'resist': 2641, 'past': 2642, 'west': 2643, 'two': 2644, 'answer': 2645, 'ownership': 2646, 'professionals': 2647, 'italian': 2648, 'sweetness': 2649, 'rewarding': 2650, \"should've\": 2651, 'choices': 2652, 'surprising': 2653, 'india': 2654, 'ordered': 2655, 'previously': 2656, 'track': 2657, 'urchin': 2658, 'momir': 2659, 'terayki': 2660, 'mex': 2661, 'kinds': 2662, 'hickory': 2663, 'loss': 2664, 'mannered': 2665, 'stars': 2666, '10yrs': 2667, 'cozy': 2668, 'assume': 2669, 'origin': 2670, 'charge': 2671, 'freezing': 2672, 'browns': 2673, 'enebriated': 2674, 'forgiving': 2675, 'decide': 2676, 'vegtable': 2677, 'entreesbut': 2678, '1993': 2679, 'poor': 2680, '45pm': 2681, 'overly': 2682, 'rope': 2683, 'fair': 2684, 'appears': 2685, 'anever': 2686, 'outstanding': 2687, 'provencal': 2688, 'kang': 2689, 'vietnamese': 2690, 'bakery': 2691, 'caesar': 2692, 'on': 2693, 'bowls': 2694, 'partitioned': 2695, 'yells': 2696, 'shanghai': 2697, 'attitude': 2698, 'toaster': 2699, 'anythign': 2700, 'approx': 2701, \"xunta's\": 2702, 'dunkin': 2703, 'rottenly': 2704, 'payard': 2705, 'gruyere': 2706, \"well'\": 2707, 'buns': 2708, 'olivey': 2709, 'spaced': 2710, 'portly': 2711, 'tiradito': 2712, 'sum': 2713, 'chicago': 2714, 'vinegary': 2715, 'nut': 2716, 'mostpart': 2717, 'permitted': 2718, 'green': 2719, 'lists': 2720, 'edamame': 2721, \"server's\": 2722, 'chap': 2723, 'hotdogs': 2724, 'coyote': 2725, 'old': 2726, 'interspersed': 2727, 'sings': 2728, 'morimoto': 2729, 'guitar': 2730, 'recalling': 2731, 'sacrificing': 2732, 'stuck': 2733, 'scoops': 2734, 'marys': 2735, 'des': 2736, 'mozart': 2737, 'figure': 2738, 'wrapped': 2739, 'cultural': 2740, \"didn't\": 2741, 'ourselves': 2742, 'angus': 2743, 'views': 2744, 'lock': 2745, 'griddles': 2746, 'cruz': 2747, 'lightly': 2748, 'budget': 2749, 'pizzaiola': 2750, 'tan': 2751, 'baccala': 2752, 'cod': 2753, 'attraction': 2754, 'booked': 2755, 'lawyers': 2756, 'glass': 2757, 'expensice': 2758, 'munch': 2759, 'food': 2760, 'finding': 2761, 'minced': 2762, 'era': 2763, 'taco': 2764, 'dumb': 2765, 'overdressed': 2766, 'additions': 2767, 'cheating': 2768, 'meets': 2769, 'jackets': 2770, 'economic': 2771, 'veal': 2772, 'purple': 2773, 'appreciable': 2774, 'cubed': 2775, 'congeniality': 2776, 'horrendous': 2777, 'said': 2778, 'tasteless': 2779, '700': 2780, 'such': 2781, 'specifically': 2782, 'cloths': 2783, 'includes': 2784, 'googling': 2785, 'peaches': 2786, 'business': 2787, 'eyes': 2788, 'tolerated': 2789, 'sporadically': 2790, 'back': 2791, 'ongoing': 2792, 'politely': 2793, 'outermost': 2794, 'cream': 2795, 'poked': 2796, 'chuckled': 2797, 'festive': 2798, 'stayed': 2799, 'island': 2800, 'twister': 2801, 'cornelia': 2802, 'goodies': 2803, 'horrors': 2804, 'member': 2805, 'broccoli': 2806, 'thus': 2807, 'hesser': 2808, 'strips': 2809, \"menu'\": 2810, 'bell': 2811, 'upto': 2812, 'gazpacho': 2813, 'fritas': 2814, 'expanding': 2815, \"'stand\": 2816, \"daniel's\": 2817, 'greasy': 2818, 'contact': 2819, \"n'awlins\": 2820, 'rooftop': 2821, 'plan': 2822, 'recommendations': 2823, 'sippers': 2824, 'ciao': 2825, 'priority': 2826, 'crumbs': 2827, 'wish': 2828, 'prefers': 2829, 'from': 2830, 'countries': 2831, 'samples': 2832, 'gotta': 2833, 'masculine': 2834, 'thanks': 2835, 'play': 2836, 'thirds': 2837, 'constantly': 2838, 'luglug': 2839, 'perference': 2840, 'mental': 2841, 'fashions': 2842, 'score': 2843, 'handing': 2844, 'perched': 2845, \"grimaldi's\": 2846, '32nd': 2847, 'cordial': 2848, 'fileted': 2849, 'washing': 2850, 'afordable': 2851, 'socializing': 2852, 'become': 2853, 'smug': 2854, 'oh': 2855, 'descriptions': 2856, 'refusing': 2857, 'socks': 2858, 'pray': 2859, 'nur': 2860, 'cheeseburger': 2861, 'marked': 2862, 'pleases': 2863, 'esque': 2864, 'yummies': 2865, 'bass': 2866, 'desssert': 2867, 'confront': 2868, 'esca': 2869, 'eclectic': 2870, 'spinach': 2871, 'camino': 2872, 'suh': 2873, 'flatly': 2874, 'precision': 2875, 'accomdating': 2876, 'must': 2877, 'foam': 2878, 'questions': 2879, \"con's\": 2880, 'playful': 2881, 'ceviche': 2882, 'resturant': 2883, \"joe's\": 2884, 'namely': 2885, 'substitute': 2886, 'canteens': 2887, 'bland': 2888, 'heros': 2889, 'cheeks': 2890, 'journey': 2891, 'fois': 2892, \"in'\": 2893, 'georges': 2894, 'amusement': 2895, 'bakeries': 2896, 'alot': 2897, 'looking': 2898, 'burrito': 2899, 'earth': 2900, '20': 2901, 'class': 2902, 'gladly': 2903, 'devote': 2904, 'merit': 2905, 'hospital': 2906, 'unwelcoming': 2907, 'sean': 2908, 'hour': 2909, 'once': 2910, 'lacking': 2911, 'intrigued': 2912, 'swim': 2913, 'gratis': 2914, 'cremed': 2915, 'slopers': 2916, 'fiesta': 2917, 'funky': 2918, 'tasy': 2919, 'she': 2920, 'stick': 2921, 'lpf': 2922, 'bull': 2923, 'favor': 2924, 'thankfully': 2925, 'removes': 2926, 'maintains': 2927, 'w': 2928, 'becco': 2929, 'looks': 2930, 'cost': 2931, 'tibetan': 2932, 'king': 2933, 'nightly': 2934, 'displeasure': 2935, 'neighbors': 2936, 'table': 2937, 'orchestrate': 2938, 'jewel': 2939, 'nondescript': 2940, 'well': 2941, 'joined': 2942, 'grill': 2943, 'sticks': 2944, '45mins': 2945, 'sizeable': 2946, 'inventive': 2947, 'fairly': 2948, 'enjoyed': 2949, 'conspicuously': 2950, 'hoisting': 2951, 'curving': 2952, 'entree': 2953, '200': 2954, 'spent': 2955, 'celebrate': 2956, 'products': 2957, 'unable': 2958, 'student': 2959, 'drinkless': 2960, 'cab': 2961, 'dimsum': 2962, 'growling': 2963, 'arizona': 2964, \"'its\": 2965, 'cooks': 2966, 'debit': 2967, 'cans': 2968, 'plaintains': 2969, 'reserve': 2970, 'now': 2971, 'pour': 2972, 'ethnic': 2973, '40min': 2974, 'captain': 2975, 'deborah': 2976, 'failing': 2977, 'mysterious': 2978, 'uninspired': 2979, 'unfamiliar': 2980, 'shift': 2981, 'pizzas': 2982, 'concessions': 2983, 'inglese': 2984, 'laborers': 2985, 'pinenuts': 2986, 'asterisk': 2987, \"midtown's\": 2988, 'tonight': 2989, 'churn': 2990, 'path': 2991, 'drank': 2992, 'drenched': 2993, 'frits': 2994, 'stuttering': 2995, 'delicions': 2996, 'buck': 2997, 'already': 2998, 'yea': 2999, 'whether': 3000, 'pile': 3001, 'hippies': 3002, 'layers': 3003, 'separately': 3004, 'windows': 3005, 'concourse': 3006, 'decadence': 3007, 'peas': 3008, 'crackling': 3009, 'advised': 3010, 'cube': 3011, 'whihc': 3012, 'confuzed': 3013, 'dj': 3014, \"patrons'\": 3015, 'memorable': 3016, 'desparate': 3017, 'meditarranean': 3018, 'high': 3019, 'cute': 3020, 'it': 3021, 'pesto': 3022, 'abundance': 3023, 'embarassing': 3024, 'game': 3025, 'enthused': 3026, 'unsatisfied': 3027, 'this': 3028, 'refillable': 3029, 'turned': 3030, 'defy': 3031, 'failed': 3032, 'sakes': 3033, 'treated': 3034, 'attended': 3035, 'subdued': 3036, 'upscale': 3037, 'refurbished': 3038, 'master': 3039, 'especialy': 3040, 'medium': 3041, 'bites': 3042, 'quantity': 3043, 'open': 3044, 'agree': 3045, 'interms': 3046, 'layed': 3047, 'supplies': 3048, 'pizzettes': 3049, 'supervisors': 3050, 'ahead': 3051, 'girls': 3052, 'updated': 3053, \"amarin's\": 3054, 'choppy': 3055, 'pasteis': 3056, 'shaker': 3057, 'lunchtime': 3058, 'caffe': 3059, 'sitting': 3060, 'pointe': 3061, 'pilgrimage': 3062, 'masked': 3063, 'pocket': 3064, 'establishment': 3065, 'pitzas': 3066, 'aac': 3067, 'bus': 3068, 'circles': 3069, \"paul's\": 3070, 'basic': 3071, 'sooooooooooooooooooooooooooooooo': 3072, 'negligent': 3073, 'remembering': 3074, 'grapefruit': 3075, '18': 3076, 'heavily': 3077, 'sell': 3078, 'department': 3079, 'hamburgers': 3080, 'forgotten': 3081, 'plush': 3082, 'is': 3083, 'monthly': 3084, 'frisee': 3085, 'babaganoush': 3086, 'close': 3087, 'each': 3088, 'kulfi': 3089, 'detail': 3090, 'rose': 3091, 'faux': 3092, 'cloudy': 3093, 'yelled': 3094, 'braised': 3095, 'minus': 3096, 'commuters': 3097, 'onto': 3098, 'clearly': 3099, 'steak': 3100, 'informative': 3101, 'staying': 3102, 'except': 3103, 'solved': 3104, 'sushis': 3105, 'tuna': 3106, 'flair': 3107, 'comedy': 3108, 'accented': 3109, 'potential': 3110, 'artichokes': 3111, 'moody': 3112, 'restaurants': 3113, 'cannot': 3114, 'skipped': 3115, 'abusing': 3116, 'loudly': 3117, 'serious': 3118, 'supermarket': 3119, 'smoothly': 3120, 'ubiquitous': 3121, 'corner': 3122, 'usual': 3123, 'wee': 3124, \"simpleton's\": 3125, 'gari': 3126, 'tavern': 3127, '90': 3128, 'trout': 3129, 'buxom': 3130, 'woman': 3131, 'chowder': 3132, 'mock': 3133, 'campari': 3134, 'sytle': 3135, 'accomodations': 3136, 'vegi': 3137, 'older': 3138, 'ratings': 3139, 'features': 3140, 'condiments': 3141, 'college': 3142, 'turns': 3143, 'prodding': 3144, 'sits': 3145, 'soufles': 3146, 'reply': 3147, 'gigantic': 3148, 'pace': 3149, 'what': 3150, 'grade': 3151, 'reign': 3152, 'leans': 3153, 'courtesy': 3154, 'tourists': 3155, 'quaintly': 3156, 'closed': 3157, 'blowing': 3158, 'break': 3159, 'optional': 3160, 'shea': 3161, 'fluorescent': 3162, 'bulgoki': 3163, 'twenty': 3164, 'ketchup': 3165, 'cafeteria': 3166, 'kicker': 3167, 'anthony': 3168, 'hits': 3169, 'spewed': 3170, 'tribeca': 3171, 'remodeling': 3172, 'pizzeria': 3173, 'lacquered': 3174, 'use': 3175, 'eyeing': 3176, 'choice': 3177, 'learned': 3178, 'complimentary': 3179, 'digest': 3180, 'tad': 3181, 'carozza': 3182, 'wishes': 3183, 'deliciouss': 3184, 'porcini': 3185, 'slept': 3186, 'of': 3187, 'thoughts': 3188, 'pepper': 3189, 'veg': 3190, 'argued': 3191, 'winner': 3192, 'create': 3193, 'black': 3194, 'paintings': 3195, 'reds': 3196, 'appetizing': 3197, 'started': 3198, 'dark': 3199, 'felt': 3200, 'wishing': 3201, 'adamantly': 3202, 'after': 3203, 'outdid': 3204, 'lunchs': 3205, 'blocks': 3206, 'balthazar': 3207, \"captain's\": 3208, 'boton': 3209, 'modestly': 3210, 'strip': 3211, 'reminders': 3212, 'peri': 3213, 'tiles': 3214, 'pleased': 3215, 'orzo': 3216, 'ie': 3217, 'ins': 3218, 'giovanny': 3219, \"cecilia's\": 3220, 'heaven': 3221, 'lima': 3222, 'variation': 3223, 'false': 3224, 'too': 3225, 'oil': 3226, 'telling': 3227, 'die': 3228, 'blueberry': 3229, 'dinnertime': 3230, '16': 3231, 'zucchini': 3232, 'timely': 3233, 'carelessly': 3234, 'interested': 3235, 'display': 3236, 'consumed': 3237, 'oyster': 3238, '55': 3239, 'rental': 3240, 'cornish': 3241, 'whiting': 3242, 'snippy': 3243, 'mug': 3244, 'voucher': 3245, 'slapped': 3246, 'stints': 3247, 'process': 3248, 'sounds': 3249, 'someplace': 3250, 'package': 3251, 'crowds': 3252, 'mentioned': 3253, 'century': 3254, 'las': 3255, 'done': 3256, 'seem': 3257, 'slices': 3258, 'toast': 3259, '11pm': 3260, 'strongly': 3261, 'cranberry': 3262, 'sorta': 3263, 'counter': 3264, 'patient': 3265, 'tolerate': 3266, 'gelled': 3267, 'shines': 3268, 'saltimbocca': 3269, 'occassion': 3270, 'opportunities': 3271, 'pound': 3272, 'dull': 3273, 'handful': 3274, 'knack': 3275, 'mario': 3276, 'powdered': 3277, 'luck': 3278, 'rubbed': 3279, 'multigrain': 3280, 'scottish': 3281, 'appetiser': 3282, 'med': 3283, 'pleasantly': 3284, 'placemats': 3285, 'textured': 3286, 'accommodate': 3287, 'we': 3288, 'dragon': 3289, 'checking': 3290, '2112': 3291, \"bar'\": 3292, 'insufficient': 3293, 'grocery': 3294, 'tricks': 3295, 'hewn': 3296, 'werent': 3297, 'excels': 3298, 'ensure': 3299, 'grigio': 3300, 'gonna': 3301, 'did': 3302, 'serene': 3303, 'knees': 3304, 'hijiki': 3305, 'recuperating': 3306, 'commercialized': 3307, 'pitstop': 3308, 'extraordinary': 3309, 'meant': 3310, 'ultra': 3311, 'bruising': 3312, 'tire': 3313, 'con': 3314, 'tablecloths': 3315, 'tinged': 3316, 'largley': 3317, 'creamed': 3318, 'anchovies': 3319, 'bf': 3320, 'enchiladas': 3321, 'delicate': 3322, 'wiping': 3323, 'tater': 3324, 'tradtional': 3325, 'bric': 3326, 'candles': 3327, 'shabu': 3328, 'taci': 3329, 'disappointing': 3330, 'pastrami': 3331, 'subjected': 3332, 'parents': 3333, 'northern': 3334, 'ammenities': 3335, 'papaya': 3336, 'sammiches': 3337, 'welcomed': 3338, 'purchase': 3339, 'bathroom': 3340, 'differs': 3341, 'hotspot': 3342, 'far': 3343, 'represents': 3344, 'yucca': 3345, 'detract': 3346, 'continuously': 3347, 'exposed': 3348, 'awful': 3349, 'eels': 3350, 'bullet': 3351, \"gobo's\": 3352, 'rear': 3353, 'bone': 3354, 'preserved': 3355, 'unobstructed': 3356, 'among': 3357, 'ombiance': 3358, 'hey': 3359, 'sangria': 3360, 'help': 3361, 'couples': 3362, 'flock': 3363, 'canopy': 3364, 'vodka': 3365, 'successful': 3366, 'communication': 3367, 'majority': 3368, 'ironically': 3369, 'doubles': 3370, 'atrocious': 3371, 'snootily': 3372, 'threatening': 3373, 'extraordinarily': 3374, 'belly': 3375, 'prixe': 3376, 'balanced': 3377, 'frozen': 3378, 'yams': 3379, 'skipping': 3380, 'appeals': 3381, 'dates': 3382, 'entertainment': 3383, 'noses': 3384, 'irving': 3385, 'waitperson': 3386, 'wed': 3387, 'broke': 3388, 'us': 3389, 'trendier': 3390, 'mexico': 3391, 'gar': 3392, 'seek': 3393, 'innovative': 3394, 'factory': 3395, 'hosting': 3396, 'bent': 3397, 'voluptuous': 3398, '9pm': 3399, 'tzatziki': 3400, 'couldnt': 3401, 'pasta': 3402, 'peppercorn': 3403, 'drizzled': 3404, 'flatbread': 3405, 'deceptively': 3406, 'boys': 3407, \"march'03\": 3408, 'barbecue': 3409, 'plain': 3410, 'add': 3411, 'initially': 3412, 'pretensious': 3413, 'endless': 3414, 'response': 3415, 'neighborhood': 3416, 'lovely': 3417, 'accomidations': 3418, 'smokers': 3419, 'properly': 3420, 'def': 3421, 'correct': 3422, 'verify': 3423, 'end': 3424, 'gai': 3425, 'coddling': 3426, 'youre': 3427, \"pro's\": 3428, 'baraonda': 3429, 'alfredo': 3430, 'name': 3431, 'consisted': 3432, 'conscious': 3433, \"houston's\": 3434, 'who': 3435, 'recycled': 3436, 'improperly': 3437, 'marzano': 3438, 'tiem': 3439, 'elegance': 3440, 'enhance': 3441, 'admit': 3442, 'chair': 3443, 'uptight': 3444, 'economy': 3445, 'miso': 3446, 'mingling': 3447, 'cases': 3448, 'pointing': 3449, 'toussaint': 3450, 'bottle': 3451, 'smacking': 3452, 'cell': 3453, 'watied': 3454, 'pronounce': 3455, 'hospitality': 3456, \"partner's\": 3457, \"they'll\": 3458, 'flight': 3459, 'eaters': 3460, 'remember': 3461, 'towners': 3462, 'explaining': 3463, 'jasmine': 3464, 'uncouth': 3465, 'boyfriend': 3466, 'declared': 3467, 'moving': 3468, 'arriba': 3469, 'cordon': 3470, 'testament': 3471, 'classical': 3472, 'nuked': 3473, 'stewed': 3474, 'squares': 3475, 'briolle': 3476, 'stopped': 3477, 'finised': 3478, 'complaints': 3479, 'legs': 3480, 'smokey': 3481, 'spoke': 3482, \"sammy's\": 3483, 'fanatical': 3484, 'eagerly': 3485, 'popping': 3486, 'reviewer': 3487, 'bag': 3488, 'third': 3489, 'sides': 3490, 'lousy': 3491, 'amateur': 3492, 'call': 3493, 'physically': 3494, \"aren't\": 3495, 'adventurous': 3496, 'charred': 3497, 'default': 3498, 'doggie': 3499, 'tab': 3500, 'awkward': 3501, 'korean': 3502, 'whilst': 3503, 'hellenic': 3504, 'tete': 3505, 'qualifies': 3506, 'foods': 3507, 'passive': 3508, 'opposite': 3509, 'regularly': 3510, 'costco': 3511, \"won't\": 3512, 'reminder': 3513, 'besides': 3514, 'unbelivable': 3515, 'tender': 3516, 'iceberg': 3517, 'melissa': 3518, 'craze': 3519, 'notable': 3520, 'blandbut': 3521, 'scratch': 3522, 'liquor': 3523, 'waste': 3524, 'cocky': 3525, 'usc': 3526, 'standby': 3527, 'antique': 3528, \"mary's\": 3529, 'chocolatey': 3530, 'dissappeared': 3531, 'yolks': 3532, 'shock': 3533, 'potatoe': 3534, 'diatribe': 3535, 'responsible': 3536, '2': 3537, 'willing': 3538, 'flanked': 3539, 'responsability': 3540, 'aunt': 3541, 'search': 3542, 'me': 3543, 'marsala': 3544, 'watches': 3545, 'light': 3546, 'ony': 3547, 'roughly': 3548, 'absolutely': 3549, 'macaroni': 3550, 'dealing': 3551, \"bar's\": 3552, 'organized': 3553, 'secondary': 3554, 'jab': 3555, 'quadruple': 3556, 'roadhouse': 3557, 'ish': 3558, 'quickly': 3559, 'inadequate': 3560, 'preparation': 3561, 'scrambled': 3562, 'referred': 3563, 'accompany': 3564, 'running': 3565, 'totaled': 3566, 'show': 3567, 'possible': 3568, 'group': 3569, 'asssortment': 3570, 'watered': 3571, \"jaiya's\": 3572, 'perusing': 3573, 'actions': 3574, 'burned': 3575, 'france': 3576, 'disparate': 3577, 'single': 3578, 'our': 3579, 'solitary': 3580, 'zucco': 3581, 'absolute': 3582, 'shy': 3583, 'stated': 3584, 'profanity': 3585, 'cracked': 3586, 'intending': 3587, 'squeeze': 3588, 's': 3589, 'olives': 3590, 'finished': 3591, 'amd': 3592, 'designed': 3593, 'whom': 3594, 'cheff': 3595, 'busiest': 3596, 'banana': 3597, 'cooing': 3598, 'mn': 3599, 'hambone': 3600, 'crunch': 3601, 'sodas': 3602, 'am': 3603, 'rocks': 3604, 'leaned': 3605, 'muddled': 3606, 'shank': 3607, 'hip': 3608, \"dish's\": 3609, 'favorite': 3610, 'boisterous': 3611, 'spide': 3612, 'women': 3613, 'dough': 3614, 'sizes': 3615, 'decor': 3616, 'confronted': 3617, 'lumpia': 3618, 'upwards': 3619, 'caught': 3620, 'chai': 3621, 'intimidated': 3622, 'wonton': 3623, 'tired': 3624, 'compensatory': 3625, 'label': 3626, 'seasoning': 3627, 'short': 3628, 'bubbles': 3629, 'bisque': 3630, 'spectacularly': 3631, 'pearl': 3632, 'every': 3633, 'bisques': 3634, 'or': 3635, 'attests': 3636, 'embracing': 3637, 'maize': 3638, 'bother': 3639, 'dingy': 3640, 'enjoy': 3641, 'uniqueness': 3642, 'pushing': 3643, 'promtply': 3644, \"nidito's\": 3645, 'smoothies': 3646, 'challenge': 3647, 'turkish': 3648, 'insouciance': 3649, 'communicate': 3650, \"'where's\": 3651, '1': 3652, 'slump': 3653, 'panacotta': 3654, 'althought': 3655, 'ditzy': 3656, \"'computer'\": 3657, 'leave': 3658, 'surprise': 3659, 'sugiyama': 3660, 'machissimo': 3661, 'dream': 3662, 'stellar': 3663, 'people': 3664, 'arched': 3665, 'guava': 3666, 'friday': 3667, 'stewy': 3668, 'oriented': 3669, 'wary': 3670, 'brusque': 3671, 'appeal': 3672, 'cabbage': 3673, 'ot': 3674, 'highlights': 3675, 'bucks': 3676, 'pan': 3677, 'indeed': 3678, 'prospect': 3679, 'strange': 3680, 'empinada': 3681, 'sodium': 3682, 'bits': 3683, 'copper': 3684, 'saturated': 3685, 'equivalent': 3686, \"gang's\": 3687, 'located': 3688, 'change': 3689, 'normal': 3690, 'microphone': 3691, 'pedal': 3692, 'plus': 3693, 'according': 3694, 'hunkered': 3695, 'then': 3696, 'downright': 3697, 'fiery': 3698, 'cult': 3699, 'anticipate': 3700, 'progresses': 3701, \"avenue's\": 3702, 'forgetful': 3703, 'smoking': 3704, 'meek': 3705, 'portion': 3706, 'character': 3707, 'at': 3708, 'fishing': 3709, 'diver': 3710, 'requested': 3711, 'powerful': 3712, 'shorter': 3713, 'travel': 3714, 'contrast': 3715, 'enhanced': 3716, 'raga': 3717, 'amazingly': 3718, 'formal': 3719, 'charges': 3720, 'found': 3721, 'replaced': 3722, 'techno': 3723, 'vanish': 3724, 'clinging': 3725, 'mahogany': 3726, 'arrives': 3727, 'granted': 3728, 'potatos': 3729, 'narita': 3730, 'silverware': 3731, 'wooing': 3732, 'san': 3733, \"we'll\": 3734, 'spilt': 3735, 'espressos': 3736, 'yellowtail': 3737, 'fordham': 3738, 'greek': 3739, 'penne': 3740, 'royalty': 3741, 'panko': 3742, 'extremely': 3743, 'organic': 3744, 'buzz': 3745, 'occasions': 3746, 'followed': 3747, 'november': 3748, 'iron': 3749, 'recieved': 3750, 'skimpy': 3751, 'are': 3752, 'smoker': 3753, 'rush': 3754, 'napkin': 3755, 'hoped': 3756, 'float': 3757, 'sluggish': 3758, 'build': 3759, 'books': 3760, 'named': 3761, 'candlelit': 3762, 'knocked': 3763, 'unenthusiastic': 3764, 'signalling': 3765, 'suggesting': 3766, 'velvet': 3767, 'tabletop': 3768, 'skillet': 3769, 'crusted': 3770, 'waiter': 3771, 'lover': 3772, 'didnt': 3773, 'fortunately': 3774, 'offending': 3775, \"reviewer's\": 3776, 'cubes': 3777, 'quatro': 3778, 'bangkok': 3779, 'remark': 3780, 'fillings': 3781, \"don't\": 3782, 'becuause': 3783, 'hoggie': 3784, 'landed': 3785, 'encountered': 3786, 'discounted': 3787, 'carries': 3788, 'cuba': 3789, 'valenciana': 3790, 'boiling': 3791, 'inquiry': 3792, 'perma': 3793, 'definitive': 3794, 'inclusive': 3795, 'hampton': 3796, \"70's\": 3797, 'tlacoyos': 3798, \"she'd\": 3799, 'amazed': 3800, 'groove': 3801, 'bowties': 3802, 'culinary': 3803, 'voicemail': 3804, 'gnocci': 3805, 'blond': 3806, 'wonderfully': 3807, \"calves'\": 3808, 'incredibly': 3809, 'record': 3810, 'ill': 3811, 'garden': 3812, 'rum': 3813, 'tourist': 3814, 'barley': 3815, 'marks': 3816, 'hummus': 3817, 'suitcases': 3818, 'slick': 3819, 'desset': 3820, 'charging': 3821, 'finistist': 3822, 'cafe': 3823, 'inconspicuous': 3824, 'brushetta': 3825, 'rare': 3826, 'baba': 3827, 'infamous': 3828, 'nightmare': 3829, 'effectively': 3830, 'presumes': 3831, 'talk': 3832, 'exec': 3833, 'lodge': 3834, 'wasabi': 3835, 'moroccan': 3836, 'staples': 3837, 'masterful': 3838, 'flavorless': 3839, 'omelettes': 3840, 'milkshake': 3841, 'alongside': 3842, 'cow': 3843, 'playlist': 3844, 'trash': 3845, 'the': 3846, 'artwork': 3847, 'beware': 3848, 'decidedly': 3849, 'whatsoever': 3850, 'sarcastic': 3851, 'something': 3852, 'lively': 3853, 'neck': 3854, 'chana': 3855, 'bleu': 3856, 'cooled': 3857, 'update': 3858, 'hop': 3859, 'dripping': 3860, 'its': 3861, 'lefthand': 3862, 'knapp': 3863, 'meet': 3864, 'bigger': 3865, 'sours': 3866, 'know': 3867, 'stringy': 3868, 'munchies': 3869, 'oj': 3870, 'scare': 3871, 'janitor': 3872, 'summary': 3873, \"'friends'\": 3874, 'exactly': 3875, 'knew': 3876, 'streamlined': 3877, 'races': 3878, 'timing': 3879, 'crowded': 3880, 'gem': 3881, 'shared': 3882, 'pick': 3883, 'underestimated': 3884, 'slack': 3885, 'why': 3886, 'policy': 3887, 'company': 3888, 'chances': 3889, 'limp': 3890, 'seasonal': 3891, 'again': 3892, 'apps': 3893, 'discplayed': 3894, 'park': 3895, 'misguide': 3896, 'signals': 3897, 'wanting': 3898, 'front': 3899, 'lived': 3900, 'diet': 3901, 'form': 3902, 'enjoying': 3903, 'eventually': 3904, 'poised': 3905, 'browse': 3906, 'ashamed': 3907, 'variations': 3908, 'items': 3909, 'loosey': 3910, 'poo': 3911, 'premixed': 3912, 'ordinary': 3913, 'ship': 3914, 'foo': 3915, 'ham': 3916, 'sunny': 3917, 'wafting': 3918, 'message': 3919, 'prie': 3920, 'luxurious': 3921, 'remaining': 3922, 'recommendation': 3923, 'momo': 3924, 'restroom': 3925, \"he'd\": 3926, 'cedar': 3927, 'expense': 3928, 'avoids': 3929, 'discreet': 3930, 'fixins': 3931, 'rape': 3932, 'stunning': 3933, 'answered': 3934, 'animal': 3935, 'rustic': 3936, 'leaks': 3937, 'ouor': 3938, 'areain': 3939, 'terrines': 3940, 'elegant': 3941, 'shady': 3942, 'difficult': 3943, 'mound': 3944, 'chesses': 3945, 'should': 3946, 'eyed': 3947, 'swordfish': 3948, 'initial': 3949, 'bests': 3950, 'smell': 3951, 'showed': 3952, 'rolls': 3953, 'busboys': 3954, 'caramel': 3955, 'floors': 3956, 'theater': 3957, 'brought': 3958, 'croquettes': 3959, \"dizzy's\": 3960, 'gelatin': 3961, 'carmelized': 3962, 'burger': 3963, 'pedestrian': 3964, 'admittedly': 3965, 'unfotunately': 3966, 'reccomended': 3967, 'pefectly': 3968, 'evoke': 3969, 'oddly': 3970, 'sing': 3971, 'understated': 3972, 'dry': 3973, 'suggestions': 3974, 'bumbling': 3975, '140': 3976, 'english': 3977, 'grey': 3978, '10min': 3979, 'tigelle': 3980, 'wacky': 3981, 'incapable': 3982, 'farmhouse': 3983, 'reason': 3984, 'chatting': 3985, 'brick': 3986, 'outrageous': 3987, 'vowed': 3988, 'announce': 3989, 'numerous': 3990, \"boyfriend's\": 3991, 'koi': 3992, 'cash': 3993, 'tile': 3994, 'dissatisfied': 3995, 'abysmally': 3996, 'saucers': 3997, 'raise': 3998, 'sandwitch': 3999, \"luger's\": 4000, 'attitudes': 4001, '80s': 4002, 'skimp': 4003, 'lease': 4004, 'shortcake': 4005, 'colorful': 4006, 'forces': 4007, 'kicks': 4008, \"there's\": 4009, 'silky': 4010, 'phoning': 4011, 'dog': 4012, 'defying': 4013, 'expected': 4014, 'meow': 4015, 'disgusting': 4016, 'midtown': 4017, 'recommand': 4018, 'eatten': 4019, 'incompetent': 4020, 'attentive': 4021, 'paid': 4022, 'appetizers': 4023, 'faucets': 4024, 'sheets': 4025, 'eve': 4026, 'sassy': 4027, 'terribly': 4028, 'roe': 4029, 'able': 4030, 'samosa': 4031, 'harried': 4032, 'flat': 4033, 'pavement': 4034, 'insisted': 4035, 'closer': 4036, 'particular': 4037, 'hover': 4038, 'mulino': 4039, 'good': 4040, 'keeps': 4041, 'case': 4042, 'glimpses': 4043, 'sticking': 4044, 'brass': 4045, 'stay': 4046, 'horrid': 4047, 'kanzuri': 4048, 'waitstaff': 4049, 'takeout': 4050, 'were': 4051, 'pros': 4052, 'fusing': 4053, 'enter': 4054, 'continued': 4055, 'champaign': 4056, 'wanted': 4057, '5': 4058, 'tangy': 4059, 'joint': 4060, 'souther': 4061, 'yucky': 4062, 'mind': 4063, 'based': 4064, 'sharing': 4065, 'lulled': 4066, 'nerve': 4067, 'tuscany': 4068, 'scheme': 4069, 'matching': 4070, 'route': 4071, 'obviously': 4072, 'kaffir': 4073, 'freshly': 4074, 'tapioca': 4075, 'superior': 4076, 'prior': 4077, 'puree': 4078, 'mushy': 4079, 'rickeys': 4080, 'kitchen': 4081, 'opens': 4082, \"ass'd\": 4083, 'embarrassed': 4084, 'noir': 4085, 'madiba': 4086, 'delightful': 4087, 'goodnight': 4088, 'aioli': 4089, 'age': 4090, 'spoiled': 4091, 'fugettaboutit': 4092, 'factor': 4093, 'either': 4094, 'flagged': 4095, 'till': 4096, 'preoccupied': 4097, 'laughable': 4098, 'dips': 4099, 'nervously': 4100, 'about': 4101, 'tasted': 4102, 'kinda': 4103, 'picking': 4104, 'insist': 4105, \"johnnie's\": 4106, 'prime': 4107, 'argurments': 4108, 'inside': 4109, 'lasagnas': 4110, '0': 4111, 'mooshu': 4112, 'offended': 4113, 'customer': 4114, 'rail': 4115, 'passed': 4116, 'often': 4117, 'louie': 4118, 'fajitas': 4119, 'frustration': 4120, 'present': 4121, 'wasting': 4122, 'terrazza': 4123, 'squeezed': 4124, 'carlessly': 4125, 'atleast': 4126, 'pulsing': 4127, 'piling': 4128, 'approach': 4129, 'actully': 4130, 'thoughtful': 4131, 'younger': 4132, 'macchiato': 4133, 'dieing': 4134, 'cucumbers': 4135, 'deal': 4136, 'photos': 4137, 'ribs': 4138, 'instructions': 4139, 'newspaper': 4140, 'euro': 4141, 'repugnato': 4142, 'noise': 4143, 'denied': 4144, 'bog': 4145, 'annoying': 4146, 'expectations': 4147, 'aggressive': 4148, 'residence': 4149, 'lusty': 4150, 'mozerella': 4151, 'mill': 4152, 'texas': 4153, 'consistency': 4154, 'connoisseur': 4155, 'velety': 4156, 'deserves': 4157, 'acting': 4158, 'alike': 4159, 'dump': 4160, 'biz': 4161, 'cleverly': 4162, 'grand': 4163, 'patiently': 4164, 'options': 4165, 'others': 4166, 'absoloutely': 4167, 'boyardee': 4168, 'hsared': 4169, 'j': 4170, 'heat': 4171, 'righteous': 4172, 'prefix': 4173, 'handrolls': 4174, 'vicinity': 4175, 'rabe': 4176, 'homemeade': 4177, 'profound': 4178, 'dumpling': 4179, 'unpaid': 4180, 'pronounced': 4181, 'clientele': 4182, 'seal': 4183, 'perfected': 4184, 'person': 4185, 'smaller': 4186, 'interrrupted': 4187, 'apology': 4188, \"isn't\": 4189, 'calculating': 4190, 'piece': 4191, 'unto': 4192, 'while': 4193, 'thrown': 4194, 'breeziness': 4195, 'yes': 4196, 'laughing': 4197, 'gravlax': 4198, 'chile': 4199, 'mediterranean': 4200, 'relatively': 4201, 'shame': 4202, 'beacon': 4203, 'visiting': 4204, 'cutie': 4205, 'big': 4206, 'everybody': 4207, 'desired': 4208, 'branzino': 4209, 'piccata': 4210, 'efforts': 4211, 'arm': 4212, 'addicted': 4213, 'solo': 4214, 'forgive': 4215, 'spaces': 4216, 'happy': 4217, 'product': 4218, 'pseudo': 4219, 'jumelle': 4220, 'shoots': 4221, 'somewhat': 4222, 'blogs': 4223, 'seitan': 4224, 'anyplace': 4225, 'spongy': 4226, \"hadn't\": 4227, 'meal': 4228, 'sweed': 4229, 'mocca': 4230, \"island's\": 4231, 'picnic': 4232, 'yellowfin': 4233, 'litte': 4234, 'deliver': 4235, 'remove': 4236, 'received': 4237, 'taverna': 4238, 'generally': 4239, 'hair': 4240, 'heeled': 4241, 'diners': 4242, 'bed': 4243, 'interior': 4244, 'involve': 4245, 'drunken': 4246, 'format': 4247, 'ladies': 4248, 'competely': 4249, 'mom': 4250, 'strawberry': 4251, 'jodi': 4252, 'prevent': 4253, 'barking': 4254, 'notably': 4255, 'border': 4256, \"mom's\": 4257, 'things': 4258, 'pupusas': 4259, 'matzo': 4260, 'few': 4261, 'mermaid': 4262, 'uncomfortable': 4263, 'poblano': 4264, 'unbelievable': 4265, 'cultured': 4266, 'tune': 4267, 'shifts': 4268, 'bars': 4269, 'lineup': 4270, 'vinagarette': 4271, 'snatched': 4272, 'nearby': 4273, 'breakfast': 4274, 'compares': 4275, 'overheard': 4276, 'discouraged': 4277, 'transfer': 4278, 'baby': 4279, 'peace': 4280, 'ceviches': 4281, 'ocho': 4282, '350': 4283, 'inexcusable': 4284, \"bubby's\": 4285, 'ranked': 4286, 'adore': 4287, 'fiance': 4288, 'throught': 4289, 'exact': 4290, 'reviewers': 4291, 'hearing': 4292, 'whats': 4293, 'result': 4294, 'supposedly': 4295, 'brunching': 4296, 'appreciating': 4297, '2000ish': 4298, 'amaizing': 4299, 'tough': 4300, 'budgets': 4301, 'hooters': 4302, 'relied': 4303, 'flawed': 4304, 'confirmations': 4305, 'online': 4306, 'cents': 4307, 'whelmed': 4308, 'wound': 4309, 'texture': 4310, 'stares': 4311, 'lovers': 4312, 'realised': 4313, 'determine': 4314, 'warn': 4315, 'suddenly': 4316, 'closely': 4317, \"'til\": 4318, 'into': 4319, 'insouciant': 4320, 'freezer': 4321, 'presides': 4322, 'glaze': 4323, 'fit': 4324, 'clothing': 4325, 'began': 4326, 'custard': 4327, 'olive': 4328, 'sur': 4329, 'desserts': 4330, 'soho': 4331, 'pork': 4332, 'reel': 4333, 'leather': 4334, 'drinker': 4335, 'objected': 4336, 'wintry': 4337, 'happening': 4338, 'tastings': 4339, 'champange': 4340, 'modern': 4341, 'coconut': 4342, 'miffed': 4343, 'discretion': 4344, 'clients': 4345, 'drycleaning': 4346, 'incredible': 4347, 'chipssalsa': 4348, 'packaged': 4349, 'dollar': 4350, 'matre': 4351, 'crust': 4352, 'practically': 4353, 'morning': 4354, 'dribble': 4355, 'double': 4356, 'frills': 4357, 'mad': 4358, 'veracruz': 4359, 'trap': 4360, 'sons': 4361, 'creme': 4362, 'attempted': 4363, 'imagination': 4364, 'extent': 4365, 'split': 4366, 'gamey': 4367, 'thrift': 4368, 'thing': 4369, 'ridicoulas': 4370, 'helps': 4371, 'buttery': 4372, 'wisely': 4373, 'dubbed': 4374, 'computer': 4375, 'touched': 4376, 'supersize': 4377, 'crammed': 4378, 'do': 4379, 'haute': 4380, 'soap': 4381, \"'bombay'\": 4382, 'focus': 4383, 'hash': 4384, \"zutto's\": 4385, 'jumping': 4386, 'cellar': 4387, 'grime': 4388, 'consideration': 4389, 'nachos': 4390, 'ages': 4391, 'fork': 4392, 'soaked': 4393, '80': 4394, 'chewy': 4395, 'village': 4396, 'uncrowded': 4397, 'cauliflower': 4398, 'earned': 4399, 'handle': 4400, 'asparagus': 4401, 'panini': 4402, 'yamie': 4403, 'fingertips': 4404, 'moisture': 4405, 'care': 4406, 'impressive': 4407, 'accommodating': 4408, 'anipasto': 4409, 'residential': 4410, 'saves': 4411, 'sancerre': 4412, 'smelling': 4413, 'tracked': 4414, 'aske': 4415, 'chow': 4416, 'enthralls': 4417, 'afternoon': 4418, 'proceeded': 4419, 'flank': 4420, 'produced': 4421, 'trained': 4422, 'bitter': 4423, 'myriad': 4424, 'conservative': 4425, 'pat': 4426, 'splits': 4427, 'itself': 4428, 'basil': 4429, 'stature': 4430, 'hoopla': 4431, 'supervising': 4432, 'lounging': 4433, 'fluffy': 4434, 'bday': 4435, 'injera': 4436, 'insulting': 4437, 'vacant': 4438, 'worldly': 4439, 'stiff': 4440, 'charged': 4441, 'depression': 4442, 'roof': 4443, 'conversations': 4444, 'coffee': 4445, 'word': 4446, 'wants': 4447, 'featherweight': 4448, 'injury': 4449, 'shopping': 4450, 'jamun': 4451, 'grab': 4452, 'weirdly': 4453, 'takeaway': 4454, 'gallaghers': 4455, 'becuase': 4456, 'serrano': 4457, 'week': 4458, 'bithday': 4459, 'miserable': 4460, 'butternut': 4461, 'totally': 4462, 'centerpiece': 4463, 'paste': 4464, 'comments': 4465, 'kong': 4466, \"time's\": 4467, 'speedy': 4468, 'helpful': 4469, 'stranger': 4470, 'placing': 4471, 'here': 4472, 'irony': 4473, 'solidly': 4474, 'excellant': 4475, 'bollos': 4476, 'approached': 4477, 'example': 4478, 'realm': 4479, 'pleaser': 4480, 'appetizier': 4481, 'lengthy': 4482, 'emilio': 4483, 'rubber': 4484, 'professional': 4485, 'treating': 4486, 'oregano': 4487, 'registers': 4488, 'chinese': 4489, \"i'd\": 4490, 'sotee': 4491, 'indicate': 4492, 'fluke': 4493, 'yourself': 4494, 'tempting': 4495, 'unappealing': 4496, 'tanks': 4497, 'compote': 4498, 'cherries': 4499, 'microwaveable': 4500, \"they're\": 4501, 'gets': 4502, 'spur': 4503, 'grits': 4504, 'hungry': 4505, 'healthy': 4506, 'strutted': 4507, 'surprized': 4508, 'artifacts': 4509, 'amble': 4510, 'pro': 4511, 'decked': 4512, 'closet': 4513, 'candy': 4514, 'waiters': 4515, 'punctual': 4516, 'madai': 4517, 'skirt': 4518, 'waitor': 4519, 'huffy': 4520, 'remains': 4521, 'extra': 4522, 'regenerator': 4523, 'cleared': 4524, 'tried': 4525, 'overrated': 4526, 'snobby': 4527, 'jerry': 4528, 'cuisines': 4529, 'crave': 4530, 'boat': 4531, 'saganaki': 4532, 'regis': 4533, 'dirty': 4534, 'lox': 4535, 'razor': 4536, 'aroused': 4537, 'visually': 4538, '300': 4539, 'rubarb': 4540, 'unflavored': 4541, 'unporfessional': 4542, 'pertain': 4543, 'brim': 4544, 'rhone': 4545, 'shortly': 4546, 'gravy': 4547, 'impolite': 4548, 'fajita': 4549, 'substutions': 4550, 'chewie': 4551, 'tantamount': 4552, 'tatin': 4553, 'longer': 4554, 'tails': 4555, 'croissants': 4556, 'leaning': 4557, 'hence': 4558, 'accustomed': 4559, 'forgetting': 4560, 'funny': 4561, 'chipped': 4562, '25': 4563, 'decorations': 4564, 'ample': 4565, 'recite': 4566, 'puching': 4567, 'notwithstanding': 4568, 'pinot': 4569, 'splash': 4570, 'waiting': 4571, 'giant': 4572, 'appearance': 4573, 'meantime': 4574, 'immediatly': 4575, 'versus': 4576, 'cobb': 4577, 'satisfies': 4578, 'marble': 4579, 'playing': 4580, 'clueless': 4581, 'worst': 4582, 'cookies': 4583, 'fond': 4584, 'goodbye': 4585, 'hiccup': 4586, 'marred': 4587, \"cafe's\": 4588, 'suffice': 4589, 'churrasco': 4590, 'warrant': 4591, 'raviloi': 4592, 'block': 4593, 'mozzarella': 4594, 'fired': 4595, 'tortillas': 4596, 'carrots': 4597, 'presumably': 4598, 'relative': 4599, 'guest': 4600, 'site': 4601, 'batch': 4602, 'tries': 4603, 'arrogant': 4604, 'juices': 4605, \"that'll\": 4606, 'til': 4607, 'ranging': 4608, 'metling': 4609, 'removing': 4610, 'conserative': 4611, 'wedges': 4612, 'sake': 4613, 'gentleman': 4614, 'fettuccine': 4615, 'wrapping': 4616, 'falling': 4617, 'fettucine': 4618, 'maintained': 4619, 'which': 4620, 'awkwardly': 4621, 'chandelier': 4622, \"quality's\": 4623, 'thali': 4624, 'drink': 4625, 'basted': 4626, 'lodgey': 4627, 'thinking': 4628, 'flecked': 4629, 'stews': 4630, 'delivering': 4631, '1999': 4632, 'drastically': 4633, 'diner': 4634, 'zero': 4635, 'preceed': 4636, 'cabrales': 4637, 'benedict': 4638, 'baked': 4639, 'obliging': 4640, 'episode': 4641, 'moments': 4642, 'grape': 4643, 'fine': 4644, 'loft': 4645, 'auspiciously': 4646, 'portions': 4647, '30pm': 4648, 'spigleau': 4649, 'dessert': 4650, 'belongs': 4651, 'replica': 4652, 'lbs': 4653, 'allow': 4654, 'gently': 4655, 'weddings': 4656, 'born': 4657, 'curiosities': 4658, 'converge': 4659, 'crabcake': 4660, 'businesspeople': 4661, 'requesting': 4662, 'expressed': 4663, 'sicily': 4664, 'unbalanced': 4665, 'cited': 4666, 'delicious': 4667, 'catch': 4668, 'mostly': 4669, 'yellow': 4670, 'ceilings': 4671, 'frequented': 4672, 'marjoram': 4673, 'skin': 4674, 'overzealous': 4675, 'announced': 4676, 'karate': 4677, 'prelude': 4678, 'tseukune': 4679, 'curious': 4680, 'screwdrivers': 4681, 'spoken': 4682, 'though': 4683, 'tenders': 4684, 'steakhouse': 4685, 'flounder': 4686, 'dependable': 4687, 'clear': 4688, 'tons': 4689, 'amazing': 4690, 'hostesses': 4691, 'salad': 4692, 'both': 4693, 'ninth': 4694, 'degree': 4695, 'overwhelm': 4696, 'rib': 4697, 'encourage': 4698, 'deplorable': 4699, 'jacks': 4700, 'slipping': 4701, 'utensils': 4702, 'tenament': 4703, 'shucked': 4704, 'souk': 4705, 'slopped': 4706, 'draws': 4707, 'cutlery': 4708, 'replenish': 4709, 'coloring': 4710, 'astronomical': 4711, 'grocer': 4712, 'gnoccho': 4713, 'sun': 4714, 'summertime': 4715, 'mussles': 4716, 'conneticut': 4717, 'reminds': 4718, 'slaw': 4719, 'lumps': 4720, 'resentment': 4721, 'gourmet': 4722, 'practice': 4723, 'umberto': 4724, 'menue': 4725, 'existent': 4726, 'keeper': 4727, 'tip': 4728, 'packed': 4729, 'visibly': 4730, 'games': 4731, 'terms': 4732, 'abusive': 4733, 'guacamole': 4734, 'troubles': 4735, \"would've\": 4736, 'vermecelli': 4737, 'receipt': 4738, 'refusal': 4739, 'albeit': 4740, 'plenty': 4741, \"frog's\": 4742, 'lends': 4743, 'moutarde': 4744, 'typically': 4745, 'platters': 4746, 'ghoulash': 4747, 'ecclectic': 4748, 'adornment': 4749, 'nope': 4750, 'hot': 4751, 'rave': 4752, 'gums': 4753, 'dressing': 4754, 'snacking': 4755, 'frantic': 4756, 'verbally': 4757, \"neighborhood's\": 4758, 'terse': 4759, 'rate': 4760, 'binegar': 4761, 'mature': 4762, 'gesture': 4763, 'hear': 4764, 'brac': 4765, 'filets': 4766, 'knots': 4767, 'blossoms': 4768, 'sales': 4769, 'starter': 4770, 'deserts': 4771, 'discrepancy': 4772, 'monitors': 4773, 'downside': 4774, 'moderately': 4775, 'sangiovese': 4776, 'stupid': 4777, 'momentarily': 4778, \"nick's\": 4779, 'sidewalk': 4780, 'botanical': 4781, 'attached': 4782, 'caucasus': 4783, 'trio': 4784, 'intervened': 4785, 'saturdays': 4786, 'mahal': 4787, 'manages': 4788, 'square': 4789, 'classics': 4790, 'get': 4791, 'treatment': 4792, 'makes': 4793, 'explain': 4794, 'flour': 4795, \"party's\": 4796, 'fresh': 4797, 'condescending': 4798, 'chipotle': 4799, \"cousin's\": 4800, 'ask': 4801, 'mistaken': 4802, 'flawless': 4803, 'ons': 4804, 'semi': 4805, 'busperson': 4806, 'looked': 4807, 'jaya': 4808, 'surperb': 4809, 'injustice': 4810, 'disliked': 4811, 'reservationist': 4812, 'invited': 4813, 'jalebi': 4814, 'raved': 4815, 'harkens': 4816, 'slimmers': 4817, 'rigatoni': 4818, 'nite': 4819, 'send': 4820, 'bursting': 4821, 'seconds': 4822, \"jesse's\": 4823, 'besieged': 4824, 'suculent': 4825, 'negative': 4826, 'extras': 4827, 'through': 4828, 'guajillo': 4829, '2nd': 4830, 'bet': 4831, 'espn': 4832, 'lowered': 4833, 'hustling': 4834, 'places': 4835, 'yours': 4836, '1pm': 4837, 'yoursef': 4838, 'apparently': 4839, 'redeaming': 4840, 'freindly': 4841, 'chewing': 4842, 'prompting': 4843, 'steward': 4844, 'escarole': 4845, 'solicitous': 4846, 'newly': 4847, 'gormet': 4848, 'stormy': 4849, 'alive': 4850, 'greeted': 4851, 'mingle': 4852, 'pink': 4853, 'body': 4854, 'literally': 4855, 'assured': 4856, 'nathan': 4857, 'brief': 4858, 'comfy': 4859, 'reliable': 4860, 'synch': 4861, 'pouring': 4862, 'becomes': 4863, 'concession': 4864, 'partners': 4865, \"date's\": 4866, 'guys': 4867, 'anniversary': 4868, 'variety': 4869, 'infer': 4870, 'louis': 4871, 'marquise': 4872, 'cooking': 4873, 'oak': 4874, 'tastes': 4875, 'beauty': 4876, 'pulled': 4877, 'chiles': 4878, 'as': 4879, 'naive': 4880, 'hold': 4881, 'give': 4882, 'nor': 4883, 'noting': 4884, 'spanokopita': 4885, 'never': 4886, 'wall': 4887, 'upon': 4888, 'thru': 4889, 'warned': 4890, 'repeatedly': 4891, 'level': 4892, 'spots': 4893, 'hearty': 4894, 'strangers': 4895, 'therefore': 4896, 'overcooked': 4897, 'inbetween': 4898, '1130': 4899, 'closest': 4900, 'told': 4901, 'checks': 4902, 'nasty': 4903, 'milieu': 4904, 'unseen': 4905, 'arugula': 4906, 'exceeded': 4907, 'di': 4908, 'porterhouse': 4909, 'totaly': 4910, 'mushroom': 4911, 'haunt': 4912, 'all': 4913, 'lacked': 4914, 'fruity': 4915, \"dinner's\": 4916, 'difilculty': 4917, 'ottoman': 4918, 'hall': 4919, 'evidently': 4920, 'laid': 4921, 'waits': 4922, 'amanda': 4923, 'pa': 4924, 'complicated': 4925, 'low': 4926, 'de': 4927, 'gulab': 4928, 'sizzling': 4929, 'spuds': 4930, 'sunset': 4931, 'nibbles': 4932, \"salad's\": 4933, 'longest': 4934, 'pretend': 4935, 'oceanic': 4936, 'wasted': 4937, 'fulfilling': 4938, 'always': 4939, 'eponymous': 4940, 'barbeque': 4941, 'overshadow': 4942, 'luscious': 4943, 'intense': 4944, 'basics': 4945, 'acknowledgment': 4946, 'collard': 4947, \"law's\": 4948, 'seamlessly': 4949, '545pm': 4950, 'veggie': 4951, 'wannabes': 4952, 'prepared': 4953, 'partially': 4954, 'enjoyment': 4955, 'd': 4956, 'cinzano': 4957, 'consisting': 4958, 'ambitious': 4959, 'butter': 4960, 'exiled': 4961, 'had': 4962, 'klump': 4963, 'tended': 4964, 'tapped': 4965, 'souffle': 4966, 'recipe': 4967, 'rests': 4968, 'soaking': 4969, 'struggle': 4970, 'superfine': 4971, 'flip': 4972, 'gainfully': 4973, 'malibu': 4974, 'justly': 4975, 'desiring': 4976, 'bond': 4977, 'custom': 4978, 'sick': 4979, 'needed': 4980, '3pm': 4981, 'expecting': 4982, 'success': 4983, 'commensurate': 4984, 'cracking': 4985, 'manageress': 4986, 'unseasoned': 4987, 'dads': 4988, 'compare': 4989, 'chasers': 4990, 'appologies': 4991, 'missed': 4992, 'patio': 4993, 'demigraphic': 4994, 'complementing': 4995, 'meals': 4996, 'lamps': 4997, 'booming': 4998, 'tamale': 4999, 'dutch': 5000, 'tartare': 5001, 'pulling': 5002, \"'soft'\": 5003, 'staff': 5004, 'sked': 5005, 'testicles': 5006, 'wrote': 5007, 'ride': 5008, 'prompted': 5009, 'ordinal': 5010, 'soda': 5011, 'dissapointed': 5012, 'bunch': 5013, 'onions': 5014, 'taxes': 5015, 'suggest': 5016, 'terrible': 5017, 'carte': 5018, 'models': 5019, 'learning': 5020, 'fabulously': 5021, 'experiance': 5022, '40times': 5023, 'nice': 5024, \"girl's\": 5025, 'dried': 5026, 'vegs': 5027, 'happily': 5028, 'mesculen': 5029, 'latin': 5030, 'discovered': 5031, 'sedate': 5032, 'skull': 5033, 'peking': 5034, 'udon': 5035, 'fighting': 5036, 'adequate': 5037, 'carafes': 5038, 'rethink': 5039, 'details': 5040, 'periodically': 5041, 'mayonnaise': 5042, 'pair': 5043, 'center': 5044, \"sm's\": 5045, 'gophers': 5046, 'lemon': 5047, 'showcases': 5048, 'probbably': 5049, 'dining': 5050, 'excited': 5051, 'dense': 5052, 'spices': 5053, 'entering': 5054, 'pennants': 5055, 'consumption': 5056, 'enjoyable': 5057, 'seafaring': 5058, \"chino's\": 5059, 'punches': 5060, 'melted': 5061, 'soups': 5062, 'different': 5063, 'moussaka': 5064, 'alaskan': 5065, 'feature': 5066, 'rightly': 5067, 'horseradish': 5068, 'immeadiately': 5069, 'moist': 5070, 'presented': 5071, 'quoted': 5072, 'complement': 5073, 're': 5074, \"waitress'\": 5075, 'grandmother': 5076, 'paella': 5077, 'gaijin': 5078, 'later': 5079, 'contained': 5080, 'layering': 5081, 'onetime': 5082, 'blew': 5083, 'yuka': 5084, 'riser': 5085, 'elsewhere': 5086, 'surly': 5087, 'wouldnt': 5088, 'fudge': 5089, 'bd': 5090, 'wet': 5091, 'invariably': 5092, 'joe': 5093, 'behavior': 5094, 'neighboring': 5095, \"williams'\": 5096, 'card': 5097, 'packing': 5098, 'shine': 5099, 'passageway': 5100, 'neighbor': 5101, 'chase': 5102, 'sad': 5103, 'chicha': 5104, 'raving': 5105, 'empties': 5106, 'duh': 5107, 'quest': 5108, 'cuties': 5109, '11': 5110, 'straws': 5111, 'slower': 5112, 'meaty': 5113, 'secluded': 5114, 'shrimps': 5115, 'cocktail': 5116, 'recalled': 5117, 'huge': 5118, 'value': 5119, 'modest': 5120, 'restored': 5121, 'posers': 5122, 'num': 5123, 'fastest': 5124, 'battle': 5125, 'club': 5126, \"mafrici's\": 5127, 'billy': 5128, 'pelligrino': 5129, \"it's\": 5130, 'great': 5131, 'sustenance': 5132, 'prunes': 5133, 'pancakes': 5134, 'quantities': 5135, 'tool': 5136, 'shall': 5137, 'riches': 5138, 'sit': 5139, \"haven't\": 5140, 'baguette': 5141, 'campbells': 5142, 'heap': 5143, 'marinara': 5144, 'chic': 5145, 'join': 5146, 'okonomiyake': 5147, 'pumpkin': 5148, 'doubled': 5149, 'mounds': 5150, 'window': 5151, 'convenient': 5152, 'sald': 5153, 'sirloin': 5154, 'least': 5155, 'losing': 5156, 'biggest': 5157, 'kind': 5158, 'insulted': 5159, 'when': 5160, 'outrageuos': 5161, 'specialties': 5162, 'oppressively': 5163, 'smooth': 5164, 'hocked': 5165, 'indian': 5166, \"i'll\": 5167, 'nyu': 5168, 'band': 5169, 'bo': 5170, 'led': 5171, 'gardens': 5172, 'quiz': 5173, 'thin': 5174, 'confit': 5175, 'inconsistent': 5176, 'lingered': 5177, 'grace': 5178, 'dont': 5179, \"'being\": 5180, 'milk': 5181, 'mean': 5182, 'asked': 5183, 'mixture': 5184, 'fits': 5185, 'ive': 5186, 'weenie': 5187, 'pull': 5188, 'painfully': 5189, 'quaint': 5190, 'peanut': 5191, 'fixings': 5192, 'interest': 5193, 'pretentious': 5194, 'ballooned': 5195, 'trend': 5196, 'wilted': 5197, 'worsts': 5198, 'powerfully': 5199, 'gotten': 5200, 'unpolished': 5201, 'staffs': 5202, 'masala': 5203, 'mackerel': 5204, 'crackles': 5205, 'workers': 5206, 'verde': 5207, 'kfc': 5208, 'porridge': 5209, 'sliding': 5210, 'linguini': 5211, 'palate': 5212, 'that': 5213, \"mogador's\": 5214, 'despicable': 5215, 'cast': 5216, 'sugarcane': 5217, 'u': 5218, 'standout': 5219, 'relaxed': 5220, 'beautifull': 5221, '3am': 5222, 'celery': 5223, 'abbondanza': 5224, 'hazelnut': 5225, 'melon': 5226, 'lechon': 5227, 'preferred': 5228, 'silently': 5229, 'otherwise': 5230, 'posing': 5231, 'stacks': 5232, \"'hungry\": 5233, 'peek': 5234, 'mules': 5235, 'read': 5236, 'sounded': 5237, 'settling': 5238, 'cerebral': 5239, 'calamar': 5240, 'jg': 5241, 'tasty': 5242, '110': 5243, '71': 5244, 'chunks': 5245, 'attempting': 5246, 'oversight': 5247, 'imaginable': 5248, 'gruff': 5249, 'naan': 5250, 'karaoke': 5251, 'horror': 5252, 'water': 5253, 'magazine': 5254, 'carnegie': 5255, 'quibbles': 5256, 'rings': 5257, 'smith': 5258, 'throughout': 5259, 'impeccible': 5260, 'primary': 5261, 'inconsistant': 5262, 'beige': 5263, 'treats': 5264, 'suzanne': 5265, 'offers': 5266, 'bouillabaisse': 5267, 'mixes': 5268, 'pure': 5269, 'pleas': 5270, 'stocks': 5271, 'tableclothed': 5272, 'disappointed': 5273, 'served': 5274, 'broadway': 5275, 'mimosas': 5276, 'ugly': 5277, 'pressed': 5278, 'apologies': 5279, '35': 5280, 'creations': 5281, 'striped': 5282, 'hangout': 5283, 'noisy': 5284, 'celebrated': 5285, 'lokie': 5286, 'occasionally': 5287, 'alternates': 5288, 'sequoias': 5289, \"square's\": 5290, 'horrible': 5291, 'liked': 5292, 'greeks': 5293, 'ignored': 5294, 'roses': 5295, 'mumbles': 5296, 'prompt': 5297, 'twist': 5298, 'ideal': 5299, 'hopes': 5300, 'celebrities': 5301, 'radio': 5302, 'brocoli': 5303, 'fabulosity': 5304, 'usda': 5305, 'industry': 5306, \"hell's\": 5307, 'orso': 5308, 'syrup': 5309, 'during': 5310, 'indicated': 5311, 'hanger': 5312, 'capable': 5313, 'worker': 5314, 'cheap': 5315, 'feet': 5316, 'decision': 5317, 'devoid': 5318, 'refills': 5319, 'businessman': 5320, 'vanilla': 5321, '70': 5322, 'artery': 5323, 'justify': 5324, 'annoyances': 5325, 'nicer': 5326, 'spenders': 5327, 'pickled': 5328, 'extended': 5329, 'super': 5330, 'succulent': 5331, 'shucker': 5332, 'foi': 5333, 'within': 5334, 'empowered': 5335, 'train': 5336, 'musicians': 5337, 'ostentatiously': 5338, 'slice': 5339, 'professionalism': 5340, \"lombardi's\": 5341, 'edges': 5342, 'explosion': 5343, 'storefront': 5344, 'dollop': 5345, 'minimun': 5346, '4': 5347, 'vegetarian': 5348, 'clarify': 5349, 'bodega': 5350, 'quiete': 5351, 'hassled': 5352, 'tooo': 5353, 'venice': 5354, 'believe': 5355, \"who's\": 5356, 'tapas': 5357, 'kinks': 5358, '40': 5359, 'reputation': 5360, 'experimental': 5361, 'kobe': 5362, 'esperanto': 5363, 'caps': 5364, 'paint': 5365, 'dinner': 5366, 'basement': 5367, 'jumbo': 5368, 'reflect': 5369, 'mojito': 5370, 'hangover': 5371, 'thyme': 5372, 'meditating': 5373, 'array': 5374, 'mein': 5375, 'toward': 5376, 'arms': 5377, 'colors': 5378, 'grilled': 5379, 'informed': 5380, 'brand': 5381, 'empty': 5382, 'versions': 5383, 'zinc': 5384, 'jean': 5385, 'yymv': 5386, 'remarkably': 5387, 'sirlion': 5388, 'clam': 5389, 'lighter': 5390, 'bodyguard': 5391, 'drinking': 5392, 'seems': 5393, 'ho': 5394, 'hum': 5395, 'whimsical': 5396, \"staff's\": 5397, 'composed': 5398, 'chief': 5399, 'consistent': 5400, 'sf': 5401, 'aisles': 5402, 'lair': 5403, 'pierogis': 5404, 'disappear': 5405, 'bottles': 5406, 'bumpy': 5407, 'upside': 5408, 'mins': 5409, 'truffle': 5410, 'sixth': 5411, 'where': 5412, 'unoriginal': 5413, 'employed': 5414, 'request': 5415, 'ultimately': 5416, 'walk': 5417, 'confirm': 5418, 'picked': 5419, 'insight': 5420, 'characteristic': 5421, 'dressed': 5422, 'huuge': 5423, 'hasty': 5424, 'combo': 5425, 'conch': 5426, 'trashy': 5427, 'fattened': 5428, 'belgium': 5429, 'lcb': 5430, 'sauteed': 5431, 'suprising': 5432, 'prints': 5433, 'insipid': 5434, 'fritter': 5435, 'serveral': 5436, 'whisking': 5437, 'stinks': 5438, 'monkfish': 5439, 'raucous': 5440, 'dreafully': 5441, 'spartan': 5442, 'murals': 5443, 'bq': 5444, 'potter': 5445, 'uncooked': 5446, 'subterranean': 5447, 'because': 5448, 'planning': 5449, 'many': 5450, 'ocean': 5451, 'fire': 5452, 'gall': 5453, 'morsel': 5454, 'managers': 5455, 'haphazardly': 5456, 'suprise': 5457, 'gristly': 5458, 'steam': 5459, 'stream': 5460, 'weve': 5461, 'jamon': 5462, 'toes': 5463, 'come': 5464, 'cole': 5465, 'worth': 5466, 'singers': 5467, 'tortilla': 5468, 'damp': 5469, 'reception': 5470, 'vegetarians': 5471, 'bleecker': 5472, 'plated': 5473, 'wide': 5474, 'vallenciana': 5475, 'backyard': 5476, 'list': 5477, 'portugues': 5478, 'dreaming': 5479, 'stock': 5480, 'executive': 5481, 'understand': 5482, 'overwhelming': 5483, 'despite': 5484, 'standing': 5485, 'fritters': 5486, 'overpriced': 5487, 'multiple': 5488, 'fortunate': 5489, 'song': 5490, 'patron': 5491, 'international': 5492, 'hong': 5493, 'yuca': 5494, 'crisp': 5495, 'expose': 5496, 'parmesan': 5497, 'mi': 5498, '400': 5499, 'reflected': 5500, 'gratuitis': 5501, 'fridays': 5502, 'paying': 5503, \"yokocho's\": 5504, 'handy': 5505, 'painless': 5506, 'somethings': 5507, 'number': 5508, 'passionless': 5509, 'tung': 5510, 'sight': 5511, 'loud': 5512, 'stuffed': 5513, 'marry': 5514, 'complain': 5515, 'drawing': 5516, 'fancier': 5517, 'tastiest': 5518, 'screw': 5519, 'creamy': 5520, 'dwarfs': 5521, 'perception': 5522, 'his': 5523, 'lingering': 5524, 'shrimp': 5525, \"friend's\": 5526, 'patina': 5527, 'step': 5528, 'ably': 5529, 'forgoes': 5530, 'lame': 5531, 'marnier': 5532, 'mars': 5533, 'raviolis': 5534, 'idiots': 5535, 'also': 5536, 'momentum': 5537, 'citysearch': 5538, 'beyond': 5539, 'knelt': 5540, 'dallas': 5541, '03': 5542, 'poached': 5543, 'handbags': 5544, 'bread': 5545, 'eggs': 5546, 'owner': 5547, 'eight': 5548, 'mains': 5549, 'sending': 5550, 'arguing': 5551, 'arriving': 5552, 'tostada': 5553, 'popped': 5554, '50pm': 5555, 'knowledgable': 5556, 'wind': 5557, 'bon': 5558, 'tofu': 5559, 'heartland': 5560, 'rated': 5561, 'fashion': 5562, 'latest': 5563, 'belleville': 5564, 'tibs': 5565, 'time': 5566, 'thumping': 5567, 'sized': 5568, 'amuse': 5569, 'nipple': 5570, 'poultry': 5571, '101': 5572, 'gloss': 5573, 'witness': 5574, 'daughters': 5575, 'greatly': 5576, 'unisex': 5577, 'brioche': 5578, 'dropping': 5579, 'gummi': 5580, 'toppings': 5581, '80pp': 5582, 'eggplant': 5583, 'raku': 5584, 'tail': 5585, 'unlimited': 5586, 'apologetic': 5587, 'dashed': 5588, 'standbys': 5589, 'price': 5590, 'became': 5591, 'napoli': 5592, 'vegas': 5593, 'identify': 5594, 'pm': 5595, 'cheers': 5596, 'newcomers': 5597, 'choose': 5598, 'ribeye': 5599, 'argentinian': 5600, 'arezzo': 5601, 'shop': 5602, \"fishermen's\": 5603, 'enlivens': 5604, 'primi': 5605, 'pancit': 5606, 'lamp': 5607, 'curb': 5608, 'forest': 5609, 'blackened': 5610, 'waters': 5611, 'sat': 5612, 'y': 5613, 'yuckity': 5614, 'sale': 5615, 'gift': 5616, 'encompasses': 5617, 'veneer': 5618, 'schnitzel': 5619, 'stifling': 5620, 'shocked': 5621, 'atkiners': 5622, 'work': 5623, 'po': 5624, 'beautiful': 5625, 'nutty': 5626, 'fru': 5627, \"york's\": 5628, 'leg': 5629, 'performance': 5630, '75': 5631, 'another': 5632, 'unadorned': 5633, 'abyssmal': 5634, 'octopus': 5635, 'unbelievably': 5636, 'follow': 5637, 'shell': 5638, 'offered': 5639, 'sweets': 5640, 'leaving': 5641, 'thwarted': 5642, 'comforting': 5643, 'substitue': 5644, 'flor': 5645, 'reach': 5646, 'slivered': 5647, 'requiring': 5648, 'slabs': 5649, 'armed': 5650, 'caper': 5651, 'greetings': 5652, \"music's\": 5653, 'scurrying': 5654, 'tends': 5655, 'camp': 5656, 'undeniable': 5657, 'appreciate': 5658, 'sword': 5659, 'pasty': 5660, 'secret': 5661, 'th': 5662, 'jelly': 5663, 'capped': 5664, 'receives': 5665, 'inconveniencing': 5666, 'whole': 5667, 'cucumber': 5668, 'created': 5669, 'genial': 5670, 'cookbook': 5671, 'topper': 5672, 'model': 5673, 'waterview': 5674, 'pleasent': 5675, 'rapidly': 5676, 'bliss': 5677, 'cd': 5678, 'complementary': 5679, 'bleaching': 5680, 'middling': 5681, 'season': 5682, 'styles': 5683, 'towards': 5684, 'aged': 5685, 'trying': 5686, \"person's\": 5687, 'accidently': 5688, 'linger': 5689, 'labor': 5690, 'pumpin': 5691, 'vegearian': 5692, 'trick': 5693, 'meaningful': 5694, 'basically': 5695, 'anymore': 5696, 'ledge': 5697, 'bac': 5698, 'offerings': 5699, 'favorites': 5700, 'easygoing': 5701, 'parsley': 5702, 'evening': 5703, 'heroic': 5704, 'getting': 5705, 'unmercifully': 5706, 'fillet': 5707, 'antiques': 5708, 'croutons': 5709, 'mcnally': 5710, 'engagement': 5711, 'mignon': 5712, 'zillion': 5713, 'romantic': 5714, 'due': 5715, 'bourbon': 5716, 'begged': 5717, 'march': 5718, 'palates': 5719, 'murray': 5720, 'toasted': 5721, 'alternatives': 5722, 'noted': 5723, 'scone': 5724, 'upstate': 5725, 'nurse': 5726, 'arepas': 5727, 'paladar': 5728, 'audacity': 5729, 'avacado': 5730, 'attempt': 5731, 'southeast': 5732, 'clothed': 5733, 'jackson': 5734, 'helped': 5735, 'madeleine': 5736, 'fished': 5737, 'surroundings': 5738, 'amped': 5739, 'quick': 5740, 'confronting': 5741, 'promethean': 5742, 'f': 5743, 'extensive': 5744, 'condition': 5745, 'floating': 5746, 'obtrusive': 5747, 'pass': 5748, 'gathered': 5749, 'porportions': 5750, 'sitdown': 5751, \"'d\": 5752, 'nautical': 5753, 'cardamom': 5754, 'entertaining': 5755, 'quarter': 5756, 'downer': 5757, 'easy': 5758, 'hunger': 5759, 'unless': 5760, 'unwarranted': 5761, 'subjective': 5762, 'carbs': 5763, 'poster': 5764, 'sugar': 5765, 'higher': 5766, 'sadly': 5767, 'bearable': 5768, 'blanc': 5769, 'clothes': 5770, 'elmhurst': 5771, 'admitably': 5772, 'boxes': 5773, 'snobbishly': 5774, 'fooled': 5775, 'lost': 5776, 'shredded': 5777, 'harder': 5778, 'dined': 5779, 'couple': 5780, 'guac': 5781, 'snooty': 5782, 'hopelessly': 5783, 'scored': 5784, 'argue': 5785, 'presumptive': 5786, 'mega': 5787, 'choosing': 5788, 'accessories': 5789, 'tok': 5790, 'warning': 5791, 'chinatown': 5792, 'tag': 5793, 'box': 5794, 'obvious': 5795, 'sencere': 5796, 'caked': 5797, 'superb': 5798, 'mouths': 5799, 'recomendation': 5800, 'showing': 5801, 'pastries': 5802, 'inefficiently': 5803, 'plaster': 5804, 'dirtier': 5805, 'atm': 5806, 'explained': 5807, 'scowling': 5808, 'dinners': 5809, 'waterperson': 5810, 'phothe': 5811, 'kunefe': 5812, 'strikes': 5813, 'winners': 5814, 'live': 5815, 'idli': 5816, 'liqueurs': 5817, 'crew': 5818, 'sophisticated': 5819, 'muzak': 5820, 'recommedation': 5821, 'shown': 5822, 'bought': 5823, 'up': 5824, 'warming': 5825, 'partner': 5826, 'pineapple': 5827, 'prune': 5828, 'noon': 5829, 'shark': 5830, 'ny': 5831, 'landmark': 5832, 'watch': 5833, 'bulletproof': 5834, 'disinterested': 5835, 'tagines': 5836, 'homespun': 5837, 'crouched': 5838, 'rancheros': 5839, 'design': 5840, 'blow': 5841, 'ground': 5842, 'stools': 5843, 'vega': 5844, 'ehh': 5845, 'increedible': 5846, 'rearely': 5847, 'wld': 5848, 'gus': 5849, 'apple': 5850, 'target': 5851, 'maker': 5852, 'retreived': 5853, 'exasperating': 5854, 'huevos': 5855, 'pounded': 5856, 'exhaustive': 5857, 'day': 5858, 'appaled': 5859, 'assumed': 5860, 'jimmy': 5861, 'cakebread': 5862, 'noticable': 5863, \"franny's\": 5864, 'nonstop': 5865, 'shots': 5866, 'customers': 5867, 'dozens': 5868, 'inattentive': 5869, 'cavatelli': 5870, 'liquid': 5871, 'home': 5872, 'fashionable': 5873, 'soba': 5874, 'tend': 5875, 'sizzler': 5876, 'behaviour': 5877, 'breadcrumbs': 5878, 'mustard': 5879, 'marinated': 5880, 'families': 5881, 'pepperoncini': 5882, 'eye': 5883, 'shrooms': 5884, 'sprawling': 5885, 'whose': 5886, 'sweetbreads': 5887, 'encounter': 5888, 'smallest': 5889, 'columns': 5890, 'avocado': 5891, 'alas': 5892, 'chairs': 5893, 'unbeatable': 5894, 'concierge': 5895, 'ambience': 5896, 'blamed': 5897, 'away': 5898, 'preety': 5899, 'reccomendation': 5900, 'combines': 5901, 'encouraging': 5902, 'promptly': 5903, 'signaled': 5904, 'ready': 5905, 'plans': 5906, 'sprouts': 5907, 'dismissed': 5908, 'matter': 5909, 'actors': 5910, 'personal': 5911, 'gray': 5912, 'seat': 5913, 'roasted': 5914, 'freezers': 5915, 'sesame': 5916, 'shows': 5917, 'chunk': 5918, 'swaths': 5919, 'seperate': 5920, 'pennsylvania': 5921, 'energy': 5922, 'ice': 5923, 'kosher': 5924, '100': 5925, 'suffer': 5926, 'hollondaise': 5927, 'calimari': 5928, 'squash': 5929, 'ceases': 5930, 'amounts': 5931, 'tossed': 5932, 'pilled': 5933, 'earn': 5934, 'demeanor': 5935, 'exploded': 5936, 'sip': 5937, 'note': 5938, 'fry': 5939, 'played': 5940, 'pisco': 5941, 'goes': 5942, 'changed': 5943, 'prepare': 5944, 'wood': 5945, 'nobody': 5946, 'manhattan': 5947, 'halibut': 5948, 'outdoor': 5949, 'tapestries': 5950, 'faces': 5951, 'temperture': 5952, 'engaging': 5953, 'winning': 5954, 'sincere': 5955, 'defensive': 5956, 'adaquate': 5957, 'shareable': 5958, 'stand': 5959, 'mentally': 5960, '26': 5961, 'challah': 5962, 'kaiseki': 5963, 'ise': 5964, 'bursts': 5965, 'demanded': 5966, 'ps': 5967, 'bait': 5968, 'beneath': 5969, 'excellen': 5970, '63': 5971, 'god': 5972, 'torro': 5973, 'permits': 5974, 'impossible': 5975, 'boards': 5976, 'giovanni': 5977, 'register': 5978, 'frazzled': 5979, 'corn': 5980, 'escalators': 5981, 'holds': 5982, 'remedied': 5983, 'has': 5984, 'ca': 5985, 'comparable': 5986, 'man': 5987, 'greets': 5988, 'somehow': 5989, 'liberal': 5990, 'essentially': 5991, 'start': 5992, 'natives': 5993, 'ices': 5994, 'apologizing': 5995, 'parked': 5996, 'irritated': 5997, 'exploration': 5998, 'wings': 5999, 'waving': 6000, 'unusually': 6001, 'busline': 6002, 'chop': 6003, 'depicts': 6004, 'walls': 6005, 'dipping': 6006, 'ceasar': 6007, 'smeared': 6008, 'healthful': 6009, \"ping's\": 6010, 'lusciously': 6011, 'entire': 6012, 'hotness': 6013, 'influenced': 6014, 'replete': 6015, 'with': 6016, 'was': 6017, 'salsa': 6018, 'appeared': 6019, 'stocked': 6020, 'party': 6021, 'anxious': 6022, 'sightings': 6023, 'describes': 6024, 'terrific': 6025, 'advertising': 6026, 'claws': 6027, 'skins': 6028, 'savor': 6029, 'unclean': 6030, 'admission': 6031, 'dancing': 6032, 'ka': 6033, 'frieeees': 6034, \"girlfriend's\": 6035, 'intriguing': 6036, 'pizza': 6037, 'commands': 6038, 'nan': 6039, 'votives': 6040, 'kills': 6041, 'bailed': 6042, 'helper': 6043, 'bundle': 6044, 'flirting': 6045, 'suggested': 6046, 'cleanse': 6047, 'competitor': 6048, 'enviornment': 6049, 'carb': 6050, 'mislead': 6051, 'aware': 6052, 'potion': 6053, 'whim': 6054, \"'\": 6055, 'appropriately': 6056, 'peruvian': 6057, \"'cue\": 6058, 'osso': 6059, 'amateurish': 6060, 'save': 6061, 'favorable': 6062, 'consists': 6063, \"grannie's\": 6064, 'walked': 6065, 'dictionary': 6066, \"sacramone's\": 6067, 'heroes': 6068, 'guastavinos': 6069, 'waxman': 6070, 'recived': 6071, 'between': 6072, 'walking': 6073, 'overfill': 6074, 'yet': 6075, 'substitutions': 6076, 'multi': 6077, 'snackers': 6078, 'impatience': 6079, 'students': 6080, 'cheescake': 6081, 'buffer': 6082, 'casually': 6083, 'samba': 6084, 'worry': 6085, 'met': 6086, 'spite': 6087, 'chopstick': 6088, 'carts': 6089, 'adobo': 6090, 'dissappoint': 6091, 'combine': 6092, 'palm': 6093, \"melissa's\": 6094, 'busy': 6095, 'fires': 6096, 'bathrooms': 6097, 'return': 6098, 'sardines': 6099, 'sense': 6100, 'tikka': 6101, 'xcept': 6102, 'stray': 6103, 'phones': 6104, 'trish': 6105, 'studiously': 6106, 'subs': 6107, 'rating': 6108, 'sort': 6109, 'luluc': 6110, 'sevice': 6111, 'replenishing': 6112, 'sinewy': 6113, 'expertly': 6114, 'emerges': 6115, 'honeycomb': 6116, 'eloquently': 6117, 'let': 6118, 'sqc': 6119, 'alittle': 6120, 'promplty': 6121, 'boon': 6122, 'meatpacking': 6123, 'unquestionably': 6124, 'beats': 6125, 'berate': 6126, '17': 6127, 'buy': 6128, 'beat': 6129, 'unfortunately': 6130, 'noticing': 6131, 'generic': 6132, 'honestly': 6133, 'unlikely': 6134, 'wks': 6135, 'calls': 6136, 'sommelier': 6137, 'desk': 6138, 'cutlet': 6139, 'beside': 6140, 'escargot': 6141, 'dampened': 6142, 'spaghetti': 6143, 'sumptuous': 6144, 'grease': 6145, 'arroz': 6146, 'al': 6147, 'popcorn': 6148, 'leafy': 6149, 'confusion': 6150, 'unexpected': 6151, 'dance': 6152, 'windowed': 6153, 'wine': 6154, \"everywhere'\": 6155, 'teeth': 6156, 'herbs': 6157, 'item': 6158, 'yummy': 6159, 'runs': 6160, 'license': 6161, 'cleaning': 6162, 'vary': 6163, 'bartender': 6164, 'cous': 6165, 'l': 6166, 'informal': 6167, 'plunks': 6168, 'unacceptable': 6169, 'unprepared': 6170, 'dominant': 6171, 'intolerable': 6172, 'teenie': 6173, \"cappuccino's\": 6174, 'excess': 6175, 'costly': 6176, 'tomatos': 6177, 'shuttle': 6178, 'trends': 6179, 'purchased': 6180, 'umiliate': 6181, '146': 6182, 'counters': 6183, 'practicly': 6184, 'perk': 6185, 'course': 6186, 'individual': 6187, 'scungilli': 6188, 'life': 6189, 'resented': 6190, 'calamari': 6191, 'bucolic': 6192, '3rd': 6193, 'hudson': 6194, 'chapeau': 6195, 'outside': 6196, 'laska': 6197, 'inscriptions': 6198, 'pongal': 6199, 'normally': 6200, 'cancel': 6201, 'hidden': 6202, 'scream': 6203, 'rushing': 6204, 'conceptually': 6205, 'event': 6206, 'doesnt': 6207, 'carefully': 6208, 'fruit': 6209, 'sashimi': 6210, 'components': 6211, 'above': 6212, 'mood': 6213, 'putting': 6214, 'privacy': 6215, 'thirty': 6216, \"sakatini's\": 6217, 'nervous': 6218, 'sushisamba': 6219, 'hostile': 6220, 'portuguese': 6221, 'signature': 6222, 'javi': 6223, 'flow': 6224, 'blank': 6225, 'salmon': 6226, 'aiter': 6227, 'partly': 6228, 'soggy': 6229, 'thereby': 6230, 'kale': 6231, 'designer': 6232, 'bastion': 6233, 'worked': 6234, 'perfectly': 6235, 'locals': 6236, 'blizzard': 6237, 'girlfriend': 6238, 'goose': 6239, 'pity': 6240, 'vividly': 6241, 'lamd': 6242, 'significantly': 6243, 'adventure': 6244, \"you'll\": 6245, 'move': 6246, 'whipped': 6247, 'drawback': 6248, 'filled': 6249, 'fifth': 6250, 'sensible': 6251, 'nerves': 6252, 'men': 6253, 'bastianich': 6254, 'odd': 6255, 'matters': 6256, 'guide': 6257, 'container': 6258, 'website': 6259, 'refused': 6260, 'blanketing': 6261, 'costs': 6262, 'want': 6263, 'hopping': 6264, 'environs': 6265, 'fava': 6266, 'situated': 6267, 'biscuits': 6268, 'dominate': 6269, 'shacks': 6270, 'fondu': 6271, 'question': 6272, 'obstained': 6273, 'wast': 6274, 'ignoring': 6275, 'thought': 6276, 'triple': 6277, 'cookie': 6278, 'range': 6279, 'frito': 6280, 'legendary': 6281, 'huddled': 6282, 'additionally': 6283, 'innappropriate': 6284, 'bagle': 6285, 'writing': 6286, 'stating': 6287, 'clashing': 6288, 'afterward': 6289, 'masses': 6290, 'mutton': 6291, 'rudely': 6292, 'starting': 6293, 'push': 6294, 'gates': 6295, 'own': 6296, 'handwritten': 6297, 'backroom': 6298, 'mr': 6299, 'employes': 6300, 'mouth': 6301, \"plated'\": 6302, 'rude': 6303, 'paired': 6304, 'some': 6305, 'shoot': 6306, 'spastic': 6307, 'sausage': 6308, 'humble': 6309, \"kids'\": 6310, 'used': 6311, 'snug': 6312, 'drunk': 6313, 'does': 6314, 'near': 6315, 'rounds': 6316, 'tea': 6317, 'softshell': 6318, 'dissatisified': 6319, 'prices': 6320, 'elaine': 6321, 'bt': 6322, 'booths': 6323, 'everyting': 6324, 'en': 6325, 'cook': 6326, 'ceramic': 6327, 'stickers': 6328, 'roma': 6329, 'sweeping': 6330, 'spell': 6331, 'celebratory': 6332, 'infront': 6333, 'refute': 6334, 'kebabs': 6335, 'matched': 6336, 'grass': 6337, 'covering': 6338, '2003': 6339, 'inauthentic': 6340, 'dumped': 6341, 'highest': 6342, 'fears': 6343, 'stomach': 6344, 'mainly': 6345, 'lines': 6346, 'order': 6347, \"weren't\": 6348, 'lake': 6349, 'buttermilk': 6350, \"denny's\": 6351, 'shaking': 6352, 'gracious': 6353, 'set': 6354, 'expert': 6355, 'scooped': 6356, 'vintages': 6357, 'sighs': 6358, 'real': 6359, 'inquiring': 6360, 'overcharged': 6361, 'compatible': 6362, 'homestyle': 6363, 'guinness': 6364, 'holding': 6365, 'adams': 6366, 'quesadillas': 6367, 'rosemary': 6368, 'waffles': 6369, 'memorial': 6370, 'see': 6371, 'worcestershire': 6372, 'basis': 6373, 'crab': 6374, 'weeks': 6375, 'casien': 6376, 'moves': 6377, 'georgous': 6378, 'pies': 6379, 'tiramisu': 6380, 'boy': 6381, 'works': 6382, 'dive': 6383, 'stuffy': 6384, 'boost': 6385, 'canyon': 6386, 'dear': 6387, 'tubes': 6388, 'thoroughfare': 6389, 'simpler': 6390, 'applies': 6391, \"everyone's\": 6392, 'expansive': 6393, 'lemongrass': 6394, 'pickles': 6395, 'tremendously': 6396, 'rocco': 6397, 'jammed': 6398, 'lastly': 6399, 'appertizers': 6400, 'booth': 6401, 'hint': 6402, 'slow': 6403, 'colder': 6404, 'leathery': 6405, 'berries': 6406, 'mild': 6407, 'diminutive': 6408, 'sopping': 6409, 'portioned': 6410, 'splurge': 6411, 'chickpeas': 6412, 'movie': 6413, 'drained': 6414, 'relax': 6415, 'overbearing': 6416, 'action': 6417, 'soil': 6418, 'prevented': 6419, 'martinis': 6420, 'belhaven': 6421, 'corcked': 6422, 'scarfing': 6423, '2001': 6424, 'attract': 6425, 'relatives': 6426, 'percentage': 6427, 'cold': 6428, '172': 6429, 'questios': 6430, 'batter': 6431, 'stuffing': 6432, 'enthusiastic': 6433, 'routine': 6434, 'moon': 6435, 'virgin': 6436, 'wonder': 6437, 'everyone': 6438, '7a': 6439, 'earnest': 6440, \"chair's\": 6441, 'unique': 6442, 'furniture': 6443, 'windowless': 6444, 'orrechiete': 6445, 'unpleasant': 6446, 'relaxing': 6447, 'central': 6448, \"telly's\": 6449, 'calle': 6450, 'tomato': 6451, 'indulge': 6452, 'a': 6453, 'sliced': 6454, 'parking': 6455, 'those': 6456, 'eventhough': 6457, 'port': 6458, 'anger': 6459, 'glasses': 6460, 'mayo': 6461, 'trouble': 6462, 'pf': 6463, 'profusely': 6464, 'inexpensive': 6465, 'area': 6466, 'bustelo': 6467, 'gig': 6468, 'sashay': 6469, 'theme': 6470, 'oops': 6471, 'to': 6472, \"you're\": 6473, 'someone': 6474, 'cigarette': 6475, 'interferes': 6476, 'pam': 6477, 'ear': 6478, 'blink': 6479, 'neighboorhood': 6480, 'reasoned': 6481, 'processed': 6482, 'similarly': 6483, 'treat': 6484, 'times': 6485, 'bothered': 6486, 'accompanying': 6487, 'mahi': 6488, 'mason': 6489, '10': 6490, 'sound': 6491, 'preview': 6492, 'standings': 6493, 'tuesday': 6494, 'sausages': 6495, 'reading': 6496, 'watery': 6497, 'stood': 6498, 'nature': 6499, 'top': 6500, 'plantains': 6501, 'pottery': 6502, 'jellow': 6503, 'disappeared': 6504, 'caperberry': 6505, 'counterparts': 6506, 'thier': 6507, 'shade': 6508, 'restaurant': 6509, 'politics': 6510, 'lure': 6511, 'occupying': 6512, 'cup': 6513, 'maritime': 6514, 'accenting': 6515, 'crostini': 6516, 'exquisite': 6517, 'agreed': 6518, \"items'\": 6519, 'bowl': 6520, 'esxperience': 6521, 'mediterreanian': 6522, 'sconces': 6523, 'charming': 6524, 'hung': 6525, 'refreshed': 6526, 'cone': 6527, 'canandian': 6528, 'spanish': 6529, 'switched': 6530, 'lidia': 6531, 'somesuch': 6532, 'selections': 6533, 'oliva': 6534, 'retreated': 6535, 'most': 6536, 'loved': 6537, 'featured': 6538, 'freebees': 6539, 'considering': 6540, 'alright': 6541, 'omelet': 6542, 'suprised': 6543, 'sweating': 6544, 'tiny': 6545, 'star': 6546, 'verylarge': 6547, 'unneccessary': 6548, 'restos': 6549, 'cadet': 6550, '7': 6551, 'thank': 6552, 'stir': 6553, 'thrilled': 6554, 'orderd': 6555, 'eventaully': 6556, 'your': 6557, 'interpretations': 6558, 'interesting': 6559, 'local': 6560, 'story': 6561, 'joking': 6562, 'foreign': 6563, 'yelling': 6564, 'regulars': 6565, 'lined': 6566, 'delievered': 6567, 'ave': 6568, 'mirrored': 6569, 'instantly': 6570, 'pub': 6571, 'deteriorated': 6572, '96th': 6573, 'lap': 6574, \"frisco's\": 6575, 'mustards': 6576, 'turkey': 6577, 'soft': 6578, 'attention': 6579, 'croqueta': 6580, 'afghani': 6581, 'inlcuding': 6582, 'slightly': 6583, \"you've\": 6584, 'sofas': 6585, 'broiled': 6586, 'ordeal': 6587, 'performances': 6588, 'valet': 6589, 'outway': 6590, 'croquetas': 6591, 'unmatched': 6592, 'comping': 6593, \"waiter's\": 6594, 'cocktails': 6595, 'screwed': 6596, 'ours': 6597, 'goin': 6598, 'snails': 6599, 'communicative': 6600, 'valve': 6601, 'nightlife': 6602, 'entirely': 6603, 'recently': 6604, 'mondays': 6605, 'children': 6606, 'contemporary': 6607, 'dominic': 6608, 'operative': 6609, 'cub': 6610, \"ben's\": 6611, 'influences': 6612, 'buyback': 6613, 'fully': 6614, 'ruin': 6615, 'pay': 6616, 'honest': 6617, 'review': 6618, \"tv's\": 6619, 'out': 6620, 'supposed': 6621, \"someone's\": 6622, 'oversized': 6623, 'increadibly': 6624, 'mexican': 6625, 'mid': 6626, 'grief': 6627, 'requires': 6628, 'outdoors': 6629, 'thanked': 6630, 'eg': 6631, 'technique': 6632, 'friend': 6633, 'flickering': 6634, 'trace': 6635, 'potato': 6636, 'sometime': 6637, 'sour': 6638, 'perhaps': 6639, \"tetley's\": 6640, 'translates': 6641, 'disorganised': 6642, 'shelf': 6643, 'midst': 6644, 'gumbo': 6645, 'roasts': 6646, \"hostess'\": 6647, 'mott': 6648, 'ridge': 6649, 'evenings': 6650, 'ignore': 6651, 'nostalgic': 6652, 'available': 6653, 'hurry': 6654, 'watched': 6655, \"he'll\": 6656, 'seven': 6657, 'hollandaise': 6658, 'needless': 6659, 'chutneys': 6660, 'reminded': 6661, 'demands': 6662, 'afraid': 6663, 'heavenly': 6664, \"emperor's\": 6665, 'keeping': 6666, 'schnapps': 6667, 'feta': 6668, 'but': 6669, 'bouncers': 6670, 'spending': 6671, 'overlook': 6672, 'finds': 6673, 'stalked': 6674, 'casino': 6675, 'place': 6676, 'zone': 6677, 'address': 6678, 'inconveniences': 6679, 'wonderland': 6680, 'stinking': 6681, 'cough': 6682, 'continually': 6683, 'scallops': 6684, 'indifferent': 6685, 'yakiniku': 6686, 'expand': 6687, 'beefy': 6688, 'monday': 6689, 'beer': 6690, 'spill': 6691, 'former': 6692, 'arguments': 6693, 'solution': 6694, 'bz': 6695, 'shumai': 6696, 'setting': 6697, 'notoriety': 6698, '5pm': 6699, 'sudsy': 6700, 'hero': 6701, 'gelato': 6702, 'links': 6703, 'usuals': 6704, 'huong': 6705, 'knowing': 6706, 'featuring': 6707, 'grandmothers': 6708, 'krob': 6709, 'catered': 6710, 'croque': 6711, 'screwing': 6712, 'acknowledgement': 6713, 'will': 6714, 'transferred': 6715, 'dancer': 6716, 'snacks': 6717, 'around': 6718, 'slighly': 6719, 'i': 6720, 'shelling': 6721, 'abominable': 6722, 'friendliness': 6723, 'seared': 6724, 'hitting': 6725, 'nightmarishly': 6726, 'slat': 6727, '3': 6728, 'chain': 6729, 'wasnt': 6730, 'flag': 6731, 'door': 6732, 'francisco': 6733, 'satisfyingly': 6734, 'informatively': 6735, 'usualy': 6736, 'corncakes': 6737, 'samosas': 6738, 'crush': 6739, 'dinin': 6740, 'somebody': 6741, 'handedly': 6742, 'mimosa': 6743, 'pretensions': 6744, 'sauces': 6745, 'sir': 6746, 'original': 6747, 'anyhow': 6748, 'fin': 6749, 'naext': 6750, 'nonexistent': 6751, 'discuss': 6752, 'conditioned': 6753, 'booze': 6754, 'ton': 6755, 'improving': 6756, 'size': 6757, 'handed': 6758, '85': 6759, 'unprofessional': 6760, 'christmas': 6761, 'listened': 6762, 'bottled': 6763, 'sparks': 6764, 'finishes': 6765, 'suitable': 6766, 'polenta': 6767, 'everywhere': 6768, 'dipped': 6769, 'lots': 6770, 'quiche': 6771, 'ludicrous': 6772, 'airy': 6773, 'balsamic': 6774, 'clever': 6775, 'battali': 6776, 'enticing': 6777, 'pulls': 6778, 'mesclun': 6779, 'raw': 6780, 'australian': 6781, 'somewhere': 6782, 'crack': 6783, 'callback': 6784, 'exceptionally': 6785, 'zuppa': 6786, 'clientel': 6787, 'hypnotically': 6788, 'definitely': 6789, \"maria's\": 6790, 'brisket': 6791, 'further': 6792, 'bitterness': 6793, 'saw': 6794, 'sauce': 6795, 'unattentive': 6796, 'passable': 6797, 'seating': 6798, \"do'\": 6799, '3sides': 6800, 'setup': 6801, 'romano': 6802, 'agozar': 6803, 'ayce': 6804, 'pdiddy': 6805, 'gimmicky': 6806, 'rather': 6807, 'husband': 6808, 'love': 6809, 'dumplings': 6810, 'scour': 6811, 'gesturing': 6812, 'jalapeno': 6813, 'amzing': 6814, 'cutting': 6815, 'curled': 6816, 'part': 6817, 'earpiece': 6818, 'rooms': 6819, 'tantalizing': 6820, 'pastas': 6821, 'moved': 6822, 'replied': 6823, 'louder': 6824, 'mediocer': 6825, 'pretends': 6826, 'surprised': 6827, 'sufferes': 6828, 'praise': 6829, 'hid': 6830, 'standard': 6831, 'sandwiches': 6832, 'replacements': 6833, 'twice': 6834, 'moment': 6835, 'discover': 6836, 'draft': 6837, 'initiative': 6838, 'regional': 6839, 'peers': 6840, 'hands': 6841, 'proximity': 6842, 'existant': 6843, 'animals': 6844, 'relish': 6845, 'sterile': 6846, 'else': 6847, 'her': 6848, 'apetit': 6849, \"pizza's\": 6850, 'marseille': 6851, 'chart': 6852, 'steamers': 6853, 'opposed': 6854, 'limply': 6855, 'palak': 6856, 'underrated': 6857, 'taking': 6858, 'hued': 6859, 'bacon': 6860, 'disco': 6861, 'more': 6862, 'mediterrean': 6863, 'huff': 6864, 'held': 6865, \"'buffet'\": 6866, \"it'll\": 6867, 'entres': 6868, 'promising': 6869, 'weak': 6870, 'portobello': 6871, 'golden': 6872, 'nevertheless': 6873, 'lettuce': 6874, 'jukebox': 6875, 'blend': 6876, 'teriyaki': 6877, 'selected': 6878, 'brazenly': 6879, '8': 6880, 'luckily': 6881, 'bumped': 6882, 'laughter': 6883, 'hurried': 6884, 'ones': 6885, 'littel': 6886, 'exagerating': 6887, 'frittata': 6888, 'select': 6889, 'whisked': 6890, 'suits': 6891, 'resteraunt': 6892, 'couches': 6893, 'friendliest': 6894, 'strawberries': 6895, 'panelling': 6896, 'scary': 6897, 'until': 6898, 'temperatures': 6899, 'salade': 6900, 'faster': 6901, 'reviewing': 6902, 'cheeseburgers': 6903, 'dinning': 6904, 'casseroles': 6905, 'marshmallows': 6906, 'traffic': 6907, 'piggy': 6908, 'overwhelmed': 6909, 'lime': 6910, 'friends': 6911, 'mekong': 6912, 'similar': 6913, 'countless': 6914, 'lovingly': 6915, 'child': 6916, 'pitcher': 6917, 'cheesesteaks': 6918, 'smiled': 6919, 'interrupted': 6920, 'familiar': 6921, 'atomosphere': 6922, 'sundried': 6923, 'vinaigrette': 6924, 'profecional': 6925, 'smart': 6926, 'luncheonette': 6927, 'litany': 6928, 'finger': 6929, 'phone': 6930, 'improved': 6931, 'educate': 6932, 'mister': 6933, 'visible': 6934, 'warmer': 6935, 'myself': 6936, 'fast': 6937, 'sublime': 6938, 'mojitos': 6939, 'slim': 6940, 'kolonaki': 6941, 'mar': 6942, 'pecorina': 6943, 'crowd': 6944, 'bold': 6945, 'destination': 6946, 'overf': 6947, 'folk': 6948, 'enchilada': 6949, 'tempted': 6950, 'discussion': 6951, 'grandma': 6952, 'plaice': 6953, 'mercer': 6954, 'snubbed': 6955, 'imported': 6956, 'ginger': 6957, 'capers': 6958, 'pushy': 6959, 'risotto': 6960, \"child's\": 6961, 'whale': 6962, \"shula's\": 6963, 'craving': 6964, 'advice': 6965, 'phenomenal': 6966, 'merly': 6967, 'adult': 6968, 'platter': 6969, 'dani': 6970, 'spoil': 6971, 'left': 6972, 'fish': 6973, 'meatballs': 6974, 'arrabiatta': 6975, 'quarters': 6976, 'pastaless': 6977, 'abundant': 6978, 'palms': 6979, 'realize': 6980, 'delish': 6981, 'yang': 6982, 'china': 6983, 'pinpointed': 6984, 'hollered': 6985, 'crock': 6986, 'quicker': 6987, 'instructed': 6988, 'aggravated': 6989, 'unwind': 6990, 'chocolat': 6991, 'cheerfully': 6992, 'opening': 6993, 'sam': 6994, 'warmed': 6995, 'oz': 6996, 'weekends': 6997, 'snapper': 6998, 'inputs': 6999, 'host': 7000, 'talent': 7001, 'prowl': 7002, 'peopl': 7003, 'gilled': 7004, 'cause': 7005, 'chelsea': 7006, 'notch': 7007, 'folks': 7008, 'cant': 7009, 'plates': 7010, 'provided': 7011, 'grew': 7012, 'comes': 7013, 'restuarnt': 7014, 'popular': 7015, 'snotty': 7016, 'weekend': 7017, 'delight': 7018, 'expanded': 7019, 'averting': 7020, 'combinations': 7021, 'gobble': 7022, 'walnut': 7023, 'alien': 7024, 'citrus': 7025, 'him': 7026, 'torreja': 7027, 'continue': 7028, 'loads': 7029, 'thursday': 7030, 'hopefuls': 7031, 'however': 7032, 'lentiles': 7033, 'premium': 7034, 'crystal': 7035, 'lights': 7036, 'forks': 7037, 'abrupt': 7038, 'dress': 7039, 'surrounding': 7040, 'hotel': 7041, 'seriously': 7042, 'evr': 7043, 'common': 7044, 'tenderloin': 7045, 'remind': 7046, 'cascina': 7047, 'jostle': 7048, 'zests': 7049, 'picky': 7050, 'tall': 7051, 'crispo': 7052, 'embarrassment': 7053, 'whence': 7054, 'seldom': 7055, 'key': 7056, 'okay': 7057, 'frequently': 7058, 'intimate': 7059, 'outright': 7060, 'convince': 7061, 'bocadillos': 7062, 'pompous': 7063, 'sundae': 7064, 'samplings': 7065, 'hing': 7066, 'bloody': 7067, 'specific': 7068, 'unuusal': 7069, 'mosaic': 7070, 'row': 7071, 'combos': 7072, 'error': 7073, 'teacup': 7074, 'pish': 7075, 'unaccomidating': 7076, 'drowned': 7077, 'unruly': 7078, 'watercress': 7079, 'aptly': 7080, 'scene': 7081, 'alone': 7082, 'miniscule': 7083, 'nearly': 7084, 'cut': 7085, 'teas': 7086, 'separate': 7087, 'lobster': 7088, 'oven': 7089, 'solicitious': 7090, 'blame': 7091, 'neither': 7092, 'audibly': 7093, 'alternative': 7094, 'unintelligible': 7095, 'cover': 7096, 'breads': 7097, 'minor': 7098, '44': 7099, 'promised': 7100, 'birthday': 7101, 'by': 7102, 'eater': 7103, 'community': 7104, 'unremarkable': 7105, 'comparing': 7106, 'futura': 7107, 'mammoth': 7108, 'count': 7109, 'bway': 7110, 'scratching': 7111, 'quirky': 7112, 'replacement': 7113, 'using': 7114, 'unfriendly': 7115, 'share': 7116, 'unaccomodating': 7117, 'delectable': 7118, 'godot': 7119, 'write': 7120, 'seen': 7121, 'kiwi': 7122, 'take': 7123, 'puff': 7124, \"vt's\": 7125, 'fattening': 7126, 'borsht': 7127, 'hugest': 7128, 'shook': 7129, 'peaceful': 7130, 'barrel': 7131, 'greet': 7132, 'seafood': 7133, 'lamely': 7134, 'reaction': 7135, 'edible': 7136, 'apologize': 7137, \"servers'\": 7138, 'knuckle': 7139, 'museum': 7140, 'finally': 7141, 'tilt': 7142, \"they'd\": 7143, 'helpings': 7144, 'fancy': 7145, 'throws': 7146, 'surprisingly': 7147, 'finesse': 7148, 'chatty': 7149, 'meanwhile': 7150, 'oden': 7151, 'spirits': 7152, 'surely': 7153, 'complained': 7154, 'missing': 7155, 'crazy': 7156, 'made': 7157, 'technically': 7158, \"sarabeth's\": 7159, 'natural': 7160, 'blvd': 7161, 'muster': 7162, 'spot': 7163, 'syrupy': 7164, 'transformation': 7165, 'bags': 7166, 'claustrophobic': 7167, 'took': 7168, \"she's\": 7169, 'girl': 7170, 'id': 7171, 'commercial': 7172, 'mess': 7173, 'mets': 7174, 'importance': 7175, 'along': 7176, 'sparse': 7177, 'access': 7178, 'concoction': 7179, 'suited': 7180, 'hills': 7181, 'news': 7182, 'shavings': 7183, 'suck': 7184, 'kick': 7185, 'frowned': 7186, 'yum': 7187, 'string': 7188, 'litlle': 7189, 'spicey': 7190, 'autographs': 7191, 'highly': 7192, 'scalloped': 7193, 'pastis': 7194, 'put': 7195, 'regretful': 7196, 'oversauced': 7197, 'lower': 7198, 'second': 7199, 'grumpy': 7200, 'citizen': 7201, 'face': 7202, 'protesting': 7203, 'managed': 7204, 'vacate': 7205, 'mediocre': 7206, 'drums': 7207, 'likes': 7208, \"chef's\": 7209, 'dal': 7210, 'especially': 7211, 'pollo': 7212, 'penang': 7213, 'just': 7214, 'doors': 7215, 'brother': 7216, 'occupied': 7217, 'road': 7218, 'brucetta': 7219, 'true': 7220, 'seaweed': 7221, 'cheerful': 7222, 'unorthodox': 7223, 'mushrooms': 7224, 'wooden': 7225, 'flan': 7226, 'pata': 7227, 'rely': 7228, 'courteously': 7229, 'appitizers': 7230, 'highlight': 7231, 'gourmand': 7232, 'comfortably': 7233, 'redding': 7234, 'matinee': 7235, 'nicely': 7236, 'dishes': 7237, 'say': 7238, 'reviews': 7239, 'wirh': 7240, 'vegetable': 7241, 'major': 7242, \"that'\": 7243, 'multicourse': 7244, 'chip': 7245, 'sipping': 7246, 'hamburger': 7247, 'stopper': 7248, 'quote': 7249, 'redo': 7250, 'cavernous': 7251, 'concerned': 7252, 'mash': 7253, 'eternity': 7254, 'adjacent': 7255, 'disappoint': 7256, 'cashews': 7257, 'resumes': 7258, 'hers': 7259, 'shocking': 7260, 'be': 7261, 'complex': 7262, 'hate': 7263, 'seabass': 7264, \"doens't\": 7265, 'inquired': 7266, 'calico': 7267, 'bottomless': 7268, 'cokes': 7269, 'stained': 7270, 'carved': 7271, 'crackle': 7272, \"steakhouse's\": 7273, 'language': 7274, 'yuck': 7275, 'crema': 7276, 'bones': 7277, 'cuts': 7278, 'my': 7279, 'jerked': 7280, 'rod': 7281, 'wrecked': 7282, 'runny': 7283, 'sucker': 7284, \"bruschetta's\": 7285, 'sushi': 7286, 'trench': 7287, 'attempts': 7288, 'exasperated': 7289, 'vivid': 7290, 'wonderful': 7291, 'standouts': 7292, 'excuse': 7293, 'counted': 7294, 'flaky': 7295, 'bacalhau': 7296, 'opt': 7297, 'vp2': 7298, 'recmmended': 7299, 'mumbled': 7300, 'clammy': 7301, 'corking': 7302, 'fall': 7303, 'other': 7304, 'darn': 7305, 'tummy': 7306, 'fondue': 7307, 'adventuresome': 7308, 'creation': 7309, 'ran': 7310, 'aspect': 7311, 'ir': 7312, 'cares': 7313, 'non': 7314, 'kill': 7315, 'brazailian': 7316, 'lose': 7317, 'regard': 7318, 'anythings': 7319, 'fries': 7320, 'effect': 7321, 'hype': 7322, 'advertised': 7323, 'plunk': 7324, 'hoping': 7325, 'juxtaposes': 7326, 'munching': 7327, 'deftly': 7328, 'orangey': 7329, 'poivre': 7330, 'encouraged': 7331, 'scallions': 7332, 'certainly': 7333, 'version': 7334, 'tex': 7335, 'rosina': 7336, \"couldn't\": 7337, 'you': 7338, 'chili': 7339, 'chosen': 7340, 'yell': 7341, 'deceptive': 7342, 'riding': 7343, 'pastitichio': 7344, 'accomidate': 7345, 'fatty': 7346, 'mmmmmmmm': 7347, 'aromatic': 7348, 'margarita': 7349, \"domenico's\": 7350, 'wrap': 7351, 'fans': 7352, 'hr': 7353, 'makeover': 7354, 'patterned': 7355, 'foie': 7356, '185': 7357, 'crabs': 7358, 'instant': 7359, 'matches': 7360, 'worthwhile': 7361, 'lentils': 7362, 'expresso': 7363, 'abysmal': 7364, 'smothered': 7365, 'cc': 7366, 'unreal': 7367, 'leads': 7368, 'sorry': 7369, 'companions': 7370, 'reeeeeeeally': 7371, 'spumoni': 7372, 'hearded': 7373, 'hang': 7374, 'starch': 7375, 'ponder': 7376, 'chilean': 7377, 'costumers': 7378, 'credit': 7379, 'salvadorean': 7380, 'descending': 7381, 'option': 7382, 'burritos': 7383, 'little': 7384, 'figured': 7385, 'untrained': 7386, 'ripped': 7387, \"ruth's\": 7388, 'hipsters': 7389, 'acceptable': 7390, 'stylish': 7391, 'happened': 7392, 'fly': 7393, 'dente': 7394, 'remotely': 7395, 'nifty': 7396, 'wednesday': 7397, 'alta': 7398, 'kangaroo': 7399, 'pieces': 7400, 'herb': 7401, 'filling': 7402, '2pm': 7403, 'mission': 7404, 'mee': 7405, 'gratuity': 7406, 'there': 7407, 'sorbet': 7408, 'repeated': 7409, 'wondered': 7410, 'mine': 7411, 'papers': 7412, 'surf': 7413, 'blockheads': 7414, 'significant': 7415, 'allergic': 7416, 'visited': 7417, 'cilantro': 7418, 'chewey': 7419, 'chevre': 7420, 'refuse': 7421, 'scrunching': 7422, 'backs': 7423, 'way': 7424, 'balls': 7425, 'mistakes': 7426, 'ticket': 7427, 'splendid': 7428, 'detracted': 7429, 'flowers': 7430, 'brew': 7431, 'peak': 7432, 'firm': 7433, 'dug': 7434, 'arrival': 7435, 'lack': 7436, 'coat': 7437, 'showcasing': 7438, 'burgers': 7439, 'edge': 7440, 'dolmades': 7441, 'eager': 7442, 'mellow': 7443, 'fundido': 7444, 'onion': 7445, 'delights': 7446, 'glowing': 7447, 'rocked': 7448, 'fanitac': 7449, 'bucco': 7450, 'rolling': 7451, 'stop': 7452, 'particularly': 7453, 'decent': 7454, 'wow': 7455, 'outsiside': 7456, 'mary': 7457, 'housemade': 7458, 'better': 7459, 'university': 7460, 'conveniently': 7461, 'wont': 7462, 'rewarded': 7463, 'chives': 7464, 'lasted': 7465, 'unbearable': 7466, 'rules': 7467, 'paper': 7468, 'latter': 7469, 'awesome': 7470, 'gods': 7471, 'rip': 7472, 'eastern': 7473, 'strength': 7474, 'wondering': 7475, 'definitley': 7476, 'tinned': 7477, 'rendered': 7478, 'pot': 7479, 'dishwasher': 7480, 'utensil': 7481, \"here's\": 7482, 'reduction': 7483, 'augmented': 7484, 'hummous': 7485, 'root': 7486, 'madison': 7487, 'sport': 7488, 'roofdeck': 7489, 'topping': 7490, 'social': 7491, 'ac': 7492, 'rialto': 7493, 'advance': 7494, 'overflowing': 7495, 'saying': 7496, 'meetless': 7497, 'tostones': 7498, 'atkins': 7499, 'delivers': 7500, '5x': 7501, 'fell': 7502, 'tequila': 7503, 'complaint': 7504, 'gruner': 7505, 'responsive': 7506, 'headed': 7507, 'pureed': 7508, 'ounce': 7509, 'scandalously': 7510, 'runner': 7511, 'marscapone': 7512, 'youll': 7513, 'look': 7514, 'avenue': 7515, 'margaritas': 7516, 'atichoke': 7517, 'elderly': 7518, 'letdowns': 7519, 'humming': 7520, 'amends': 7521, 'preposterous': 7522, 'remained': 7523, 'varieties': 7524, 'comfort': 7525, 'notches': 7526, 'merits': 7527, 'various': 7528, 'fixing': 7529, 'refilled': 7530, 'ease': 7531, 'refreshing': 7532, 'oversea': 7533, 'listen': 7534, 'ginza': 7535, 'selling': 7536, 'improve': 7537, 'increasing': 7538, \"mcdonald's\": 7539, 'shoulder': 7540, 'prissy': 7541, 'fishsticks': 7542, 'calorie': 7543, 'emphasize': 7544, 'grow': 7545, 'kielbasa': 7546, 'finery': 7547, 'garnishes': 7548, 'fishy': 7549, 'hors': 7550, 'faultlessly': 7551, 'angry': 7552, 'lemony': 7553, 'noticed': 7554, 'promises': 7555, 'public': 7556, 'haphazard': 7557, 'empanas': 7558, 'travelled': 7559, 'famous': 7560, 'failure': 7561, 'realized': 7562, 'sucks': 7563, 'vastly': 7564, 'coca': 7565, \"risotto's\": 7566, 'cousin': 7567, 'clad': 7568, 'small': 7569, 'full': 7570, 'mantoo': 7571, 'etc': 7572, 'prix': 7573, 'whatever': 7574, 'negro': 7575, \"'side'\": 7576, 'mills': 7577, 'stopping': 7578, 'tented': 7579, 'reeses': 7580, 'bearing': 7581, 'sent': 7582, 'homefires': 7583, 'color': 7584, 'bouncy': 7585, 'furnishings': 7586, 'spinning': 7587, 'baklava': 7588, 'sapprosota': 7589, 'pane': 7590, 'blast': 7591, 'champagne': 7592, 'traditional': 7593, 'revise': 7594, 'drops': 7595, 'recharge': 7596, 'decently': 7597, 'concluded': 7598, 'au': 7599, 'stared': 7600, 'mistake': 7601, 'deliciously': 7602, 'evident': 7603, 'bamboo': 7604, 'service': 7605, 'kifto': 7606, 'runners': 7607, 'andtomato': 7608, 'before': 7609, 'planned': 7610, 'wisked': 7611, 'letters': 7612, 'participate': 7613, 'patios': 7614, 'easily': 7615, 'comfortable': 7616, 'checked': 7617, 'protions': 7618, 'paneling': 7619, 'wars': 7620, 'pomegranate': 7621, 'vinegar': 7622, 'gin': 7623, 'ridiculous': 7624, 'satisfaction': 7625, 'still': 7626, 'years': 7627, 'selection': 7628, 'wins': 7629, 'japan': 7630, 'classy': 7631, 'gives': 7632, 'annoyance': 7633, 'melting': 7634, 'thinnest': 7635, 'team': 7636, 'appetizer': 7637, 'espresso': 7638, 'entre': 7639, 'vegan': 7640, 'tendency': 7641, 'dominates': 7642, 'graphed': 7643, 'aerosmith': 7644, 'brink': 7645, \"tom's\": 7646, 'giving': 7647, 'downhill': 7648, 'downtown': 7649, 'tasting': 7650, 'hangar': 7651, 'napkins': 7652, 'heard': 7653, \"ernie's\": 7654, 'dated': 7655, 'pho': 7656, 'surrounded': 7657, 'whenever': 7658, 'mapquest': 7659, 'maybe': 7660, 'dorm': 7661, 'difference': 7662, 'servings': 7663, 'absurdly': 7664, 'heart': 7665, 'useless': 7666, 'tear': 7667, 'hail': 7668, 'polish': 7669, 'bonus': 7670, 'rudeness': 7671, 'fathers': 7672, 'bundt': 7673, 'drop': 7674, 'seated': 7675, 'imposing': 7676, 'ate': 7677, 'claim': 7678, 'russe': 7679, 'yankee': 7680, 'tables': 7681, 'muffin': 7682, 'fo': 7683, 'rediculous': 7684, 'maine': 7685, 'dragging': 7686, 'personalities': 7687, 'faced': 7688, 'makeshift': 7689, 'ravioli': 7690, 'theatrical': 7691, 'intervene': 7692, 'wrongly': 7693, 'grades': 7694, 'ordering': 7695, 'motif': 7696, 'desides': 7697, 'pea': 7698, 'saag': 7699, 'strong': 7700, 'crispy': 7701, 'much': 7702, 'insanely': 7703, 'julienned': 7704, 'comprehend': 7705, 'bistro': 7706, 'wealthy': 7707, 'inquire': 7708, 'attractive': 7709, 'sunglasses': 7710, 'cherry': 7711, 'el': 7712, 'philly': 7713, 'load': 7714, 'canteena': 7715, 'infused': 7716, 'midway': 7717, \"tabla's\": 7718, 'hand': 7719, 'caf': 7720, 'practical': 7721, 'guessing': 7722, 'awsome': 7723, 'family': 7724, 'tart': 7725, \"they've\": 7726, 'theatre': 7727, 'distinct': 7728, 'dragged': 7729, 'pestering': 7730, 'them': 7731, 'calling': 7732, 'craft': 7733, 'noteworthy': 7734, \"goin'\": 7735, 'bored': 7736, 'glad': 7737, 'jalepeno': 7738, 'fixed': 7739, 'efficient': 7740, 'style': 7741, 'groups': 7742, 'bbq': 7743, 'sparked': 7744, 'eatery': 7745, 'piled': 7746, 'popularity': 7747, 'cleaner': 7748, 'commission': 7749, 'kashmiri': 7750, 'isnt': 7751, 'remarkable': 7752, 'challenged': 7753, 'eases': 7754, 'spooning': 7755, 'togehter': 7756, 'lunches': 7757, 'gain': 7758, 'connotes': 7759, 'ago': 7760, 'quality': 7761, 'prawns': 7762, '3x': 7763, 'run': 7764, 'witnessing': 7765, 'intially': 7766, 'convivial': 7767, \"night's\": 7768, 'lupa': 7769, 'snack': 7770, 'youthful': 7771, 'lie': 7772, 'blankly': 7773, 'likely': 7774, 'tangerine': 7775, 'afterthought': 7776, 'arty': 7777, 'glamourpuss': 7778, 'dip': 7779, 'receive': 7780, 'sol': 7781, 'quinoa': 7782, 'originate': 7783, 'eggplants': 7784, 'herbed': 7785, 'office': 7786, 'fake': 7787, 'consistently': 7788, 'crunchy': 7789, 'convert': 7790, 'starving': 7791, 'desperation': 7792, 'football': 7793, 'scarf': 7794, 'substantial': 7795, 'acknowledge': 7796, 'delightfully': 7797, 'hoist': 7798, 'nicoise': 7799, 'tempura': 7800, 'squid': 7801, 'themselves': 7802, 'terminal': 7803, 'copious': 7804, \"year's\": 7805, 'sandwhich': 7806, 'waited': 7807, 'soy': 7808, 'isolates': 7809, 'and': 7810, 'molecular': 7811, 'flakes': 7812, 'pitchers': 7813, 'meeting': 7814, 'leftovers': 7815, 'provides': 7816, 'disaster': 7817, '20mins': 7818, 'cheesiest': 7819, 'cheapest': 7820, 'persons': 7821, 'words': 7822, 'yu': 7823, 'andouille': 7824, 'amply': 7825, 'pad': 7826, 'finish': 7827, 'canelloni': 7828, 'coney': 7829, 'soothe': 7830, 'breast': 7831, 'coctails': 7832, 'cattle': 7833, 'plataforma': 7834, \"husband's\": 7835, 'gorgeous': 7836, 'stone': 7837, 'adds': 7838, 'forno': 7839, 'act': 7840, 'vein': 7841, 'jam': 7842, 'storage': 7843, 'claimed': 7844, 'fascinating': 7845, 'simple': 7846, 'rotisserie': 7847, 'receptive': 7848, 'muy': 7849, 'rice': 7850, 'garnish': 7851, 'white': 7852, 'piping': 7853, 'might': 7854, 'offering': 7855, 'brainer': 7856, 'hovered': 7857, 'needing': 7858, 'immediately': 7859, 'polyjuice': 7860, 'completely': 7861, 'lava': 7862, 'masters': 7863, 'ten': 7864, 'msg': 7865, 'overdone': 7866, 'wether': 7867, 'artsy': 7868, 'can': 7869, 'reality': 7870, 'entertained': 7871, 'leper': 7872, 'adorn': 7873, 'occasional': 7874, 'overtones': 7875, 'biting': 7876, 'force': 7877, 'cajun': 7878, 'knife': 7879, 'gluttony': 7880, 'system': 7881, 'accomidating': 7882, '57th': 7883, 'under': 7884, 'quite': 7885, 'ability': 7886, 'torn': 7887, 'offshoot': 7888, 'piano': 7889, 'waitresses': 7890, 'turf': 7891, 'crowding': 7892, 'attacking': 7893, 'oily': 7894, 'sandwich': 7895, 'juju': 7896, 'admitted': 7897}\n"
     ]
    }
   ],
   "source": [
    "voc = []\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "for example in train:\n",
    "  text_tokens = text_to_word_sequence(example[0])\n",
    "  aspect_tokens = text_to_word_sequence(example[1])\n",
    "  voc.extend(aspect_tokens)\n",
    "  voc.extend(text_tokens)\n",
    "voc = set(voc)\n",
    "print(len(voc))\n",
    "\n",
    "word_index = dict()\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  \n",
    "word_index[\"<EOS>\"] = 3\n",
    "for w in voc:\n",
    "  word_index[w] = len(word_index)\n",
    "print(len(word_index))\n",
    "\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tvuu4KhStqei"
   },
   "source": [
    "According to the word_index and the tokenizer function (text_to_word_sequence), we can convert the review text and aspect words to word tokens and integers separately:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 649,
     "status": "ok",
     "timestamp": 1647368514404,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "gMCH1OoDrSNR",
    "outputId": "dc1431ef-447b-4ea7-d108-84850b498a74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_review[0]:\n",
      "['the', 'decor', 'is', 'not', 'special', 'at', 'all', 'but', 'their', 'food', 'and', 'amazing', 'prices', 'make', 'up', 'for', 'it']\n",
      "x_train_aspect[0]:\n",
      "['decor']\n",
      "x_train_review_int[0]:\n",
      "[3846, 3616, 3083, 2363, 267, 3708, 4913, 6669, 2271, 2760, 7810, 4690, 6320, 274, 5824, 1030, 3021]\n",
      "x_train_aspect_int[0]:\n",
      "[3616]\n"
     ]
    }
   ],
   "source": [
    "# Please write your code to generate the following data\n",
    "\n",
    "# Create Empty\n",
    "x_train_review = list()\n",
    "x_train_aspect = list()\n",
    "x_train_review_int = list()\n",
    "x_train_aspect_int = list()\n",
    "\n",
    "x_dev_review = list()\n",
    "x_dev_aspect = list()\n",
    "x_dev_review_int = list()\n",
    "x_dev_aspect_int = list()\n",
    "\n",
    "x_test_review = list()\n",
    "x_test_aspect = list()\n",
    "x_test_review_int = list()\n",
    "x_test_aspect_int = list()\n",
    "\n",
    "# your code goes here:\n",
    "    \n",
    "def create_train_dev_test(data,review,aspect,review_int,aspect_int):\n",
    "    \n",
    "    i = 0\n",
    "    for example in data:\n",
    "        #Create review and aspect list using text_to_word_sequence for train,dev and test\n",
    "        review.append(text_to_word_sequence(example[0]))\n",
    "        aspect.append(text_to_word_sequence(example[1]))\n",
    "\n",
    "        #Create the review_int list for all train,dev and text\n",
    "        temp = []\n",
    "        temp = [word_index[word] if word in word_index else word_index[\"<UNK>\"] for word in review[i]]\n",
    "        review_int.append(temp)\n",
    "        \n",
    "        #Create the aspect_int list for all train,dev and text\n",
    "        temp = []\n",
    "        temp = [word_index[aspect] if aspect in word_index else word_index[\"<UNK>\"] for aspect in aspect[i]]\n",
    "        aspect_int.append(temp)\n",
    "        i += 1\n",
    "\n",
    "    return review,aspect,review_int,aspect_int\n",
    "\n",
    "# For Train\n",
    "x_train_review, x_train_aspect, x_train_review_int, x_train_aspect_int = create_train_dev_test(train,x_train_review,\n",
    "                                                                                              x_train_aspect,\n",
    "                                                                                              x_train_review_int,\n",
    "                                                                                              x_train_aspect_int)\n",
    "# For Dev\n",
    "x_dev_review, x_dev_aspect, x_dev_review_int, x_dev_aspect_int = create_train_dev_test(val,x_dev_review,\n",
    "                                                                                        x_dev_aspect,\n",
    "                                                                                        x_dev_review_int,\n",
    "                                                                                        x_dev_aspect_int)\n",
    "\n",
    "# For Test\n",
    "x_test_review, x_test_aspect, x_test_review_int, x_test_aspect_int = create_train_dev_test(test,x_test_review,\n",
    "                                                                                              x_test_aspect,\n",
    "                                                                                              x_test_review_int,\n",
    "                                                                                              x_test_aspect_int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# If use the previous word_index, you can get a print result like:\n",
    "assert len(x_train_aspect) == len(train)\n",
    "assert len(x_train_aspect) == len(x_train_aspect_int)\n",
    "assert len(x_test_aspect) == len(test)\n",
    "assert len(x_test_aspect) == len(x_test_aspect_int)\n",
    "print(\"x_train_review[0]:\")\n",
    "print(x_train_review[0])\n",
    "print(\"x_train_aspect[0]:\")\n",
    "print(x_train_aspect[0])\n",
    "print(\"x_train_review_int[0]:\")\n",
    "print(x_train_review_int[0])\n",
    "print(\"x_train_aspect_int[0]:\")\n",
    "print(x_train_aspect_int[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IreFXgruZot"
   },
   "source": [
    "We use 4 to represent \"positive\", 2 for \"neutral\", and 1 for \"negative\". Then we can convert the lables to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1647368514404,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "abIb7Fe5u3GQ",
    "outputId": "a6d1f0e1-744a-4ed2-b5af-f3876c7627c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1]\n",
      "[1 0 0]\n",
      "[1 0 0]\n",
      "[0 1 0]\n",
      "[0 0 1]\n"
     ]
    }
   ],
   "source": [
    "def label2int(dataset):\n",
    "  y = []\n",
    "  for example in dataset:\n",
    "    if example[2].lower() == \"negative\":\n",
    "      y.append([0,0,1])\n",
    "    elif example[2].lower() == \"neutral\":\n",
    "      y.append([0,1,0])\n",
    "    else:\n",
    "      # assert example[2].lower() == \"positive\"\n",
    "      y.append([1,0,0])\n",
    "  return y\n",
    "  \n",
    "y_train = label2int(train)\n",
    "y_dev = label2int(val)\n",
    "y_test = label2int(test)\n",
    "y_train = np.array(y_train)\n",
    "y_dev = np.array(y_dev)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train[1])\n",
    "print(y_train[2])\n",
    "print(y_train[3])\n",
    "print(y_train[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TnnSuspvC5b"
   },
   "source": [
    "Now we have almost done the data preprocessing. Unlike the previous lab, there are two x (review and aspect) to input the model in here. The easiest way is to combine the review and aspect into one sentence and then input it into the model. Thus we can use the previous model directly.\n",
    "\n",
    "(This means our model is similar to a simplified version of the Vo & Zhang model from the lectures: we have an input sequence containing an aspect embedding paired with the sentence word embeddings (but not separating into left & right sentence context as Vo & Zhang do)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 681,
     "status": "ok",
     "timestamp": 1647368515083,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "nKOiVVXQu-_I",
    "outputId": "b5c434d6-6497-400f-d3a8-e8889203d7f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before paded:\n",
      "[['decor', '<START>', 'the', 'decor', 'is', 'not', 'special', 'at', 'all', 'but', 'their', 'food', 'and', 'amazing', 'prices', 'make', 'up', 'for', 'it'], ['food', '<START>', 'the', 'decor', 'is', 'not', 'special', 'at', 'all', 'but', 'their', 'food', 'and', 'amazing', 'prices', 'make', 'up', 'for', 'it']]\n",
      "[[3616, 1, 3846, 3616, 3083, 2363, 267, 3708, 4913, 6669, 2271, 2760, 7810, 4690, 6320, 274, 5824, 1030, 3021], [2760, 1, 3846, 3616, 3083, 2363, 267, 3708, 4913, 6669, 2271, 2760, 7810, 4690, 6320, 274, 5824, 1030, 3021]]\n",
      "After paded:\n",
      "[3616    1 3846 3616 3083 2363  267 3708 4913 6669 2271 2760 7810 4690\n",
      " 6320  274 5824 1030 3021    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "[2183    1 3203 6453 5780 3187 2183 3846 3893 6720 1856 3846 7088  529\n",
      " 2017  457 7810 3846    2 7801 1007  116 5131    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "[2760    1 3846 2760 6017 5274 5903 6669 3846 4228 2036 2426 3288 4962\n",
      " 4741 3187 5566 6472 3641 3846 4023 7810 3579 1262 4879 2941 4879 5139\n",
      " 7810 2014 4193 2471 5824 3579 2183  358 3203 3288 4022    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "# Please write your code to combine the x_*_review and x_*_aspect into the following varibles\n",
    "# x_train\n",
    "# x_train_int\n",
    "# x_dev\n",
    "# x_dev_int\n",
    "# x_test\n",
    "# x_test_int\n",
    "\n",
    "# your code goes here\n",
    "# Tips: \n",
    "# 1) We can use the special token <START> to concatenate the reviews and aspects.\n",
    "# 2) After combine them, do not foget to pad the sequences.\n",
    "def combine_review_aspect(review,aspect,review_int,aspect_int):\n",
    "    list1 = list()\n",
    "    list2 = list()\n",
    "    \n",
    "    for i in range(len(review)):\n",
    "        temp = []\n",
    "        temp.extend(aspect[i])\n",
    "        temp.append(\"<START>\")\n",
    "        temp.extend(review[i])\n",
    "        list1.append(temp)\n",
    "\n",
    "        temp_int = []\n",
    "        temp_int.extend(aspect_int[i])\n",
    "        temp_int.append(1)\n",
    "        temp_int.extend(review_int[i])\n",
    "        list2.append(temp_int)\n",
    "        \n",
    "    return list1, list2\n",
    "\n",
    "# For Train Data\n",
    "x_train, x_train_int = combine_review_aspect(x_train_review, x_train_aspect, x_train_review_int, x_train_aspect_int)\n",
    "\n",
    "# for Validation Data\n",
    "x_dev, x_dev_int = combine_review_aspect(x_dev_review, x_dev_aspect, x_dev_review_int, x_dev_aspect_int)\n",
    "\n",
    "# For Test Data\n",
    "x_test, x_test_int = combine_review_aspect(x_test_review, x_test_aspect, x_test_review_int, x_test_aspect_int)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad the train data and convert it into numpy array\n",
    "x_train_pad = pad_sequences(x_train_int, value=word_index[\"<PAD>\"], maxlen=128, padding='post')\n",
    "x_train_pad = np.array(x_train_pad)\n",
    "\n",
    "# pad the validation data and convert it into numpy array\n",
    "x_dev_pad = pad_sequences(x_dev_int, value=word_index[\"<PAD>\"], padding='post', maxlen=128)\n",
    "x_dev_pad = np.array(x_dev_pad)\n",
    "\n",
    "# pad the test data and convert it into numpy array\n",
    "x_test_pad = pad_sequences(x_test_int, value=word_index[\"<PAD>\"], padding='post', maxlen=128)\n",
    "x_test_pad = np.array(x_test_pad)\n",
    "\n",
    "# Don't forget the to use np.array function to wrap the ouput of pad_sequences function, such as: x_train_pad = np.array(x_train_pad)\n",
    "# Only pad the *_int varibles\n",
    "print(\"Before paded:\")\n",
    "print(x_train[0:2])\n",
    "print(x_train_int[0:2])\n",
    "print(\"After paded:\")\n",
    "print(x_train_pad[0])\n",
    "print(x_dev_pad[0])\n",
    "print(x_test_pad[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7OwOQw4h8RX"
   },
   "source": [
    "#Model 1: Previous models without pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CuAHDGQ3Mmp"
   },
   "source": [
    "## Model 1-1: Neural bag of words without pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-QzOMO_P4jc"
   },
   "source": [
    "Now we use the model2 in lab4 to deal with our task. However, the previous model works only for the binary classification task. Therefore, we need to modify the output layer to fix the multi-class problem. You can read this tutorial for more details: https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1647368515083,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "Yi04MLIvJOGZ"
   },
   "outputs": [],
   "source": [
    "class GlobalAveragePooling1DMasked(GlobalAveragePooling1D):\n",
    "    def call(self, x, mask=None):\n",
    "        if mask != None:\n",
    "            return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
    "        else:\n",
    "            return super().call(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18878,
     "status": "ok",
     "timestamp": 1647368533959,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "MFrCsL-NBFVL",
    "outputId": "72ae9320-f879-4e0a-908f-92ccddb00a9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 128)]             0         \n",
      "                                                                 \n",
      " embedding_layer (Embedding)  (None, 128, 100)         789800    \n",
      "                                                                 \n",
      " global_average_pooling1d_ma  (None, 100)              0         \n",
      " sked (GlobalAveragePooling1                                     \n",
      " DMasked)                                                        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                1616      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 791,467\n",
      "Trainable params: 791,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "22/22 [==============================] - 5s 23ms/step - loss: 1.0770 - accuracy: 0.4491 - val_loss: 1.0627 - val_accuracy: 0.4535\n",
      "Epoch 2/30\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 1.0624 - accuracy: 0.4507 - val_loss: 1.0587 - val_accuracy: 0.4535\n",
      "Epoch 3/30\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 1.0568 - accuracy: 0.4507 - val_loss: 1.0528 - val_accuracy: 0.4535\n",
      "Epoch 4/30\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 1.0481 - accuracy: 0.4507 - val_loss: 1.0435 - val_accuracy: 0.4535\n",
      "Epoch 5/30\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 1.0350 - accuracy: 0.4515 - val_loss: 1.0293 - val_accuracy: 0.4535\n",
      "Epoch 6/30\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 1.0161 - accuracy: 0.4601 - val_loss: 1.0131 - val_accuracy: 0.4842\n",
      "Epoch 7/30\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.9936 - accuracy: 0.4961 - val_loss: 0.9936 - val_accuracy: 0.4640\n",
      "Epoch 8/30\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.9699 - accuracy: 0.5055 - val_loss: 0.9782 - val_accuracy: 0.4767\n",
      "Epoch 9/30\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.9443 - accuracy: 0.5209 - val_loss: 0.9557 - val_accuracy: 0.5173\n",
      "Epoch 10/30\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.9191 - accuracy: 0.5437 - val_loss: 0.9417 - val_accuracy: 0.5173\n",
      "Epoch 11/30\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.8965 - accuracy: 0.5561 - val_loss: 0.9255 - val_accuracy: 0.5293\n",
      "Epoch 12/30\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.8749 - accuracy: 0.5702 - val_loss: 0.9161 - val_accuracy: 0.5420\n",
      "Epoch 13/30\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.8575 - accuracy: 0.5812 - val_loss: 0.9084 - val_accuracy: 0.5330\n",
      "Epoch 14/30\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.8401 - accuracy: 0.5913 - val_loss: 0.8974 - val_accuracy: 0.5435\n",
      "Epoch 15/30\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.8236 - accuracy: 0.6015 - val_loss: 0.8900 - val_accuracy: 0.5623\n",
      "Epoch 16/30\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.8105 - accuracy: 0.6095 - val_loss: 0.8908 - val_accuracy: 0.5465\n",
      "Epoch 17/30\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7969 - accuracy: 0.6176 - val_loss: 0.8819 - val_accuracy: 0.5623\n",
      "Epoch 18/30\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7848 - accuracy: 0.6273 - val_loss: 0.8810 - val_accuracy: 0.5661\n",
      "Epoch 19/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7728 - accuracy: 0.6320 - val_loss: 0.8818 - val_accuracy: 0.5608\n",
      "Epoch 20/30\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7637 - accuracy: 0.6366 - val_loss: 0.8820 - val_accuracy: 0.5593\n",
      "Epoch 21/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7544 - accuracy: 0.6425 - val_loss: 0.8799 - val_accuracy: 0.5638\n",
      "Epoch 22/30\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7464 - accuracy: 0.6480 - val_loss: 0.8878 - val_accuracy: 0.5646\n",
      "Epoch 23/30\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7377 - accuracy: 0.6541 - val_loss: 0.8836 - val_accuracy: 0.5571\n",
      "Epoch 24/30\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.7285 - accuracy: 0.6587 - val_loss: 0.8875 - val_accuracy: 0.5638\n",
      "Epoch 25/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7213 - accuracy: 0.6639 - val_loss: 0.8880 - val_accuracy: 0.5653\n",
      "Epoch 26/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7140 - accuracy: 0.6645 - val_loss: 0.8918 - val_accuracy: 0.5631\n",
      "Epoch 27/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7075 - accuracy: 0.6674 - val_loss: 0.8980 - val_accuracy: 0.5601\n",
      "Epoch 28/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7012 - accuracy: 0.6725 - val_loss: 0.9033 - val_accuracy: 0.5608\n",
      "Epoch 29/30\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.6941 - accuracy: 0.6766 - val_loss: 0.9044 - val_accuracy: 0.5676\n",
      "Epoch 30/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6890 - accuracy: 0.6792 - val_loss: 0.9173 - val_accuracy: 0.5638\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.9176 - accuracy: 0.5823\n",
      "loss: 0.9175584316253662 accuracy: 0.582335352897644\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "# Tips: The activation function of the output layer is softmax.\n",
    "\n",
    "VOCAB_SIZE = 7898\n",
    "EMBED_SIZE = 100\n",
    "\n",
    "inputs = Input((128,))\n",
    "embedding = Embedding(VOCAB_SIZE, EMBED_SIZE, name='embedding_layer',embeddings_initializer = 'glorot_uniform',\n",
    "                      input_length = 128)(inputs)\n",
    "global_average_pooling1d_masked_10 = GlobalAveragePooling1DMasked()(embedding)\n",
    "Hidden_1 = Dense(16)(global_average_pooling1d_masked_10)\n",
    "Output = Dense(3, input_shape=(16,), activation = 'softmax')(Hidden_1)\n",
    "\n",
    "model11 = Model(inputs=[inputs],outputs=[Output])\n",
    "\n",
    "model11.summary()\n",
    "\n",
    "\n",
    "model11.compile(optimizer='adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history11 = model11.fit(x_train_pad, y_train, epochs=30, batch_size=512, validation_data=(x_dev_pad,y_dev), verbose=1)\n",
    "\n",
    "results11 = model11.evaluate(x_test_pad,y_test)\n",
    "\n",
    "print(\"loss:\",results11[0],\"accuracy:\",results11[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FBpTc_rXGvQ"
   },
   "source": [
    "The accuracy of lab3 model2 in this task is around 46%. If you use the \"glorot_uniform\" initialization method, the accuracy can reach around 55%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iafDTygK28fv"
   },
   "source": [
    "##  Model 1-2: CNN or LSTM without pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2TnuiKb2-vE"
   },
   "source": [
    "Please try one more model (CNN or LSTM) without pre-trained word embeddings in here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10522,
     "status": "ok",
     "timestamp": 1647368544466,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "oXjbq6WcJosQ",
    "outputId": "2310dada-62f7-4fdd-f7ca-5c43a5e62bcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 128)]             0         \n",
      "                                                                 \n",
      " embedding_layer (Embedding)  (None, 128, 100)         789800    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 123, 100)          60100     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 100)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 850,203\n",
      "Trainable params: 850,203\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6842 - accuracy: 0.6814 - val_loss: 0.9159 - val_accuracy: 0.5623\n",
      "Epoch 2/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6789 - accuracy: 0.6850 - val_loss: 0.9205 - val_accuracy: 0.5623\n",
      "Epoch 3/30\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.6721 - accuracy: 0.6920 - val_loss: 0.9258 - val_accuracy: 0.5608\n",
      "Epoch 4/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6674 - accuracy: 0.6934 - val_loss: 0.9360 - val_accuracy: 0.5578\n",
      "Epoch 5/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6638 - accuracy: 0.6951 - val_loss: 0.9398 - val_accuracy: 0.5563\n",
      "Epoch 6/30\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.6563 - accuracy: 0.7013 - val_loss: 0.9480 - val_accuracy: 0.5616\n",
      "Epoch 7/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6513 - accuracy: 0.7059 - val_loss: 0.9593 - val_accuracy: 0.5586\n",
      "Epoch 8/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6450 - accuracy: 0.7097 - val_loss: 0.9619 - val_accuracy: 0.5518\n",
      "Epoch 9/30\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.6406 - accuracy: 0.7095 - val_loss: 0.9699 - val_accuracy: 0.5578\n",
      "Epoch 10/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6381 - accuracy: 0.7124 - val_loss: 0.9780 - val_accuracy: 0.5571\n",
      "Epoch 11/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6332 - accuracy: 0.7157 - val_loss: 0.9880 - val_accuracy: 0.5556\n",
      "Epoch 12/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6270 - accuracy: 0.7178 - val_loss: 0.9964 - val_accuracy: 0.5571\n",
      "Epoch 13/30\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.6222 - accuracy: 0.7206 - val_loss: 1.0107 - val_accuracy: 0.5548\n",
      "Epoch 14/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6181 - accuracy: 0.7267 - val_loss: 1.0130 - val_accuracy: 0.5608\n",
      "Epoch 15/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6142 - accuracy: 0.7279 - val_loss: 1.0234 - val_accuracy: 0.5601\n",
      "Epoch 16/30\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.6088 - accuracy: 0.7281 - val_loss: 1.0320 - val_accuracy: 0.5608\n",
      "Epoch 17/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6061 - accuracy: 0.7284 - val_loss: 1.0442 - val_accuracy: 0.5548\n",
      "Epoch 18/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.6017 - accuracy: 0.7349 - val_loss: 1.0628 - val_accuracy: 0.5608\n",
      "Epoch 19/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5997 - accuracy: 0.7329 - val_loss: 1.0652 - val_accuracy: 0.5571\n",
      "Epoch 20/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5933 - accuracy: 0.7400 - val_loss: 1.0749 - val_accuracy: 0.5608\n",
      "Epoch 21/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5912 - accuracy: 0.7395 - val_loss: 1.0884 - val_accuracy: 0.5578\n",
      "Epoch 22/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5894 - accuracy: 0.7401 - val_loss: 1.1058 - val_accuracy: 0.5503\n",
      "Epoch 23/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5855 - accuracy: 0.7420 - val_loss: 1.1118 - val_accuracy: 0.5541\n",
      "Epoch 24/30\n",
      "22/22 [==============================] - 0s 12ms/step - loss: 0.5788 - accuracy: 0.7496 - val_loss: 1.1251 - val_accuracy: 0.5578\n",
      "Epoch 25/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5756 - accuracy: 0.7508 - val_loss: 1.1330 - val_accuracy: 0.5586\n",
      "Epoch 26/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5728 - accuracy: 0.7526 - val_loss: 1.1423 - val_accuracy: 0.5601\n",
      "Epoch 27/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5720 - accuracy: 0.7527 - val_loss: 1.1741 - val_accuracy: 0.5526\n",
      "Epoch 28/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5680 - accuracy: 0.7535 - val_loss: 1.1663 - val_accuracy: 0.5586\n",
      "Epoch 29/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5642 - accuracy: 0.7571 - val_loss: 1.1859 - val_accuracy: 0.5653\n",
      "Epoch 30/30\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.5622 - accuracy: 0.7576 - val_loss: 1.1959 - val_accuracy: 0.5661\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 1.2465 - accuracy: 0.5681\n",
      "loss: 1.2465183734893799 accuracy: 0.5681137442588806\n"
     ]
    }
   ],
   "source": [
    "# Try CNN or LSTM without pre-trained word embeddings in here:\n",
    "from keras.layers.convolutional import Conv1D\n",
    "\n",
    "inputs = Input((128,), dtype='int32')\n",
    "embedding = Embedding(VOCAB_SIZE, EMBED_SIZE, name='embedding_layer',embeddings_initializer = 'glorot_uniform',\n",
    "                      input_length = 128)(inputs)\n",
    "CNN1D = Conv1D(filters = 100, kernel_size = 6, activation = 'relu')(embedding)\n",
    "maxpool = GlobalMaxPooling1D()(CNN1D)\n",
    "Output = Dense(3, input_shape=(16,), activation = 'softmax')(maxpool)\n",
    "\n",
    "model12 = Model(inputs=[inputs],outputs=[Output])\n",
    "\n",
    "model12.summary()\n",
    "\n",
    "\n",
    "model12.compile(optimizer='adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history12 = model11.fit(x_train_pad, y_train, epochs=30, batch_size=512, validation_data=(x_dev_pad,y_dev), verbose=1)\n",
    "\n",
    "results12 = model11.evaluate(x_test_pad,y_test)\n",
    "\n",
    "print(\"loss:\",results12[0],\"accuracy:\",results12[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--020hfG6rN2"
   },
   "source": [
    "# Model 2: Using pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GdY2-64YG1B"
   },
   "source": [
    "### Preparing pre-trained word embeddings (GLOVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4gBeOyi4gkM"
   },
   "source": [
    "The Embedding layer can be used to load a pre-trained word embedding model. We are going to use GloVe embeddings, which you can read about it here (https://nlp.stanford.edu/projects/glove/). GloVe stands for \"Global Vectors for Word Representation\". It's a somewhat popular embedding technique based on factorizing a matrix of word co-occurence statistics. You can download GloVe and we can seed the Keras Embedding layer with weights from the pre-trained embedding for the words in your dataset.\n",
    "First, we need to read GloVe and map words to GloVe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1647368544467,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "f_PypdqG9Iis"
   },
   "outputs": [],
   "source": [
    "def readGloveFile(gloveFile):\n",
    "    with open(gloveFile, 'r') as f:\n",
    "        wordToGlove = {}  \n",
    "        wordToIndex = {}  \n",
    "        indexToWord = {}  \n",
    "\n",
    "        for line in f:\n",
    "            record = line.strip().split()\n",
    "            token = record[0] \n",
    "            wordToGlove[token] = np.array(record[1:], dtype=np.float64) \n",
    "            \n",
    "        tokens = sorted(wordToGlove.keys())\n",
    "        for idx, tok in enumerate(tokens):\n",
    "            kerasIdx = idx + 1  \n",
    "            wordToIndex[tok] = kerasIdx \n",
    "            indexToWord[kerasIdx] = tok \n",
    "\n",
    "    return wordToIndex, indexToWord, wordToGlove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcIZ3dq59bCh"
   },
   "source": [
    "Now, we create our pre-trained Embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1647368544467,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "gembn7VM3ex8"
   },
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "\n",
    "def createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable):\n",
    "    vocabLen = len(wordToIndex) + 1  \n",
    "    embDim = next(iter(wordToGlove.values())).shape[0]  \n",
    "   \n",
    "    embeddingMatrix = np.zeros((vocabLen, embDim))  \n",
    "    for word, index in wordToIndex.items():\n",
    "        embeddingMatrix[index, :] = wordToGlove[word] \n",
    "\n",
    "    embeddingLayer = Embedding(vocabLen, embDim, embeddings_initializer=Constant(embeddingMatrix), trainable=isTrainable, name='GloVe_Embeddings')\n",
    "    return embeddingLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 183772,
     "status": "ok",
     "timestamp": 1647368728229,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "8OC1wuctdFvA",
    "outputId": "600c8357-a738-44cf-ad3f-2b1d25372802"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-15 18:22:23--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2022-03-15 18:22:24--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2022-03-15 18:22:24--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: â€˜glove.6B.zipâ€™\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.19MB/s    in 2m 40s  \n",
      "\n",
      "2022-03-15 18:25:04 (5.14 MB/s) - â€˜glove.6B.zipâ€™ saved [862182613/862182613]\n",
      "\n",
      "Archive:  /content/glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip '/content/glove.6B.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGxciLK4-xOr"
   },
   "source": [
    "We freeze the weights. To create the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32881,
     "status": "ok",
     "timestamp": 1647368761107,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "PZCPUM0W_Drc",
    "outputId": "ad2c0c72-accb-4d86-f452-c61614512acf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Embedding:  300\n",
      "400001\n"
     ]
    }
   ],
   "source": [
    "# wordToIndex,indexToWord,wordToGlove=readGloveFile('/content/glove.6B.50d.txt')\n",
    "# wordToIndex,indexToWord,wordToGlove=readGloveFile('/content/glove.6B.100d.txt')\n",
    "wordToIndex,indexToWord,wordToGlove=readGloveFile('/content/glove.6B.300d.txt')\n",
    "\n",
    "vocabLen = len(wordToIndex) + 1 \n",
    "\n",
    "EMBED_SIZE = next(iter(wordToGlove.values())).shape[0]\n",
    "print('Size of Embedding: ',EMBED_SIZE)\n",
    "print(vocabLen)\n",
    "embeddingLayer=createPretrainedEmbeddingLayer(wordToGlove,wordToIndex,isTrainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RyeDPimMW7c"
   },
   "source": [
    "### Convert the data to GLOVE word index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_X6nC57NGXv"
   },
   "source": [
    "The index in our vocabulary is different from that in GLOVE. For example, the word \"you\" corresponds to 394475 in GLOVE, while it corresponds to another index in our vocabulary. Thus we can not directly use the index data in the last section. We convert them from text tokens to GLOVE word index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1647368761108,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "ODUoIts1NM6b",
    "outputId": "776d1add-d727-4fff-ae7d-e29070259add"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394475\n",
      "you\n",
      "7338\n"
     ]
    }
   ],
   "source": [
    "print(wordToIndex[\"you\"])\n",
    "print(indexToWord[394475])\n",
    "print(word_index[\"you\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1647368761738,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "p037PLDyNXhV",
    "outputId": "eee6f83c-69a6-4af8-e6f8-176bc6b700fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_review_glove[0]:\n",
      "[357266, 118926, 192973, 264550, 338995, 62065, 51582, 87775, 357354, 151204, 54718, 53201, 292136, 231458, 373317, 151349, 193716]\n",
      "x_train_aspect_glove[0]:\n",
      "[118926]\n"
     ]
    }
   ],
   "source": [
    "# Please write your code to generate the following data\n",
    "# x_train_review_glove\n",
    "# x_train_aspect_glove\n",
    "\n",
    "# x_dev_review_glove\n",
    "# x_dev_aspect_glove\n",
    "\n",
    "# x_test_review_glove\n",
    "# x_test_aspect_glove\n",
    "\n",
    "# your code goes here\n",
    "def generate_review_aspect_Glove(data):\n",
    "    \n",
    "    review_int=[]\n",
    "    aspect_int=[]\n",
    "\n",
    "    for sample in data:\n",
    "        idx=[]\n",
    "        for token in text_to_word_sequence(sample[0]): \n",
    "            if token in wordToIndex.keys(): \n",
    "                idx.append(wordToIndex[token])\n",
    "            else:\n",
    "                idx.append(wordToIndex[\"unk\"])\n",
    "\n",
    "        review_int.append(idx)\n",
    "    \n",
    "        idx=[]\n",
    "        for token in text_to_word_sequence(sample[1]):\n",
    "            if token in wordToIndex.keys(): \n",
    "                idx.append(wordToIndex[token])\n",
    "            else: \n",
    "                idx.append(wordToIndex[\"unk\"])\n",
    "\n",
    "        aspect_int.append(idx)\n",
    "\n",
    "    return review_int, aspect_int\n",
    "\n",
    "x_train_review_glove, x_train_aspect_glove = generate_review_aspect_Glove(train)\n",
    "x_dev_review_glove, x_dev_aspect_glove = generate_review_aspect_Glove(val)\n",
    "x_test_review_glove, x_test_aspect_glove = generate_review_aspect_Glove(test)\n",
    "\n",
    "\n",
    "# You should get a print result like:\n",
    "assert len(x_train_review_glove) == len(train)\n",
    "assert len(x_train_aspect_glove) == len(x_train_aspect_int)\n",
    "assert len(x_test_review_glove) == len(test)\n",
    "assert len(x_test_aspect_glove) == len(x_test_aspect_int)\n",
    "print(\"x_train_review_glove[0]:\")\n",
    "print(x_train_review_glove[0])\n",
    "print(\"x_train_aspect_glove[0]:\")\n",
    "print(x_train_aspect_glove[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z--QOlb1vXtN"
   },
   "source": [
    "As before, we concatenate the tweets and topics for fitting in the previous model. Let us do it again for GLOVE version variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1647368761738,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "1-D_szbDQ5X5",
    "outputId": "d7d7e756-f588-47dd-dd7d-b2d2bd07542f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before paded:\n",
      "[118926, 1, 357266, 118926, 192973, 264550, 338995, 62065, 51582, 87775, 357354, 151204, 54718, 53201, 292136, 231458, 373317, 151349, 193716]\n",
      "After paded:\n",
      "[118926      1 357266 118926 192973 264550 338995  62065  51582  87775\n",
      " 357354 151204  54718  53201 292136 231458 373317 151349 193716      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0]\n"
     ]
    }
   ],
   "source": [
    "# Please write your code to combine the x_*_review_glove and x_*_aspect_glove into the following varibles\n",
    "# x_train_glove\n",
    "# x_dev_glove\n",
    "# x_test_glove\n",
    "\n",
    "# Tips: \n",
    "# 1) There is no <START> token in GLOVE. Here we can use integer 1 to concatenate.\n",
    "# 2) After combine them, do not foget to pad the sequences.\n",
    "\n",
    "def combine_x(x_aspect_glove,x_review_glove):\n",
    "    x_glove=[]\n",
    "    \n",
    "    for index in range(len(x_review_glove)):\n",
    "        x_glove.append(x_aspect_glove[index]+[1]+x_review_glove[index])\n",
    "    return x_glove\n",
    "\n",
    "x_train_glove = combine_x(x_train_aspect_glove,x_train_review_glove)\n",
    "x_dev_glove   = combine_x(x_dev_aspect_glove  ,x_dev_review_glove)\n",
    "x_test_glove  = combine_x(x_test_aspect_glove ,x_test_review_glove)\n",
    "\n",
    "\n",
    "x_train_pad_glove = keras.preprocessing.sequence.pad_sequences(x_train_glove,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=128)\n",
    "\n",
    "x_dev_pad_glove = keras.preprocessing.sequence.pad_sequences(x_dev_glove,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=128)\n",
    "\n",
    "x_test_pad_glove = keras.preprocessing.sequence.pad_sequences(x_test_glove,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=128)\n",
    "x_train_pad_glove = np.array(x_train_pad_glove)\n",
    "x_dev_pad_glove = np.array(x_dev_pad_glove)\n",
    "x_test_pad_glove = np.array(x_test_pad_glove)\n",
    "#######################TODO: REMOVE ABOVE CODE####################\n",
    "\n",
    "# Don't forget the to use np.array function to wrap the ouput of pad_sequences function, such as: x_train_pad = np.array(x_train_pad)\n",
    "# Only pad the *_int varibles\n",
    "print(\"Before paded:\")\n",
    "print(x_train_glove[0])\n",
    "print(\"After paded:\")\n",
    "print(x_train_pad_glove[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdZ4nl08vp9A"
   },
   "source": [
    "## Model 2-1: Neural bag of words using pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gyCwXFj_R5w"
   },
   "source": [
    "We use model3-1 in lab4 to deal with this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38276,
     "status": "ok",
     "timestamp": 1647368800012,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "VICS9rY8C7KH",
    "outputId": "a8c92093-0cc1-4b23-e823-58280df75813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 128)]             0         \n",
      "                                                                 \n",
      " GloVe_Embeddings (Embedding  (None, 128, 300)         120000300 \n",
      " )                                                               \n",
      "                                                                 \n",
      " global_average_pooling1d_ma  (None, 300)              0         \n",
      " sked_1 (GlobalAveragePoolin                                     \n",
      " g1DMasked)                                                      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                4816      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120,005,167\n",
      "Trainable params: 4,867\n",
      "Non-trainable params: 120,000,300\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "22/22 [==============================] - 1s 16ms/step - loss: 1.0691 - accuracy: 0.4507 - val_loss: 1.0565 - val_accuracy: 0.4535\n",
      "Epoch 2/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1.0521 - accuracy: 0.4507 - val_loss: 1.0413 - val_accuracy: 0.4535\n",
      "Epoch 3/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 1.0389 - accuracy: 0.4508 - val_loss: 1.0285 - val_accuracy: 0.4557\n",
      "Epoch 4/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 1.0269 - accuracy: 0.4523 - val_loss: 1.0156 - val_accuracy: 0.4640\n",
      "Epoch 5/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1.0156 - accuracy: 0.4607 - val_loss: 1.0052 - val_accuracy: 0.4745\n",
      "Epoch 6/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 1.0058 - accuracy: 0.4735 - val_loss: 0.9964 - val_accuracy: 0.4805\n",
      "Epoch 7/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9981 - accuracy: 0.4814 - val_loss: 0.9888 - val_accuracy: 0.4880\n",
      "Epoch 8/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.9912 - accuracy: 0.4886 - val_loss: 0.9823 - val_accuracy: 0.4955\n",
      "Epoch 9/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.9853 - accuracy: 0.4944 - val_loss: 0.9773 - val_accuracy: 0.4992\n",
      "Epoch 10/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9800 - accuracy: 0.4996 - val_loss: 0.9717 - val_accuracy: 0.5038\n",
      "Epoch 11/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9753 - accuracy: 0.5024 - val_loss: 0.9683 - val_accuracy: 0.5075\n",
      "Epoch 12/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.9707 - accuracy: 0.5078 - val_loss: 0.9637 - val_accuracy: 0.5143\n",
      "Epoch 13/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9671 - accuracy: 0.5084 - val_loss: 0.9598 - val_accuracy: 0.5143\n",
      "Epoch 14/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.9631 - accuracy: 0.5143 - val_loss: 0.9565 - val_accuracy: 0.5135\n",
      "Epoch 15/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9592 - accuracy: 0.5114 - val_loss: 0.9548 - val_accuracy: 0.5188\n",
      "Epoch 16/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9558 - accuracy: 0.5145 - val_loss: 0.9503 - val_accuracy: 0.5240\n",
      "Epoch 17/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.9526 - accuracy: 0.5164 - val_loss: 0.9492 - val_accuracy: 0.5210\n",
      "Epoch 18/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.9498 - accuracy: 0.5187 - val_loss: 0.9464 - val_accuracy: 0.5285\n",
      "Epoch 19/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.9468 - accuracy: 0.5225 - val_loss: 0.9441 - val_accuracy: 0.5308\n",
      "Epoch 20/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9442 - accuracy: 0.5211 - val_loss: 0.9399 - val_accuracy: 0.5233\n",
      "Epoch 21/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9417 - accuracy: 0.5233 - val_loss: 0.9372 - val_accuracy: 0.5308\n",
      "Epoch 22/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.9394 - accuracy: 0.5240 - val_loss: 0.9353 - val_accuracy: 0.5285\n",
      "Epoch 23/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9367 - accuracy: 0.5257 - val_loss: 0.9349 - val_accuracy: 0.5308\n",
      "Epoch 24/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9348 - accuracy: 0.5290 - val_loss: 0.9325 - val_accuracy: 0.5330\n",
      "Epoch 25/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9325 - accuracy: 0.5313 - val_loss: 0.9325 - val_accuracy: 0.5323\n",
      "Epoch 26/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9312 - accuracy: 0.5330 - val_loss: 0.9307 - val_accuracy: 0.5353\n",
      "Epoch 27/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9285 - accuracy: 0.5357 - val_loss: 0.9273 - val_accuracy: 0.5263\n",
      "Epoch 28/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9274 - accuracy: 0.5359 - val_loss: 0.9256 - val_accuracy: 0.5315\n",
      "Epoch 29/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9269 - accuracy: 0.5370 - val_loss: 0.9249 - val_accuracy: 0.5353\n",
      "Epoch 30/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.9248 - accuracy: 0.5343 - val_loss: 0.9242 - val_accuracy: 0.5323\n",
      "Epoch 31/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.9227 - accuracy: 0.5388 - val_loss: 0.9224 - val_accuracy: 0.5405\n",
      "Epoch 32/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9209 - accuracy: 0.5384 - val_loss: 0.9204 - val_accuracy: 0.5435\n",
      "Epoch 33/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9200 - accuracy: 0.5401 - val_loss: 0.9195 - val_accuracy: 0.5398\n",
      "Epoch 34/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9186 - accuracy: 0.5417 - val_loss: 0.9189 - val_accuracy: 0.5413\n",
      "Epoch 35/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9166 - accuracy: 0.5424 - val_loss: 0.9199 - val_accuracy: 0.5428\n",
      "Epoch 36/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9160 - accuracy: 0.5423 - val_loss: 0.9175 - val_accuracy: 0.5360\n",
      "Epoch 37/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9148 - accuracy: 0.5443 - val_loss: 0.9158 - val_accuracy: 0.5450\n",
      "Epoch 38/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9142 - accuracy: 0.5412 - val_loss: 0.9172 - val_accuracy: 0.5413\n",
      "Epoch 39/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.9132 - accuracy: 0.5434 - val_loss: 0.9158 - val_accuracy: 0.5428\n",
      "Epoch 40/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.9121 - accuracy: 0.5460 - val_loss: 0.9152 - val_accuracy: 0.5458\n",
      "Epoch 41/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9108 - accuracy: 0.5459 - val_loss: 0.9140 - val_accuracy: 0.5435\n",
      "Epoch 42/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9098 - accuracy: 0.5444 - val_loss: 0.9162 - val_accuracy: 0.5428\n",
      "Epoch 43/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.9091 - accuracy: 0.5456 - val_loss: 0.9135 - val_accuracy: 0.5458\n",
      "Epoch 44/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9079 - accuracy: 0.5480 - val_loss: 0.9119 - val_accuracy: 0.5495\n",
      "Epoch 45/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9070 - accuracy: 0.5489 - val_loss: 0.9116 - val_accuracy: 0.5556\n",
      "Epoch 46/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9072 - accuracy: 0.5505 - val_loss: 0.9117 - val_accuracy: 0.5465\n",
      "Epoch 47/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.9061 - accuracy: 0.5497 - val_loss: 0.9104 - val_accuracy: 0.5480\n",
      "Epoch 48/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.9055 - accuracy: 0.5502 - val_loss: 0.9104 - val_accuracy: 0.5548\n",
      "Epoch 49/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9045 - accuracy: 0.5491 - val_loss: 0.9104 - val_accuracy: 0.5511\n",
      "Epoch 50/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9033 - accuracy: 0.5492 - val_loss: 0.9094 - val_accuracy: 0.5480\n",
      "Epoch 51/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.9028 - accuracy: 0.5505 - val_loss: 0.9096 - val_accuracy: 0.5518\n",
      "Epoch 52/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9021 - accuracy: 0.5501 - val_loss: 0.9109 - val_accuracy: 0.5458\n",
      "Epoch 53/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9016 - accuracy: 0.5523 - val_loss: 0.9083 - val_accuracy: 0.5503\n",
      "Epoch 54/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9010 - accuracy: 0.5501 - val_loss: 0.9082 - val_accuracy: 0.5518\n",
      "Epoch 55/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9001 - accuracy: 0.5518 - val_loss: 0.9073 - val_accuracy: 0.5541\n",
      "Epoch 56/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9007 - accuracy: 0.5534 - val_loss: 0.9073 - val_accuracy: 0.5556\n",
      "Epoch 57/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8987 - accuracy: 0.5522 - val_loss: 0.9074 - val_accuracy: 0.5578\n",
      "Epoch 58/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8976 - accuracy: 0.5537 - val_loss: 0.9070 - val_accuracy: 0.5563\n",
      "Epoch 59/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8967 - accuracy: 0.5535 - val_loss: 0.9065 - val_accuracy: 0.5541\n",
      "Epoch 60/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8959 - accuracy: 0.5556 - val_loss: 0.9074 - val_accuracy: 0.5563\n",
      "Epoch 61/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8952 - accuracy: 0.5554 - val_loss: 0.9055 - val_accuracy: 0.5556\n",
      "Epoch 62/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8944 - accuracy: 0.5570 - val_loss: 0.9060 - val_accuracy: 0.5608\n",
      "Epoch 63/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8939 - accuracy: 0.5591 - val_loss: 0.9051 - val_accuracy: 0.5608\n",
      "Epoch 64/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8931 - accuracy: 0.5598 - val_loss: 0.9039 - val_accuracy: 0.5533\n",
      "Epoch 65/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8926 - accuracy: 0.5608 - val_loss: 0.9033 - val_accuracy: 0.5548\n",
      "Epoch 66/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8919 - accuracy: 0.5591 - val_loss: 0.9042 - val_accuracy: 0.5616\n",
      "Epoch 67/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8910 - accuracy: 0.5609 - val_loss: 0.9026 - val_accuracy: 0.5578\n",
      "Epoch 68/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8906 - accuracy: 0.5599 - val_loss: 0.9044 - val_accuracy: 0.5608\n",
      "Epoch 69/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8896 - accuracy: 0.5611 - val_loss: 0.9034 - val_accuracy: 0.5698\n",
      "Epoch 70/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8893 - accuracy: 0.5617 - val_loss: 0.9043 - val_accuracy: 0.5623\n",
      "Epoch 71/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8892 - accuracy: 0.5640 - val_loss: 0.9038 - val_accuracy: 0.5661\n",
      "Epoch 72/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8875 - accuracy: 0.5639 - val_loss: 0.9024 - val_accuracy: 0.5698\n",
      "Epoch 73/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8874 - accuracy: 0.5647 - val_loss: 0.9024 - val_accuracy: 0.5638\n",
      "Epoch 74/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.8863 - accuracy: 0.5665 - val_loss: 0.9004 - val_accuracy: 0.5616\n",
      "Epoch 75/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.8857 - accuracy: 0.5663 - val_loss: 0.8999 - val_accuracy: 0.5623\n",
      "Epoch 76/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8856 - accuracy: 0.5678 - val_loss: 0.8999 - val_accuracy: 0.5571\n",
      "Epoch 77/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.8847 - accuracy: 0.5662 - val_loss: 0.9001 - val_accuracy: 0.5668\n",
      "Epoch 78/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8838 - accuracy: 0.5687 - val_loss: 0.9013 - val_accuracy: 0.5736\n",
      "Epoch 79/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.8840 - accuracy: 0.5687 - val_loss: 0.8999 - val_accuracy: 0.5698\n",
      "Epoch 80/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8826 - accuracy: 0.5670 - val_loss: 0.8993 - val_accuracy: 0.5668\n",
      "Epoch 81/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.8825 - accuracy: 0.5698 - val_loss: 0.9005 - val_accuracy: 0.5728\n",
      "Epoch 82/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.8816 - accuracy: 0.5699 - val_loss: 0.8984 - val_accuracy: 0.5721\n",
      "Epoch 83/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.8810 - accuracy: 0.5704 - val_loss: 0.8979 - val_accuracy: 0.5653\n",
      "Epoch 84/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8807 - accuracy: 0.5706 - val_loss: 0.8974 - val_accuracy: 0.5668\n",
      "Epoch 85/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.8804 - accuracy: 0.5713 - val_loss: 0.8987 - val_accuracy: 0.5713\n",
      "Epoch 86/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.8792 - accuracy: 0.5722 - val_loss: 0.8977 - val_accuracy: 0.5683\n",
      "Epoch 87/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8797 - accuracy: 0.5702 - val_loss: 0.8975 - val_accuracy: 0.5728\n",
      "Epoch 88/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.8785 - accuracy: 0.5706 - val_loss: 0.8968 - val_accuracy: 0.5691\n",
      "Epoch 89/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8778 - accuracy: 0.5731 - val_loss: 0.8967 - val_accuracy: 0.5728\n",
      "Epoch 90/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8773 - accuracy: 0.5712 - val_loss: 0.8981 - val_accuracy: 0.5743\n",
      "Epoch 91/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.8766 - accuracy: 0.5729 - val_loss: 0.8971 - val_accuracy: 0.5721\n",
      "Epoch 92/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8765 - accuracy: 0.5742 - val_loss: 0.8977 - val_accuracy: 0.5706\n",
      "Epoch 93/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.8759 - accuracy: 0.5727 - val_loss: 0.8967 - val_accuracy: 0.5736\n",
      "Epoch 94/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.8752 - accuracy: 0.5755 - val_loss: 0.8967 - val_accuracy: 0.5728\n",
      "Epoch 95/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8749 - accuracy: 0.5755 - val_loss: 0.8950 - val_accuracy: 0.5661\n",
      "Epoch 96/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8748 - accuracy: 0.5763 - val_loss: 0.8973 - val_accuracy: 0.5788\n",
      "Epoch 97/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8738 - accuracy: 0.5761 - val_loss: 0.8943 - val_accuracy: 0.5691\n",
      "Epoch 98/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.8732 - accuracy: 0.5776 - val_loss: 0.8947 - val_accuracy: 0.5706\n",
      "Epoch 99/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8730 - accuracy: 0.5765 - val_loss: 0.8937 - val_accuracy: 0.5713\n",
      "Epoch 100/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.8725 - accuracy: 0.5754 - val_loss: 0.8957 - val_accuracy: 0.5683\n",
      "Epoch 101/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8720 - accuracy: 0.5782 - val_loss: 0.8953 - val_accuracy: 0.5743\n",
      "Epoch 102/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.8719 - accuracy: 0.5781 - val_loss: 0.8935 - val_accuracy: 0.5758\n",
      "Epoch 103/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8717 - accuracy: 0.5772 - val_loss: 0.8932 - val_accuracy: 0.5751\n",
      "Epoch 104/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.8710 - accuracy: 0.5784 - val_loss: 0.8924 - val_accuracy: 0.5751\n",
      "Epoch 105/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8707 - accuracy: 0.5769 - val_loss: 0.8927 - val_accuracy: 0.5743\n",
      "Epoch 106/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8701 - accuracy: 0.5780 - val_loss: 0.8930 - val_accuracy: 0.5758\n",
      "Epoch 107/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8696 - accuracy: 0.5803 - val_loss: 0.8921 - val_accuracy: 0.5728\n",
      "Epoch 108/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8695 - accuracy: 0.5783 - val_loss: 0.8923 - val_accuracy: 0.5781\n",
      "Epoch 109/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8691 - accuracy: 0.5797 - val_loss: 0.8925 - val_accuracy: 0.5773\n",
      "Epoch 110/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8683 - accuracy: 0.5792 - val_loss: 0.8928 - val_accuracy: 0.5758\n",
      "Epoch 111/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8681 - accuracy: 0.5808 - val_loss: 0.8923 - val_accuracy: 0.5706\n",
      "Epoch 112/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8679 - accuracy: 0.5807 - val_loss: 0.8946 - val_accuracy: 0.5713\n",
      "Epoch 113/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8678 - accuracy: 0.5817 - val_loss: 0.8943 - val_accuracy: 0.5766\n",
      "Epoch 114/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8672 - accuracy: 0.5806 - val_loss: 0.8926 - val_accuracy: 0.5781\n",
      "Epoch 115/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8668 - accuracy: 0.5812 - val_loss: 0.8917 - val_accuracy: 0.5736\n",
      "Epoch 116/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8662 - accuracy: 0.5826 - val_loss: 0.8926 - val_accuracy: 0.5668\n",
      "Epoch 117/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.8658 - accuracy: 0.5827 - val_loss: 0.8917 - val_accuracy: 0.5773\n",
      "Epoch 118/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8653 - accuracy: 0.5836 - val_loss: 0.8911 - val_accuracy: 0.5803\n",
      "Epoch 119/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8655 - accuracy: 0.5812 - val_loss: 0.8917 - val_accuracy: 0.5698\n",
      "Epoch 120/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8655 - accuracy: 0.5822 - val_loss: 0.8957 - val_accuracy: 0.5706\n",
      "Epoch 121/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8659 - accuracy: 0.5800 - val_loss: 0.8933 - val_accuracy: 0.5751\n",
      "Epoch 122/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8642 - accuracy: 0.5846 - val_loss: 0.8909 - val_accuracy: 0.5751\n",
      "Epoch 123/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8642 - accuracy: 0.5825 - val_loss: 0.8901 - val_accuracy: 0.5736\n",
      "Epoch 124/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8637 - accuracy: 0.5856 - val_loss: 0.8893 - val_accuracy: 0.5691\n",
      "Epoch 125/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8640 - accuracy: 0.5832 - val_loss: 0.8904 - val_accuracy: 0.5676\n",
      "Epoch 126/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8635 - accuracy: 0.5809 - val_loss: 0.8890 - val_accuracy: 0.5721\n",
      "Epoch 127/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8633 - accuracy: 0.5820 - val_loss: 0.8923 - val_accuracy: 0.5788\n",
      "Epoch 128/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8622 - accuracy: 0.5849 - val_loss: 0.8890 - val_accuracy: 0.5788\n",
      "Epoch 129/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8624 - accuracy: 0.5880 - val_loss: 0.8904 - val_accuracy: 0.5698\n",
      "Epoch 130/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8623 - accuracy: 0.5877 - val_loss: 0.8901 - val_accuracy: 0.5691\n",
      "Epoch 131/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8615 - accuracy: 0.5848 - val_loss: 0.8892 - val_accuracy: 0.5796\n",
      "Epoch 132/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8609 - accuracy: 0.5856 - val_loss: 0.8888 - val_accuracy: 0.5803\n",
      "Epoch 133/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8605 - accuracy: 0.5859 - val_loss: 0.8898 - val_accuracy: 0.5766\n",
      "Epoch 134/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8601 - accuracy: 0.5880 - val_loss: 0.8881 - val_accuracy: 0.5796\n",
      "Epoch 135/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8602 - accuracy: 0.5887 - val_loss: 0.8896 - val_accuracy: 0.5721\n",
      "Epoch 136/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8598 - accuracy: 0.5864 - val_loss: 0.8900 - val_accuracy: 0.5728\n",
      "Epoch 137/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8597 - accuracy: 0.5853 - val_loss: 0.8894 - val_accuracy: 0.5758\n",
      "Epoch 138/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8586 - accuracy: 0.5864 - val_loss: 0.8884 - val_accuracy: 0.5743\n",
      "Epoch 139/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8594 - accuracy: 0.5885 - val_loss: 0.8889 - val_accuracy: 0.5728\n",
      "Epoch 140/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8598 - accuracy: 0.5854 - val_loss: 0.8875 - val_accuracy: 0.5781\n",
      "Epoch 141/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8583 - accuracy: 0.5864 - val_loss: 0.8881 - val_accuracy: 0.5781\n",
      "Epoch 142/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8580 - accuracy: 0.5894 - val_loss: 0.8901 - val_accuracy: 0.5743\n",
      "Epoch 143/200\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.8575 - accuracy: 0.5888 - val_loss: 0.8873 - val_accuracy: 0.5788\n",
      "Epoch 144/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8571 - accuracy: 0.5871 - val_loss: 0.8878 - val_accuracy: 0.5781\n",
      "Epoch 145/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8576 - accuracy: 0.5876 - val_loss: 0.8882 - val_accuracy: 0.5698\n",
      "Epoch 146/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8572 - accuracy: 0.5873 - val_loss: 0.8890 - val_accuracy: 0.5773\n",
      "Epoch 147/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8565 - accuracy: 0.5887 - val_loss: 0.8883 - val_accuracy: 0.5736\n",
      "Epoch 148/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8563 - accuracy: 0.5907 - val_loss: 0.8875 - val_accuracy: 0.5781\n",
      "Epoch 149/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8556 - accuracy: 0.5896 - val_loss: 0.8879 - val_accuracy: 0.5751\n",
      "Epoch 150/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8560 - accuracy: 0.5894 - val_loss: 0.8876 - val_accuracy: 0.5781\n",
      "Epoch 151/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.8555 - accuracy: 0.5910 - val_loss: 0.8879 - val_accuracy: 0.5751\n",
      "Epoch 152/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8553 - accuracy: 0.5906 - val_loss: 0.8881 - val_accuracy: 0.5728\n",
      "Epoch 153/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8546 - accuracy: 0.5907 - val_loss: 0.8878 - val_accuracy: 0.5728\n",
      "Epoch 154/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8545 - accuracy: 0.5909 - val_loss: 0.8866 - val_accuracy: 0.5743\n",
      "Epoch 155/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8541 - accuracy: 0.5898 - val_loss: 0.8864 - val_accuracy: 0.5788\n",
      "Epoch 156/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8539 - accuracy: 0.5898 - val_loss: 0.8860 - val_accuracy: 0.5743\n",
      "Epoch 157/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8536 - accuracy: 0.5906 - val_loss: 0.8874 - val_accuracy: 0.5803\n",
      "Epoch 158/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8534 - accuracy: 0.5910 - val_loss: 0.8860 - val_accuracy: 0.5781\n",
      "Epoch 159/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8537 - accuracy: 0.5937 - val_loss: 0.8862 - val_accuracy: 0.5811\n",
      "Epoch 160/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8531 - accuracy: 0.5911 - val_loss: 0.8865 - val_accuracy: 0.5758\n",
      "Epoch 161/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8525 - accuracy: 0.5934 - val_loss: 0.8859 - val_accuracy: 0.5781\n",
      "Epoch 162/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8528 - accuracy: 0.5923 - val_loss: 0.8855 - val_accuracy: 0.5751\n",
      "Epoch 163/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8522 - accuracy: 0.5918 - val_loss: 0.8852 - val_accuracy: 0.5811\n",
      "Epoch 164/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8522 - accuracy: 0.5922 - val_loss: 0.8874 - val_accuracy: 0.5743\n",
      "Epoch 165/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.8517 - accuracy: 0.5915 - val_loss: 0.8864 - val_accuracy: 0.5758\n",
      "Epoch 166/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8516 - accuracy: 0.5936 - val_loss: 0.8855 - val_accuracy: 0.5781\n",
      "Epoch 167/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8514 - accuracy: 0.5905 - val_loss: 0.8850 - val_accuracy: 0.5766\n",
      "Epoch 168/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8513 - accuracy: 0.5941 - val_loss: 0.8852 - val_accuracy: 0.5728\n",
      "Epoch 169/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8512 - accuracy: 0.5941 - val_loss: 0.8861 - val_accuracy: 0.5743\n",
      "Epoch 170/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8508 - accuracy: 0.5926 - val_loss: 0.8856 - val_accuracy: 0.5788\n",
      "Epoch 171/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8504 - accuracy: 0.5923 - val_loss: 0.8847 - val_accuracy: 0.5766\n",
      "Epoch 172/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8503 - accuracy: 0.5934 - val_loss: 0.8846 - val_accuracy: 0.5803\n",
      "Epoch 173/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8500 - accuracy: 0.5943 - val_loss: 0.8847 - val_accuracy: 0.5781\n",
      "Epoch 174/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8499 - accuracy: 0.5926 - val_loss: 0.8874 - val_accuracy: 0.5698\n",
      "Epoch 175/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8494 - accuracy: 0.5946 - val_loss: 0.8885 - val_accuracy: 0.5691\n",
      "Epoch 176/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8494 - accuracy: 0.5942 - val_loss: 0.8855 - val_accuracy: 0.5743\n",
      "Epoch 177/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8488 - accuracy: 0.5936 - val_loss: 0.8854 - val_accuracy: 0.5773\n",
      "Epoch 178/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.8484 - accuracy: 0.5957 - val_loss: 0.8852 - val_accuracy: 0.5766\n",
      "Epoch 179/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8485 - accuracy: 0.5935 - val_loss: 0.8845 - val_accuracy: 0.5781\n",
      "Epoch 180/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8488 - accuracy: 0.5957 - val_loss: 0.8841 - val_accuracy: 0.5781\n",
      "Epoch 181/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8478 - accuracy: 0.5937 - val_loss: 0.8853 - val_accuracy: 0.5758\n",
      "Epoch 182/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8480 - accuracy: 0.5947 - val_loss: 0.8850 - val_accuracy: 0.5811\n",
      "Epoch 183/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8476 - accuracy: 0.5959 - val_loss: 0.8847 - val_accuracy: 0.5803\n",
      "Epoch 184/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8478 - accuracy: 0.5968 - val_loss: 0.8846 - val_accuracy: 0.5781\n",
      "Epoch 185/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8475 - accuracy: 0.5921 - val_loss: 0.8854 - val_accuracy: 0.5841\n",
      "Epoch 186/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8475 - accuracy: 0.5959 - val_loss: 0.8873 - val_accuracy: 0.5751\n",
      "Epoch 187/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8475 - accuracy: 0.5957 - val_loss: 0.8847 - val_accuracy: 0.5811\n",
      "Epoch 188/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8459 - accuracy: 0.5970 - val_loss: 0.8884 - val_accuracy: 0.5691\n",
      "Epoch 189/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8473 - accuracy: 0.5947 - val_loss: 0.8877 - val_accuracy: 0.5728\n",
      "Epoch 190/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.8465 - accuracy: 0.5953 - val_loss: 0.8847 - val_accuracy: 0.5758\n",
      "Epoch 191/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8459 - accuracy: 0.5965 - val_loss: 0.8857 - val_accuracy: 0.5668\n",
      "Epoch 192/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8459 - accuracy: 0.5985 - val_loss: 0.8854 - val_accuracy: 0.5706\n",
      "Epoch 193/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8452 - accuracy: 0.5982 - val_loss: 0.8840 - val_accuracy: 0.5773\n",
      "Epoch 194/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8456 - accuracy: 0.5963 - val_loss: 0.8900 - val_accuracy: 0.5661\n",
      "Epoch 195/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.8460 - accuracy: 0.5960 - val_loss: 0.8868 - val_accuracy: 0.5646\n",
      "Epoch 196/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.8451 - accuracy: 0.5979 - val_loss: 0.8859 - val_accuracy: 0.5676\n",
      "Epoch 197/200\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.8443 - accuracy: 0.5981 - val_loss: 0.8848 - val_accuracy: 0.5766\n",
      "Epoch 198/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.8448 - accuracy: 0.5968 - val_loss: 0.8852 - val_accuracy: 0.5698\n",
      "Epoch 199/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8443 - accuracy: 0.5982 - val_loss: 0.8889 - val_accuracy: 0.5691\n",
      "Epoch 200/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.8439 - accuracy: 0.5992 - val_loss: 0.8842 - val_accuracy: 0.5758\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.8815 - accuracy: 0.5689\n",
      "loss: 0.8814734816551208 accuracy: 0.56886225938797\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "# Tips: Do not misuse the training data\n",
    "\n",
    "inputs = Input((128,), dtype='int32')\n",
    "GloVe_Embeddings = embeddingLayer(inputs)\n",
    "global_avg_pooling1d_masked= GlobalAveragePooling1DMasked()(GloVe_Embeddings)\n",
    "Hidden_1 = Dense(16, activation = \"relu\")(global_avg_pooling1d_masked)\n",
    "Output = Dense(3, input_shape=(16,) , activation = 'softmax')(Hidden_1)\n",
    "\n",
    "model21 = Model(inputs=[inputs],outputs=[Output])\n",
    "\n",
    "model21.summary()\n",
    "\n",
    "model21.compile(optimizer='adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history21 = model21.fit(x_train_pad_glove, y_train, epochs=200, batch_size=512, validation_data=(x_dev_pad_glove,y_dev), verbose=1)\n",
    "\n",
    "results21 = model21.evaluate(x_test_pad_glove,y_test)\n",
    "\n",
    "print(\"loss:\",results21[0],\"accuracy:\",results21[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aTWUWQS2VFd"
   },
   "source": [
    "The accuracy is around 56%. In this version, the \"glorot_uniform\" initialization method does not improve model performance significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZ1KWFKvcagS"
   },
   "source": [
    "##  Model 2-2: CNN or LSTM with pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYubGfP-2VEL"
   },
   "source": [
    "Please try one more model (CNN or LSTM) with pre-trained word embeddings in here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 202709,
     "status": "ok",
     "timestamp": 1647369002704,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "KeLTrJ-3zKBj",
    "outputId": "ec07702d-b1a4-4045-8c20-bd40257ce8a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 128)]             0         \n",
      "                                                                 \n",
      " GloVe_Embeddings (Embedding  (None, 128, 300)         120000300 \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 123, 100)          180100    \n",
      "                                                                 \n",
      " global_average_pooling1d_ma  (None, 100)              0         \n",
      " sked_2 (GlobalAveragePoolin                                     \n",
      " g1DMasked)                                                      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 120,180,703\n",
      "Trainable params: 180,403\n",
      "Non-trainable params: 120,000,300\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "22/22 [==============================] - 10s 78ms/step - loss: 1.0509 - accuracy: 0.4361 - val_loss: 1.0159 - val_accuracy: 0.4542\n",
      "Epoch 2/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 1.0072 - accuracy: 0.4571 - val_loss: 0.9897 - val_accuracy: 0.4670\n",
      "Epoch 3/200\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.9866 - accuracy: 0.4828 - val_loss: 0.9801 - val_accuracy: 0.4865\n",
      "Epoch 4/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.9715 - accuracy: 0.4932 - val_loss: 0.9628 - val_accuracy: 0.4910\n",
      "Epoch 5/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.9574 - accuracy: 0.5040 - val_loss: 0.9564 - val_accuracy: 0.5098\n",
      "Epoch 6/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.9485 - accuracy: 0.5081 - val_loss: 0.9473 - val_accuracy: 0.5015\n",
      "Epoch 7/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.9402 - accuracy: 0.5122 - val_loss: 0.9418 - val_accuracy: 0.5203\n",
      "Epoch 8/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.9321 - accuracy: 0.5204 - val_loss: 0.9371 - val_accuracy: 0.5060\n",
      "Epoch 9/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.9264 - accuracy: 0.5248 - val_loss: 0.9413 - val_accuracy: 0.5158\n",
      "Epoch 10/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.9194 - accuracy: 0.5290 - val_loss: 0.9282 - val_accuracy: 0.5360\n",
      "Epoch 11/200\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.9108 - accuracy: 0.5401 - val_loss: 0.9191 - val_accuracy: 0.5420\n",
      "Epoch 12/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.9008 - accuracy: 0.5494 - val_loss: 0.9149 - val_accuracy: 0.5578\n",
      "Epoch 13/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.8943 - accuracy: 0.5581 - val_loss: 0.9048 - val_accuracy: 0.5713\n",
      "Epoch 14/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.8813 - accuracy: 0.5717 - val_loss: 0.8960 - val_accuracy: 0.5833\n",
      "Epoch 15/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.8707 - accuracy: 0.5793 - val_loss: 0.8893 - val_accuracy: 0.5923\n",
      "Epoch 16/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.8620 - accuracy: 0.5897 - val_loss: 0.8848 - val_accuracy: 0.5893\n",
      "Epoch 17/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.8507 - accuracy: 0.5993 - val_loss: 0.8758 - val_accuracy: 0.5893\n",
      "Epoch 18/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.8424 - accuracy: 0.6086 - val_loss: 0.8737 - val_accuracy: 0.5818\n",
      "Epoch 19/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.8347 - accuracy: 0.6087 - val_loss: 0.8635 - val_accuracy: 0.6096\n",
      "Epoch 20/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.8247 - accuracy: 0.6187 - val_loss: 0.8561 - val_accuracy: 0.6134\n",
      "Epoch 21/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.8157 - accuracy: 0.6250 - val_loss: 0.8537 - val_accuracy: 0.6081\n",
      "Epoch 22/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.8070 - accuracy: 0.6297 - val_loss: 0.8475 - val_accuracy: 0.6149\n",
      "Epoch 23/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.7997 - accuracy: 0.6365 - val_loss: 0.8436 - val_accuracy: 0.6216\n",
      "Epoch 24/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.7938 - accuracy: 0.6372 - val_loss: 0.8449 - val_accuracy: 0.6089\n",
      "Epoch 25/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.7867 - accuracy: 0.6430 - val_loss: 0.8371 - val_accuracy: 0.6269\n",
      "Epoch 26/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.7828 - accuracy: 0.6491 - val_loss: 0.8413 - val_accuracy: 0.6156\n",
      "Epoch 27/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.7785 - accuracy: 0.6480 - val_loss: 0.8317 - val_accuracy: 0.6366\n",
      "Epoch 28/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.7736 - accuracy: 0.6516 - val_loss: 0.8292 - val_accuracy: 0.6329\n",
      "Epoch 29/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.7641 - accuracy: 0.6571 - val_loss: 0.8243 - val_accuracy: 0.6381\n",
      "Epoch 30/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.7572 - accuracy: 0.6613 - val_loss: 0.8256 - val_accuracy: 0.6336\n",
      "Epoch 31/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.7534 - accuracy: 0.6646 - val_loss: 0.8224 - val_accuracy: 0.6366\n",
      "Epoch 32/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.7498 - accuracy: 0.6655 - val_loss: 0.8247 - val_accuracy: 0.6231\n",
      "Epoch 33/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.7444 - accuracy: 0.6709 - val_loss: 0.8203 - val_accuracy: 0.6426\n",
      "Epoch 34/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.7392 - accuracy: 0.6719 - val_loss: 0.8184 - val_accuracy: 0.6366\n",
      "Epoch 35/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.7384 - accuracy: 0.6711 - val_loss: 0.8186 - val_accuracy: 0.6366\n",
      "Epoch 36/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.7319 - accuracy: 0.6745 - val_loss: 0.8191 - val_accuracy: 0.6329\n",
      "Epoch 37/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.7317 - accuracy: 0.6766 - val_loss: 0.8210 - val_accuracy: 0.6471\n",
      "Epoch 38/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.7264 - accuracy: 0.6803 - val_loss: 0.8128 - val_accuracy: 0.6524\n",
      "Epoch 39/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.7232 - accuracy: 0.6795 - val_loss: 0.8149 - val_accuracy: 0.6494\n",
      "Epoch 40/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.7200 - accuracy: 0.6802 - val_loss: 0.8155 - val_accuracy: 0.6426\n",
      "Epoch 41/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.7136 - accuracy: 0.6859 - val_loss: 0.8183 - val_accuracy: 0.6389\n",
      "Epoch 42/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.7169 - accuracy: 0.6834 - val_loss: 0.8107 - val_accuracy: 0.6509\n",
      "Epoch 43/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.7087 - accuracy: 0.6859 - val_loss: 0.8154 - val_accuracy: 0.6464\n",
      "Epoch 44/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.7069 - accuracy: 0.6868 - val_loss: 0.8096 - val_accuracy: 0.6562\n",
      "Epoch 45/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.7013 - accuracy: 0.6931 - val_loss: 0.8100 - val_accuracy: 0.6569\n",
      "Epoch 46/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.7007 - accuracy: 0.6915 - val_loss: 0.8062 - val_accuracy: 0.6494\n",
      "Epoch 47/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.6954 - accuracy: 0.6939 - val_loss: 0.8081 - val_accuracy: 0.6539\n",
      "Epoch 48/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.6984 - accuracy: 0.6936 - val_loss: 0.8069 - val_accuracy: 0.6532\n",
      "Epoch 49/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6939 - accuracy: 0.6937 - val_loss: 0.8096 - val_accuracy: 0.6494\n",
      "Epoch 50/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6879 - accuracy: 0.6980 - val_loss: 0.8071 - val_accuracy: 0.6509\n",
      "Epoch 51/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6869 - accuracy: 0.6984 - val_loss: 0.8080 - val_accuracy: 0.6547\n",
      "Epoch 52/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.6845 - accuracy: 0.7007 - val_loss: 0.8126 - val_accuracy: 0.6434\n",
      "Epoch 53/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.6839 - accuracy: 0.6991 - val_loss: 0.8052 - val_accuracy: 0.6509\n",
      "Epoch 54/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6779 - accuracy: 0.7044 - val_loss: 0.8057 - val_accuracy: 0.6532\n",
      "Epoch 55/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.6763 - accuracy: 0.7028 - val_loss: 0.8039 - val_accuracy: 0.6547\n",
      "Epoch 56/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.6743 - accuracy: 0.7046 - val_loss: 0.8073 - val_accuracy: 0.6464\n",
      "Epoch 57/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.6707 - accuracy: 0.7054 - val_loss: 0.8229 - val_accuracy: 0.6389\n",
      "Epoch 58/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6743 - accuracy: 0.7067 - val_loss: 0.8026 - val_accuracy: 0.6539\n",
      "Epoch 59/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6653 - accuracy: 0.7109 - val_loss: 0.8060 - val_accuracy: 0.6547\n",
      "Epoch 60/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.6639 - accuracy: 0.7114 - val_loss: 0.8040 - val_accuracy: 0.6509\n",
      "Epoch 61/200\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 0.6602 - accuracy: 0.7140 - val_loss: 0.8033 - val_accuracy: 0.6577\n",
      "Epoch 62/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6583 - accuracy: 0.7149 - val_loss: 0.8021 - val_accuracy: 0.6532\n",
      "Epoch 63/200\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.6602 - accuracy: 0.7131 - val_loss: 0.8010 - val_accuracy: 0.6479\n",
      "Epoch 64/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6563 - accuracy: 0.7147 - val_loss: 0.8012 - val_accuracy: 0.6524\n",
      "Epoch 65/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6545 - accuracy: 0.7162 - val_loss: 0.8035 - val_accuracy: 0.6562\n",
      "Epoch 66/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6517 - accuracy: 0.7147 - val_loss: 0.8028 - val_accuracy: 0.6479\n",
      "Epoch 67/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6509 - accuracy: 0.7171 - val_loss: 0.8069 - val_accuracy: 0.6517\n",
      "Epoch 68/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6471 - accuracy: 0.7179 - val_loss: 0.8088 - val_accuracy: 0.6569\n",
      "Epoch 69/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6489 - accuracy: 0.7176 - val_loss: 0.8056 - val_accuracy: 0.6502\n",
      "Epoch 70/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6441 - accuracy: 0.7198 - val_loss: 0.8012 - val_accuracy: 0.6456\n",
      "Epoch 71/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6431 - accuracy: 0.7224 - val_loss: 0.8018 - val_accuracy: 0.6486\n",
      "Epoch 72/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6397 - accuracy: 0.7211 - val_loss: 0.8019 - val_accuracy: 0.6562\n",
      "Epoch 73/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6371 - accuracy: 0.7223 - val_loss: 0.8013 - val_accuracy: 0.6494\n",
      "Epoch 74/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6353 - accuracy: 0.7247 - val_loss: 0.8019 - val_accuracy: 0.6509\n",
      "Epoch 75/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6339 - accuracy: 0.7244 - val_loss: 0.8018 - val_accuracy: 0.6471\n",
      "Epoch 76/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6336 - accuracy: 0.7281 - val_loss: 0.8107 - val_accuracy: 0.6517\n",
      "Epoch 77/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6333 - accuracy: 0.7243 - val_loss: 0.8068 - val_accuracy: 0.6547\n",
      "Epoch 78/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6303 - accuracy: 0.7279 - val_loss: 0.8035 - val_accuracy: 0.6502\n",
      "Epoch 79/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6274 - accuracy: 0.7283 - val_loss: 0.8031 - val_accuracy: 0.6532\n",
      "Epoch 80/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6269 - accuracy: 0.7271 - val_loss: 0.8016 - val_accuracy: 0.6517\n",
      "Epoch 81/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6231 - accuracy: 0.7306 - val_loss: 0.8049 - val_accuracy: 0.6532\n",
      "Epoch 82/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6259 - accuracy: 0.7281 - val_loss: 0.8294 - val_accuracy: 0.6532\n",
      "Epoch 83/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6239 - accuracy: 0.7289 - val_loss: 0.8120 - val_accuracy: 0.6532\n",
      "Epoch 84/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6192 - accuracy: 0.7334 - val_loss: 0.8087 - val_accuracy: 0.6449\n",
      "Epoch 85/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6196 - accuracy: 0.7286 - val_loss: 0.8024 - val_accuracy: 0.6464\n",
      "Epoch 86/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6159 - accuracy: 0.7360 - val_loss: 0.8116 - val_accuracy: 0.6479\n",
      "Epoch 87/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.6148 - accuracy: 0.7348 - val_loss: 0.8093 - val_accuracy: 0.6577\n",
      "Epoch 88/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.6115 - accuracy: 0.7368 - val_loss: 0.8098 - val_accuracy: 0.6554\n",
      "Epoch 89/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6114 - accuracy: 0.7357 - val_loss: 0.8023 - val_accuracy: 0.6404\n",
      "Epoch 90/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6088 - accuracy: 0.7395 - val_loss: 0.8117 - val_accuracy: 0.6584\n",
      "Epoch 91/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6102 - accuracy: 0.7366 - val_loss: 0.8038 - val_accuracy: 0.6441\n",
      "Epoch 92/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6051 - accuracy: 0.7414 - val_loss: 0.8032 - val_accuracy: 0.6479\n",
      "Epoch 93/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6037 - accuracy: 0.7393 - val_loss: 0.8036 - val_accuracy: 0.6419\n",
      "Epoch 94/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6033 - accuracy: 0.7420 - val_loss: 0.8044 - val_accuracy: 0.6434\n",
      "Epoch 95/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.6010 - accuracy: 0.7433 - val_loss: 0.8056 - val_accuracy: 0.6449\n",
      "Epoch 96/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6035 - accuracy: 0.7409 - val_loss: 0.8180 - val_accuracy: 0.6517\n",
      "Epoch 97/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.6006 - accuracy: 0.7421 - val_loss: 0.8079 - val_accuracy: 0.6456\n",
      "Epoch 98/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5990 - accuracy: 0.7445 - val_loss: 0.8065 - val_accuracy: 0.6524\n",
      "Epoch 99/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.5964 - accuracy: 0.7416 - val_loss: 0.8066 - val_accuracy: 0.6539\n",
      "Epoch 100/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5971 - accuracy: 0.7452 - val_loss: 0.8082 - val_accuracy: 0.6471\n",
      "Epoch 101/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.5922 - accuracy: 0.7449 - val_loss: 0.8095 - val_accuracy: 0.6456\n",
      "Epoch 102/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.5919 - accuracy: 0.7462 - val_loss: 0.8086 - val_accuracy: 0.6644\n",
      "Epoch 103/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5911 - accuracy: 0.7455 - val_loss: 0.8082 - val_accuracy: 0.6502\n",
      "Epoch 104/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.5888 - accuracy: 0.7501 - val_loss: 0.8096 - val_accuracy: 0.6509\n",
      "Epoch 105/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5883 - accuracy: 0.7481 - val_loss: 0.8082 - val_accuracy: 0.6577\n",
      "Epoch 106/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5855 - accuracy: 0.7513 - val_loss: 0.8126 - val_accuracy: 0.6517\n",
      "Epoch 107/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5845 - accuracy: 0.7491 - val_loss: 0.8116 - val_accuracy: 0.6471\n",
      "Epoch 108/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.5859 - accuracy: 0.7476 - val_loss: 0.8122 - val_accuracy: 0.6456\n",
      "Epoch 109/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.5845 - accuracy: 0.7486 - val_loss: 0.8168 - val_accuracy: 0.6554\n",
      "Epoch 110/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5838 - accuracy: 0.7510 - val_loss: 0.8193 - val_accuracy: 0.6471\n",
      "Epoch 111/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5849 - accuracy: 0.7501 - val_loss: 0.8098 - val_accuracy: 0.6464\n",
      "Epoch 112/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5798 - accuracy: 0.7514 - val_loss: 0.8151 - val_accuracy: 0.6359\n",
      "Epoch 113/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5838 - accuracy: 0.7492 - val_loss: 0.8182 - val_accuracy: 0.6554\n",
      "Epoch 114/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5795 - accuracy: 0.7508 - val_loss: 0.8098 - val_accuracy: 0.6449\n",
      "Epoch 115/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5799 - accuracy: 0.7499 - val_loss: 0.8127 - val_accuracy: 0.6404\n",
      "Epoch 116/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5749 - accuracy: 0.7555 - val_loss: 0.8087 - val_accuracy: 0.6441\n",
      "Epoch 117/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5747 - accuracy: 0.7561 - val_loss: 0.8148 - val_accuracy: 0.6509\n",
      "Epoch 118/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5739 - accuracy: 0.7580 - val_loss: 0.8120 - val_accuracy: 0.6464\n",
      "Epoch 119/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5727 - accuracy: 0.7580 - val_loss: 0.8130 - val_accuracy: 0.6441\n",
      "Epoch 120/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5721 - accuracy: 0.7574 - val_loss: 0.8161 - val_accuracy: 0.6449\n",
      "Epoch 121/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5685 - accuracy: 0.7580 - val_loss: 0.8231 - val_accuracy: 0.6449\n",
      "Epoch 122/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5666 - accuracy: 0.7586 - val_loss: 0.8144 - val_accuracy: 0.6456\n",
      "Epoch 123/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5651 - accuracy: 0.7626 - val_loss: 0.8144 - val_accuracy: 0.6449\n",
      "Epoch 124/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5644 - accuracy: 0.7603 - val_loss: 0.8133 - val_accuracy: 0.6471\n",
      "Epoch 125/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5645 - accuracy: 0.7610 - val_loss: 0.8210 - val_accuracy: 0.6389\n",
      "Epoch 126/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5648 - accuracy: 0.7594 - val_loss: 0.8199 - val_accuracy: 0.6502\n",
      "Epoch 127/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5630 - accuracy: 0.7634 - val_loss: 0.8134 - val_accuracy: 0.6441\n",
      "Epoch 128/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5606 - accuracy: 0.7619 - val_loss: 0.8244 - val_accuracy: 0.6532\n",
      "Epoch 129/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5644 - accuracy: 0.7573 - val_loss: 0.8262 - val_accuracy: 0.6389\n",
      "Epoch 130/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5587 - accuracy: 0.7608 - val_loss: 0.8148 - val_accuracy: 0.6494\n",
      "Epoch 131/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5562 - accuracy: 0.7652 - val_loss: 0.8169 - val_accuracy: 0.6464\n",
      "Epoch 132/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5589 - accuracy: 0.7595 - val_loss: 0.8248 - val_accuracy: 0.6471\n",
      "Epoch 133/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5571 - accuracy: 0.7654 - val_loss: 0.8219 - val_accuracy: 0.6502\n",
      "Epoch 134/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.5544 - accuracy: 0.7672 - val_loss: 0.8210 - val_accuracy: 0.6464\n",
      "Epoch 135/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5523 - accuracy: 0.7652 - val_loss: 0.8211 - val_accuracy: 0.6479\n",
      "Epoch 136/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5547 - accuracy: 0.7640 - val_loss: 0.8327 - val_accuracy: 0.6434\n",
      "Epoch 137/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5528 - accuracy: 0.7658 - val_loss: 0.8215 - val_accuracy: 0.6502\n",
      "Epoch 138/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5494 - accuracy: 0.7674 - val_loss: 0.8190 - val_accuracy: 0.6449\n",
      "Epoch 139/200\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.5485 - accuracy: 0.7703 - val_loss: 0.8267 - val_accuracy: 0.6502\n",
      "Epoch 140/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5499 - accuracy: 0.7652 - val_loss: 0.8200 - val_accuracy: 0.6479\n",
      "Epoch 141/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5473 - accuracy: 0.7682 - val_loss: 0.8230 - val_accuracy: 0.6494\n",
      "Epoch 142/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5456 - accuracy: 0.7705 - val_loss: 0.8252 - val_accuracy: 0.6434\n",
      "Epoch 143/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5469 - accuracy: 0.7695 - val_loss: 0.8303 - val_accuracy: 0.6509\n",
      "Epoch 144/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5461 - accuracy: 0.7697 - val_loss: 0.8204 - val_accuracy: 0.6464\n",
      "Epoch 145/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5440 - accuracy: 0.7715 - val_loss: 0.8244 - val_accuracy: 0.6456\n",
      "Epoch 146/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5433 - accuracy: 0.7697 - val_loss: 0.8268 - val_accuracy: 0.6502\n",
      "Epoch 147/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5486 - accuracy: 0.7674 - val_loss: 0.8272 - val_accuracy: 0.6524\n",
      "Epoch 148/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5423 - accuracy: 0.7697 - val_loss: 0.8276 - val_accuracy: 0.6464\n",
      "Epoch 149/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5417 - accuracy: 0.7730 - val_loss: 0.8257 - val_accuracy: 0.6517\n",
      "Epoch 150/200\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.5388 - accuracy: 0.7722 - val_loss: 0.8242 - val_accuracy: 0.6464\n",
      "Epoch 151/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5370 - accuracy: 0.7702 - val_loss: 0.8279 - val_accuracy: 0.6517\n",
      "Epoch 152/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5424 - accuracy: 0.7690 - val_loss: 0.8239 - val_accuracy: 0.6471\n",
      "Epoch 153/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5397 - accuracy: 0.7717 - val_loss: 0.8399 - val_accuracy: 0.6464\n",
      "Epoch 154/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5415 - accuracy: 0.7710 - val_loss: 0.8358 - val_accuracy: 0.6547\n",
      "Epoch 155/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5363 - accuracy: 0.7743 - val_loss: 0.8261 - val_accuracy: 0.6464\n",
      "Epoch 156/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5367 - accuracy: 0.7725 - val_loss: 0.8406 - val_accuracy: 0.6539\n",
      "Epoch 157/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5355 - accuracy: 0.7727 - val_loss: 0.8268 - val_accuracy: 0.6441\n",
      "Epoch 158/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5309 - accuracy: 0.7771 - val_loss: 0.8306 - val_accuracy: 0.6486\n",
      "Epoch 159/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5307 - accuracy: 0.7769 - val_loss: 0.8289 - val_accuracy: 0.6449\n",
      "Epoch 160/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5369 - accuracy: 0.7713 - val_loss: 0.8518 - val_accuracy: 0.6471\n",
      "Epoch 161/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5392 - accuracy: 0.7717 - val_loss: 0.8467 - val_accuracy: 0.6562\n",
      "Epoch 162/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5306 - accuracy: 0.7771 - val_loss: 0.8288 - val_accuracy: 0.6471\n",
      "Epoch 163/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5306 - accuracy: 0.7772 - val_loss: 0.8337 - val_accuracy: 0.6494\n",
      "Epoch 164/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5273 - accuracy: 0.7789 - val_loss: 0.8396 - val_accuracy: 0.6494\n",
      "Epoch 165/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5255 - accuracy: 0.7796 - val_loss: 0.8310 - val_accuracy: 0.6464\n",
      "Epoch 166/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5253 - accuracy: 0.7804 - val_loss: 0.8330 - val_accuracy: 0.6464\n",
      "Epoch 167/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5253 - accuracy: 0.7785 - val_loss: 0.8343 - val_accuracy: 0.6494\n",
      "Epoch 168/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5272 - accuracy: 0.7767 - val_loss: 0.8352 - val_accuracy: 0.6471\n",
      "Epoch 169/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5238 - accuracy: 0.7791 - val_loss: 0.8317 - val_accuracy: 0.6456\n",
      "Epoch 170/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5256 - accuracy: 0.7813 - val_loss: 0.8424 - val_accuracy: 0.6396\n",
      "Epoch 171/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5222 - accuracy: 0.7804 - val_loss: 0.8355 - val_accuracy: 0.6494\n",
      "Epoch 172/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5232 - accuracy: 0.7814 - val_loss: 0.8350 - val_accuracy: 0.6441\n",
      "Epoch 173/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.5203 - accuracy: 0.7823 - val_loss: 0.8365 - val_accuracy: 0.6456\n",
      "Epoch 174/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5186 - accuracy: 0.7838 - val_loss: 0.8394 - val_accuracy: 0.6471\n",
      "Epoch 175/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5197 - accuracy: 0.7835 - val_loss: 0.8347 - val_accuracy: 0.6509\n",
      "Epoch 176/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5159 - accuracy: 0.7843 - val_loss: 0.8361 - val_accuracy: 0.6479\n",
      "Epoch 177/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.5189 - accuracy: 0.7837 - val_loss: 0.8387 - val_accuracy: 0.6509\n",
      "Epoch 178/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5184 - accuracy: 0.7837 - val_loss: 0.8415 - val_accuracy: 0.6404\n",
      "Epoch 179/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5188 - accuracy: 0.7823 - val_loss: 0.8393 - val_accuracy: 0.6479\n",
      "Epoch 180/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.5167 - accuracy: 0.7841 - val_loss: 0.8453 - val_accuracy: 0.6524\n",
      "Epoch 181/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5165 - accuracy: 0.7827 - val_loss: 0.8378 - val_accuracy: 0.6479\n",
      "Epoch 182/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5166 - accuracy: 0.7844 - val_loss: 0.8459 - val_accuracy: 0.6532\n",
      "Epoch 183/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.5137 - accuracy: 0.7861 - val_loss: 0.8454 - val_accuracy: 0.6434\n",
      "Epoch 184/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5153 - accuracy: 0.7841 - val_loss: 0.8528 - val_accuracy: 0.6524\n",
      "Epoch 185/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5132 - accuracy: 0.7864 - val_loss: 0.8396 - val_accuracy: 0.6471\n",
      "Epoch 186/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.5105 - accuracy: 0.7861 - val_loss: 0.8450 - val_accuracy: 0.6449\n",
      "Epoch 187/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.5149 - accuracy: 0.7838 - val_loss: 0.8440 - val_accuracy: 0.6479\n",
      "Epoch 188/200\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.5127 - accuracy: 0.7833 - val_loss: 0.8487 - val_accuracy: 0.6494\n",
      "Epoch 189/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5083 - accuracy: 0.7890 - val_loss: 0.8469 - val_accuracy: 0.6486\n",
      "Epoch 190/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5077 - accuracy: 0.7901 - val_loss: 0.8459 - val_accuracy: 0.6464\n",
      "Epoch 191/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5075 - accuracy: 0.7867 - val_loss: 0.8538 - val_accuracy: 0.6509\n",
      "Epoch 192/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5066 - accuracy: 0.7918 - val_loss: 0.8448 - val_accuracy: 0.6456\n",
      "Epoch 193/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5044 - accuracy: 0.7883 - val_loss: 0.8476 - val_accuracy: 0.6502\n",
      "Epoch 194/200\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.5074 - accuracy: 0.7878 - val_loss: 0.8494 - val_accuracy: 0.6486\n",
      "Epoch 195/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5087 - accuracy: 0.7871 - val_loss: 0.8488 - val_accuracy: 0.6524\n",
      "Epoch 196/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5054 - accuracy: 0.7913 - val_loss: 0.8482 - val_accuracy: 0.6517\n",
      "Epoch 197/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5038 - accuracy: 0.7893 - val_loss: 0.8542 - val_accuracy: 0.6479\n",
      "Epoch 198/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.5012 - accuracy: 0.7944 - val_loss: 0.8490 - val_accuracy: 0.6486\n",
      "Epoch 199/200\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.5031 - accuracy: 0.7905 - val_loss: 0.8543 - val_accuracy: 0.6502\n",
      "Epoch 200/200\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.5016 - accuracy: 0.7911 - val_loss: 0.8481 - val_accuracy: 0.6486\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "\n",
    "inputs = Input((128,), dtype='int32')\n",
    "GloVe_Embeddings = embeddingLayer(inputs)\n",
    "CNN1D = Conv1D(filters = 100, kernel_size = 6, activation='relu')(GloVe_Embeddings)\n",
    "maxpool = GlobalAveragePooling1DMasked()(CNN1D)\n",
    "Output = Dense(3, activation = 'softmax')(maxpool)\n",
    "\n",
    "model22 = Model(inputs=[inputs],outputs=[Output])\n",
    "\n",
    "model22.summary()\n",
    "\n",
    "model22.compile(optimizer='adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history22 = model22.fit(x_train_pad_glove, y_train, epochs=200, batch_size=512, validation_data=(x_dev_pad_glove,y_dev), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1647369002983,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "wUTdRzAbmXKO",
    "outputId": "b09eb4f2-342f-467f-b493-cc8a52e958c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 5ms/step - loss: 0.8447 - accuracy: 0.6482\n",
      "loss: 0.8446930050849915 accuracy: 0.6482036113739014\n"
     ]
    }
   ],
   "source": [
    "results22 = model22.evaluate(x_test_pad_glove,y_test)\n",
    "print(\"loss:\",results22[0],\"accuracy:\",results22[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-bZ5SCHiIMl"
   },
   "source": [
    "#  Model 3: Model with multiple-input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G85QM3lSV7qp"
   },
   "source": [
    "Model 1 and 2 are copied from lab4. We build new models in this section. \n",
    "\n",
    "In models 1 and 2, we combine the reviews and aspects to input into the models. In model 3, we separately input these two data into the model and use different layers to analyze them. \n",
    "\n",
    "(This will give us a model similar to a simplified version of the Xue & Li model from the lectures - we have a separate paths through the network for the aspect embedding and the sentence, being combined - but we don't have to use gating like Xue & Li)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1647369003244,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "ztiFcOWuA0xH"
   },
   "outputs": [],
   "source": [
    "# First of all, pad the review and aspect separately\n",
    "x_train_review_pad_glove = keras.preprocessing.sequence.pad_sequences(x_train_review_glove,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=128)\n",
    "\n",
    "x_dev_review_pad_glove = keras.preprocessing.sequence.pad_sequences(x_dev_review_glove,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=128)\n",
    "\n",
    "x_test_review_pad_glove = keras.preprocessing.sequence.pad_sequences(x_test_review_glove,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=128)\n",
    "\n",
    "x_train_aspect_pad_glove = keras.preprocessing.sequence.pad_sequences(x_train_aspect_glove,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=16)\n",
    "\n",
    "x_dev_aspect_pad_glove = keras.preprocessing.sequence.pad_sequences(x_dev_aspect_glove,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=16)\n",
    "\n",
    "x_test_aspect_pad_glove = keras.preprocessing.sequence.pad_sequences(x_test_aspect_glove,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExgX8bxpVgps"
   },
   "source": [
    "## Model 3-1 Neural bag of words model with multiple-input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-TN6yup01mC"
   },
   "source": [
    "Model 3-1 needs you to modify the model 2-1 to be compatible with multiple-input.\n",
    "You could find some tutorial examples from (https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/).\n",
    "\n",
    "Please print the model summary and visualize it using vis_utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 196,
     "status": "ok",
     "timestamp": 1647369003436,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "dTgD_gMzXa1z",
    "outputId": "1533d4b1-fbc3-4e92-e2a5-afcd0368a475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 16)]         0           []                               \n",
      "                                                                                                  \n",
      " GloVe_Embeddings (Embedding)   multiple             120000300   ['input_5[0][0]',                \n",
      "                                                                  'input_6[0][0]']                \n",
      "                                                                                                  \n",
      " global_average_pooling1d_maske  (None, 300)         0           ['GloVe_Embeddings[2][0]']       \n",
      " d_3 (GlobalAveragePooling1DMas                                                                   \n",
      " ked)                                                                                             \n",
      "                                                                                                  \n",
      " global_average_pooling1d_maske  (None, 300)         0           ['GloVe_Embeddings[3][0]']       \n",
      " d_4 (GlobalAveragePooling1DMas                                                                   \n",
      " ked)                                                                                             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 16)           4816        ['global_average_pooling1d_masked\n",
      "                                                                 _3[0][0]']                       \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 16)           4816        ['global_average_pooling1d_masked\n",
      "                                                                 _4[0][0]']                       \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 32)           0           ['dense_6[0][0]',                \n",
      "                                                                  'dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 3)            99          ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 120,010,031\n",
      "Trainable params: 9,731\n",
      "Non-trainable params: 120,000,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dot, Concatenate\n",
    "# your code goes here\n",
    "\n",
    "input1 = Input((128,), dtype = 'int32')\n",
    "input2 = Input((16,), dtype='int32')\n",
    "embed1 = embeddingLayer(input1)\n",
    "embed2 = embeddingLayer(input2)\n",
    "global_avg_pooling1d_masked1= GlobalAveragePooling1DMasked()(embed1)\n",
    "global_avg_pooling1d_masked2= GlobalAveragePooling1DMasked()(embed2)\n",
    "Hidden_1 = Dense(16)(global_avg_pooling1d_masked1)\n",
    "Hidden_2 = Dense(16)(global_avg_pooling1d_masked2)\n",
    "merged_hidden = Concatenate(axis=-1)([Hidden_1, Hidden_2])\n",
    "Output = Dense(3, activation = 'softmax')(merged_hidden)\n",
    "\n",
    "model31 = Model(inputs=[input1, input2],outputs=[Output])\n",
    "\n",
    "model31.summary()\n",
    "\n",
    "model31.compile(optimizer='adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "executionInfo": {
     "elapsed": 637,
     "status": "ok",
     "timestamp": 1647369004072,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "UlJbDRHeAMIh",
    "outputId": "581372f4-3943-4bab-bf69-f7045075e4e8"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"392pt\" viewBox=\"0.00 0.00 1017.50 470.00\" width=\"848pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(.8333 .8333) rotate(0) translate(4 466)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-466 1013.5,-466 1013.5,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140320482599376 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140320482599376</title>\n",
       "<polygon fill=\"none\" points=\"171.5,-415.5 171.5,-461.5 501.5,-461.5 501.5,-415.5 171.5,-415.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"211.5\" y=\"-446.3\">input_5</text>\n",
       "<polyline fill=\"none\" points=\"171.5,-438.5 251.5,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"211.5\" y=\"-423.3\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"251.5,-415.5 251.5,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"251.5,-438.5 309.5,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"280.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"309.5,-415.5 309.5,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"357.5\" y=\"-434.8\">[(None, 128)]</text>\n",
       "<polyline fill=\"none\" points=\"405.5,-415.5 405.5,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"453.5\" y=\"-434.8\">[(None, 128)]</text>\n",
       "</g>\n",
       "<!-- 140316056250640 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140316056250640</title>\n",
       "<polygon fill=\"none\" points=\"387.5,-332.5 387.5,-378.5 625.5,-378.5 625.5,-332.5 387.5,-332.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"454.5\" y=\"-363.3\">GloVe_Embeddings</text>\n",
       "<polyline fill=\"none\" points=\"387.5,-355.5 521.5,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"454.5\" y=\"-340.3\">Embedding</text>\n",
       "<polyline fill=\"none\" points=\"521.5,-332.5 521.5,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"550.5\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"521.5,-355.5 579.5,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"550.5\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"579.5,-332.5 579.5,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"591\" y=\"-351.8\">?</text>\n",
       "<polyline fill=\"none\" points=\"602.5,-332.5 602.5,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"614\" y=\"-351.8\">?</text>\n",
       "</g>\n",
       "<!-- 140320482599376&#45;&gt;140316056250640 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140320482599376-&gt;140316056250640</title>\n",
       "<path d=\"M383.8545,-415.3799C404.4,-405.3488 428.6229,-393.5224 450.0915,-383.0406\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"451.6544,-386.1725 459.1049,-378.6399 448.5832,-379.8822 451.6544,-386.1725\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140316056284304 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140316056284304</title>\n",
       "<polygon fill=\"none\" points=\"519.5,-415.5 519.5,-461.5 835.5,-461.5 835.5,-415.5 519.5,-415.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"559.5\" y=\"-446.3\">input_6</text>\n",
       "<polyline fill=\"none\" points=\"519.5,-438.5 599.5,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"559.5\" y=\"-423.3\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"599.5,-415.5 599.5,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"628.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"599.5,-438.5 657.5,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"628.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"657.5,-415.5 657.5,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"702\" y=\"-434.8\">[(None, 16)]</text>\n",
       "<polyline fill=\"none\" points=\"746.5,-415.5 746.5,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"791\" y=\"-434.8\">[(None, 16)]</text>\n",
       "</g>\n",
       "<!-- 140316056284304&#45;&gt;140316056250640 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140316056284304-&gt;140316056250640</title>\n",
       "<path d=\"M629.867,-415.3799C609.2006,-405.3488 584.8352,-393.5224 563.2403,-383.0406\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"564.6985,-379.8579 554.1739,-378.6399 561.6418,-386.1553 564.6985,-379.8579\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140320482859152 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140320482859152</title>\n",
       "<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 499,-295.5 499,-249.5 0,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"118.5\" y=\"-280.3\">global_average_pooling1d_masked_3</text>\n",
       "<polyline fill=\"none\" points=\"0,-272.5 237,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"118.5\" y=\"-257.3\">GlobalAveragePooling1DMasked</text>\n",
       "<polyline fill=\"none\" points=\"237,-249.5 237,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"237,-272.5 295,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"266\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"295,-249.5 295,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"353.5\" y=\"-268.8\">(None, 128, 300)</text>\n",
       "<polyline fill=\"none\" points=\"412,-249.5 412,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"455.5\" y=\"-268.8\">(None, 300)</text>\n",
       "</g>\n",
       "<!-- 140316056250640&#45;&gt;140320482859152 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140316056250640-&gt;140320482859152</title>\n",
       "<path d=\"M435.2525,-332.4901C402.8035,-322.0105 364.232,-309.5535 330.6684,-298.7139\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"331.4778,-295.2974 320.8861,-295.5547 329.3265,-301.9586 331.4778,-295.2974\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140320482518096 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140320482518096</title>\n",
       "<polygon fill=\"none\" points=\"517.5,-249.5 517.5,-295.5 1009.5,-295.5 1009.5,-249.5 517.5,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"636\" y=\"-280.3\">global_average_pooling1d_masked_4</text>\n",
       "<polyline fill=\"none\" points=\"517.5,-272.5 754.5,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"636\" y=\"-257.3\">GlobalAveragePooling1DMasked</text>\n",
       "<polyline fill=\"none\" points=\"754.5,-249.5 754.5,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"783.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"754.5,-272.5 812.5,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"783.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"812.5,-249.5 812.5,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"867.5\" y=\"-268.8\">(None, 16, 300)</text>\n",
       "<polyline fill=\"none\" points=\"922.5,-249.5 922.5,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"966\" y=\"-268.8\">(None, 300)</text>\n",
       "</g>\n",
       "<!-- 140316056250640&#45;&gt;140320482518096 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140316056250640-&gt;140320482518096</title>\n",
       "<path d=\"M577.7475,-332.4901C610.1965,-322.0105 648.768,-309.5535 682.3316,-298.7139\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"683.6735,-301.9586 692.1139,-295.5547 681.5222,-295.2974 683.6735,-301.9586\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140320482970832 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140320482970832</title>\n",
       "<polygon fill=\"none\" points=\"208,-166.5 208,-212.5 497,-212.5 497,-166.5 208,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"240\" y=\"-197.3\">dense_6</text>\n",
       "<polyline fill=\"none\" points=\"208,-189.5 272,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"240\" y=\"-174.3\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"272,-166.5 272,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"301\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"272,-189.5 330,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"301\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"330,-166.5 330,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373.5\" y=\"-185.8\">(None, 300)</text>\n",
       "<polyline fill=\"none\" points=\"417,-166.5 417,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"457\" y=\"-185.8\">(None, 16)</text>\n",
       "</g>\n",
       "<!-- 140320482859152&#45;&gt;140320482970832 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140320482859152-&gt;140320482970832</title>\n",
       "<path d=\"M278.1912,-249.3799C289.8126,-240.0151 303.3758,-229.0855 315.7102,-219.1462\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"318.015,-221.7839 323.6054,-212.784 313.6227,-216.3333 318.015,-221.7839\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140320482967824 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140320482967824</title>\n",
       "<polygon fill=\"none\" points=\"567,-166.5 567,-212.5 856,-212.5 856,-166.5 567,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"599\" y=\"-197.3\">dense_7</text>\n",
       "<polyline fill=\"none\" points=\"567,-189.5 631,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"599\" y=\"-174.3\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"631,-166.5 631,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"660\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"631,-189.5 689,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"660\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"689,-166.5 689,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"732.5\" y=\"-185.8\">(None, 300)</text>\n",
       "<polyline fill=\"none\" points=\"776,-166.5 776,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"816\" y=\"-185.8\">(None, 16)</text>\n",
       "</g>\n",
       "<!-- 140320482518096&#45;&gt;140320482967824 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140320482518096-&gt;140320482967824</title>\n",
       "<path d=\"M749.0151,-249.3799C743.595,-240.7286 737.3383,-230.7419 731.5069,-221.4341\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"734.3628,-219.4 726.0876,-212.784 728.4308,-223.1165 734.3628,-219.4\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140320484130384 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140320484130384</title>\n",
       "<polygon fill=\"none\" points=\"314,-83.5 314,-129.5 697,-129.5 697,-83.5 314,-83.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"357\" y=\"-114.3\">concatenate</text>\n",
       "<polyline fill=\"none\" points=\"314,-106.5 400,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"357\" y=\"-91.3\">Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"400,-83.5 400,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"429\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"400,-106.5 458,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"429\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"458,-83.5 458,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"537.5\" y=\"-102.8\">[(None, 16), (None, 16)]</text>\n",
       "<polyline fill=\"none\" points=\"617,-83.5 617,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"657\" y=\"-102.8\">(None, 32)</text>\n",
       "</g>\n",
       "<!-- 140320482970832&#45;&gt;140320484130384 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140320482970832-&gt;140320484130384</title>\n",
       "<path d=\"M395.119,-166.3799C413.3624,-156.4832 434.8273,-144.8388 453.9547,-134.4625\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"455.7235,-137.4849 462.8444,-129.6399 452.3856,-131.3319 455.7235,-137.4849\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140320482967824&#45;&gt;140320484130384 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>140320482967824-&gt;140320484130384</title>\n",
       "<path d=\"M654.3911,-166.4901C628.9395,-156.2353 598.7888,-144.0872 572.2985,-133.414\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"573.3035,-130.0455 562.72,-129.5547 570.6874,-136.5383 573.3035,-130.0455\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140320482520400 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>140320482520400</title>\n",
       "<polygon fill=\"none\" points=\"368.5,-.5 368.5,-46.5 642.5,-46.5 642.5,-.5 368.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"400.5\" y=\"-31.3\">dense_8</text>\n",
       "<polyline fill=\"none\" points=\"368.5,-23.5 432.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"400.5\" y=\"-8.3\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"432.5,-.5 432.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"461.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"432.5,-23.5 490.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"461.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"490.5,-.5 490.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"530.5\" y=\"-19.8\">(None, 32)</text>\n",
       "<polyline fill=\"none\" points=\"570.5,-.5 570.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"606.5\" y=\"-19.8\">(None, 3)</text>\n",
       "</g>\n",
       "<!-- 140320484130384&#45;&gt;140320482520400 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>140320484130384-&gt;140320482520400</title>\n",
       "<path d=\"M505.5,-83.3799C505.5,-75.1745 505.5,-65.7679 505.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"509.0001,-56.784 505.5,-46.784 502.0001,-56.784 509.0001,-56.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils import vis_utils\n",
    "SVG(vis_utils.model_to_dot(model31, show_shapes=True, show_layer_names=True, dpi=60).create(prog='dot', format='svg'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVl_cKt903Z9"
   },
   "source": [
    "Train and evaluate your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39484,
     "status": "ok",
     "timestamp": 1647369043553,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "wEzz9deDF2rc",
    "outputId": "2c8aa184-5c6a-45c9-fe88-f810ebc172f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "22/22 [==============================] - 1s 16ms/step - loss: 1.0605 - accuracy: 0.4382 - val_loss: 1.0254 - val_accuracy: 0.4535\n",
      "Epoch 2/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 1.0180 - accuracy: 0.4511 - val_loss: 0.9926 - val_accuracy: 0.4520\n",
      "Epoch 3/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9885 - accuracy: 0.4869 - val_loss: 0.9641 - val_accuracy: 0.5188\n",
      "Epoch 4/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9621 - accuracy: 0.5463 - val_loss: 0.9385 - val_accuracy: 0.5601\n",
      "Epoch 5/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9388 - accuracy: 0.5712 - val_loss: 0.9168 - val_accuracy: 0.5803\n",
      "Epoch 6/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9185 - accuracy: 0.5828 - val_loss: 0.8977 - val_accuracy: 0.5983\n",
      "Epoch 7/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.9011 - accuracy: 0.5903 - val_loss: 0.8820 - val_accuracy: 0.6014\n",
      "Epoch 8/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8869 - accuracy: 0.5965 - val_loss: 0.8696 - val_accuracy: 0.6051\n",
      "Epoch 9/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8746 - accuracy: 0.6022 - val_loss: 0.8601 - val_accuracy: 0.6126\n",
      "Epoch 10/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8645 - accuracy: 0.6061 - val_loss: 0.8513 - val_accuracy: 0.6156\n",
      "Epoch 11/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8556 - accuracy: 0.6092 - val_loss: 0.8433 - val_accuracy: 0.6231\n",
      "Epoch 12/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8479 - accuracy: 0.6131 - val_loss: 0.8390 - val_accuracy: 0.6239\n",
      "Epoch 13/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8411 - accuracy: 0.6145 - val_loss: 0.8317 - val_accuracy: 0.6321\n",
      "Epoch 14/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8347 - accuracy: 0.6210 - val_loss: 0.8265 - val_accuracy: 0.6381\n",
      "Epoch 15/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8287 - accuracy: 0.6238 - val_loss: 0.8221 - val_accuracy: 0.6396\n",
      "Epoch 16/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8236 - accuracy: 0.6261 - val_loss: 0.8182 - val_accuracy: 0.6411\n",
      "Epoch 17/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8187 - accuracy: 0.6283 - val_loss: 0.8148 - val_accuracy: 0.6426\n",
      "Epoch 18/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8145 - accuracy: 0.6279 - val_loss: 0.8116 - val_accuracy: 0.6426\n",
      "Epoch 19/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8100 - accuracy: 0.6301 - val_loss: 0.8082 - val_accuracy: 0.6434\n",
      "Epoch 20/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8064 - accuracy: 0.6345 - val_loss: 0.8070 - val_accuracy: 0.6479\n",
      "Epoch 21/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.8029 - accuracy: 0.6356 - val_loss: 0.8044 - val_accuracy: 0.6486\n",
      "Epoch 22/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7995 - accuracy: 0.6364 - val_loss: 0.8013 - val_accuracy: 0.6547\n",
      "Epoch 23/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7966 - accuracy: 0.6405 - val_loss: 0.8000 - val_accuracy: 0.6502\n",
      "Epoch 24/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7940 - accuracy: 0.6394 - val_loss: 0.8016 - val_accuracy: 0.6486\n",
      "Epoch 25/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7929 - accuracy: 0.6414 - val_loss: 0.7979 - val_accuracy: 0.6524\n",
      "Epoch 26/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7894 - accuracy: 0.6439 - val_loss: 0.7966 - val_accuracy: 0.6622\n",
      "Epoch 27/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7877 - accuracy: 0.6482 - val_loss: 0.7956 - val_accuracy: 0.6614\n",
      "Epoch 28/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7861 - accuracy: 0.6480 - val_loss: 0.7936 - val_accuracy: 0.6659\n",
      "Epoch 29/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7830 - accuracy: 0.6494 - val_loss: 0.7931 - val_accuracy: 0.6547\n",
      "Epoch 30/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7814 - accuracy: 0.6510 - val_loss: 0.7913 - val_accuracy: 0.6652\n",
      "Epoch 31/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7799 - accuracy: 0.6515 - val_loss: 0.7916 - val_accuracy: 0.6554\n",
      "Epoch 32/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7781 - accuracy: 0.6526 - val_loss: 0.7905 - val_accuracy: 0.6622\n",
      "Epoch 33/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7769 - accuracy: 0.6524 - val_loss: 0.7902 - val_accuracy: 0.6622\n",
      "Epoch 34/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7763 - accuracy: 0.6519 - val_loss: 0.7898 - val_accuracy: 0.6629\n",
      "Epoch 35/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7747 - accuracy: 0.6546 - val_loss: 0.7929 - val_accuracy: 0.6494\n",
      "Epoch 36/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7734 - accuracy: 0.6550 - val_loss: 0.7885 - val_accuracy: 0.6629\n",
      "Epoch 37/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7720 - accuracy: 0.6542 - val_loss: 0.7877 - val_accuracy: 0.6667\n",
      "Epoch 38/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7724 - accuracy: 0.6547 - val_loss: 0.7913 - val_accuracy: 0.6464\n",
      "Epoch 39/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7700 - accuracy: 0.6581 - val_loss: 0.7887 - val_accuracy: 0.6584\n",
      "Epoch 40/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7688 - accuracy: 0.6556 - val_loss: 0.7881 - val_accuracy: 0.6592\n",
      "Epoch 41/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7670 - accuracy: 0.6581 - val_loss: 0.7876 - val_accuracy: 0.6622\n",
      "Epoch 42/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7675 - accuracy: 0.6566 - val_loss: 0.7873 - val_accuracy: 0.6629\n",
      "Epoch 43/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7665 - accuracy: 0.6574 - val_loss: 0.7871 - val_accuracy: 0.6637\n",
      "Epoch 44/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7650 - accuracy: 0.6594 - val_loss: 0.7866 - val_accuracy: 0.6629\n",
      "Epoch 45/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7639 - accuracy: 0.6606 - val_loss: 0.7865 - val_accuracy: 0.6622\n",
      "Epoch 46/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7628 - accuracy: 0.6612 - val_loss: 0.7871 - val_accuracy: 0.6622\n",
      "Epoch 47/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7625 - accuracy: 0.6600 - val_loss: 0.7905 - val_accuracy: 0.6464\n",
      "Epoch 48/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7625 - accuracy: 0.6628 - val_loss: 0.7883 - val_accuracy: 0.6599\n",
      "Epoch 49/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7620 - accuracy: 0.6611 - val_loss: 0.7878 - val_accuracy: 0.6607\n",
      "Epoch 50/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7608 - accuracy: 0.6616 - val_loss: 0.7863 - val_accuracy: 0.6629\n",
      "Epoch 51/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7599 - accuracy: 0.6631 - val_loss: 0.7870 - val_accuracy: 0.6599\n",
      "Epoch 52/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7594 - accuracy: 0.6609 - val_loss: 0.7867 - val_accuracy: 0.6592\n",
      "Epoch 53/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7589 - accuracy: 0.6624 - val_loss: 0.7865 - val_accuracy: 0.6637\n",
      "Epoch 54/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7580 - accuracy: 0.6638 - val_loss: 0.7861 - val_accuracy: 0.6629\n",
      "Epoch 55/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7579 - accuracy: 0.6642 - val_loss: 0.7867 - val_accuracy: 0.6622\n",
      "Epoch 56/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7582 - accuracy: 0.6628 - val_loss: 0.7883 - val_accuracy: 0.6524\n",
      "Epoch 57/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7571 - accuracy: 0.6651 - val_loss: 0.7892 - val_accuracy: 0.6532\n",
      "Epoch 58/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7561 - accuracy: 0.6648 - val_loss: 0.7909 - val_accuracy: 0.6494\n",
      "Epoch 59/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7562 - accuracy: 0.6648 - val_loss: 0.7899 - val_accuracy: 0.6509\n",
      "Epoch 60/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7545 - accuracy: 0.6651 - val_loss: 0.7868 - val_accuracy: 0.6607\n",
      "Epoch 61/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7539 - accuracy: 0.6652 - val_loss: 0.7886 - val_accuracy: 0.6622\n",
      "Epoch 62/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7541 - accuracy: 0.6669 - val_loss: 0.7875 - val_accuracy: 0.6614\n",
      "Epoch 63/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7535 - accuracy: 0.6667 - val_loss: 0.7874 - val_accuracy: 0.6599\n",
      "Epoch 64/200\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.7533 - accuracy: 0.6655 - val_loss: 0.7881 - val_accuracy: 0.6607\n",
      "Epoch 65/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7527 - accuracy: 0.6651 - val_loss: 0.7900 - val_accuracy: 0.6479\n",
      "Epoch 66/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7522 - accuracy: 0.6657 - val_loss: 0.7877 - val_accuracy: 0.6569\n",
      "Epoch 67/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7516 - accuracy: 0.6657 - val_loss: 0.7886 - val_accuracy: 0.6577\n",
      "Epoch 68/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7516 - accuracy: 0.6674 - val_loss: 0.7906 - val_accuracy: 0.6562\n",
      "Epoch 69/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7517 - accuracy: 0.6658 - val_loss: 0.7889 - val_accuracy: 0.6562\n",
      "Epoch 70/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7516 - accuracy: 0.6662 - val_loss: 0.7895 - val_accuracy: 0.6584\n",
      "Epoch 71/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7510 - accuracy: 0.6658 - val_loss: 0.7887 - val_accuracy: 0.6584\n",
      "Epoch 72/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7497 - accuracy: 0.6665 - val_loss: 0.7901 - val_accuracy: 0.6562\n",
      "Epoch 73/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7492 - accuracy: 0.6673 - val_loss: 0.7888 - val_accuracy: 0.6562\n",
      "Epoch 74/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7492 - accuracy: 0.6667 - val_loss: 0.7897 - val_accuracy: 0.6584\n",
      "Epoch 75/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7489 - accuracy: 0.6686 - val_loss: 0.7887 - val_accuracy: 0.6577\n",
      "Epoch 76/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7489 - accuracy: 0.6692 - val_loss: 0.7889 - val_accuracy: 0.6562\n",
      "Epoch 77/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7481 - accuracy: 0.6677 - val_loss: 0.7895 - val_accuracy: 0.6592\n",
      "Epoch 78/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7484 - accuracy: 0.6685 - val_loss: 0.7894 - val_accuracy: 0.6547\n",
      "Epoch 79/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7487 - accuracy: 0.6685 - val_loss: 0.7893 - val_accuracy: 0.6577\n",
      "Epoch 80/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7475 - accuracy: 0.6678 - val_loss: 0.7897 - val_accuracy: 0.6539\n",
      "Epoch 81/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7466 - accuracy: 0.6693 - val_loss: 0.7927 - val_accuracy: 0.6554\n",
      "Epoch 82/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7473 - accuracy: 0.6691 - val_loss: 0.7908 - val_accuracy: 0.6539\n",
      "Epoch 83/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7464 - accuracy: 0.6684 - val_loss: 0.7902 - val_accuracy: 0.6562\n",
      "Epoch 84/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7462 - accuracy: 0.6688 - val_loss: 0.7899 - val_accuracy: 0.6584\n",
      "Epoch 85/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7456 - accuracy: 0.6704 - val_loss: 0.7908 - val_accuracy: 0.6539\n",
      "Epoch 86/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7464 - accuracy: 0.6682 - val_loss: 0.7908 - val_accuracy: 0.6592\n",
      "Epoch 87/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7474 - accuracy: 0.6696 - val_loss: 0.7908 - val_accuracy: 0.6547\n",
      "Epoch 88/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7451 - accuracy: 0.6691 - val_loss: 0.7930 - val_accuracy: 0.6592\n",
      "Epoch 89/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7451 - accuracy: 0.6698 - val_loss: 0.7913 - val_accuracy: 0.6584\n",
      "Epoch 90/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7450 - accuracy: 0.6697 - val_loss: 0.7914 - val_accuracy: 0.6517\n",
      "Epoch 91/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7453 - accuracy: 0.6706 - val_loss: 0.7908 - val_accuracy: 0.6569\n",
      "Epoch 92/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7449 - accuracy: 0.6671 - val_loss: 0.7927 - val_accuracy: 0.6524\n",
      "Epoch 93/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7444 - accuracy: 0.6708 - val_loss: 0.7934 - val_accuracy: 0.6539\n",
      "Epoch 94/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7444 - accuracy: 0.6706 - val_loss: 0.7921 - val_accuracy: 0.6547\n",
      "Epoch 95/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7453 - accuracy: 0.6710 - val_loss: 0.7947 - val_accuracy: 0.6494\n",
      "Epoch 96/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7438 - accuracy: 0.6690 - val_loss: 0.7926 - val_accuracy: 0.6554\n",
      "Epoch 97/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7430 - accuracy: 0.6698 - val_loss: 0.7927 - val_accuracy: 0.6517\n",
      "Epoch 98/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7432 - accuracy: 0.6689 - val_loss: 0.7923 - val_accuracy: 0.6577\n",
      "Epoch 99/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7429 - accuracy: 0.6698 - val_loss: 0.7933 - val_accuracy: 0.6569\n",
      "Epoch 100/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7427 - accuracy: 0.6716 - val_loss: 0.7928 - val_accuracy: 0.6524\n",
      "Epoch 101/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7424 - accuracy: 0.6712 - val_loss: 0.7926 - val_accuracy: 0.6517\n",
      "Epoch 102/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7426 - accuracy: 0.6707 - val_loss: 0.7934 - val_accuracy: 0.6554\n",
      "Epoch 103/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7422 - accuracy: 0.6710 - val_loss: 0.7929 - val_accuracy: 0.6539\n",
      "Epoch 104/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7422 - accuracy: 0.6707 - val_loss: 0.7961 - val_accuracy: 0.6494\n",
      "Epoch 105/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7419 - accuracy: 0.6726 - val_loss: 0.7951 - val_accuracy: 0.6547\n",
      "Epoch 106/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7433 - accuracy: 0.6710 - val_loss: 0.7947 - val_accuracy: 0.6554\n",
      "Epoch 107/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7414 - accuracy: 0.6732 - val_loss: 0.7954 - val_accuracy: 0.6464\n",
      "Epoch 108/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7418 - accuracy: 0.6714 - val_loss: 0.7934 - val_accuracy: 0.6532\n",
      "Epoch 109/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7415 - accuracy: 0.6724 - val_loss: 0.7937 - val_accuracy: 0.6547\n",
      "Epoch 110/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7411 - accuracy: 0.6720 - val_loss: 0.7956 - val_accuracy: 0.6509\n",
      "Epoch 111/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7411 - accuracy: 0.6734 - val_loss: 0.7946 - val_accuracy: 0.6539\n",
      "Epoch 112/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7410 - accuracy: 0.6713 - val_loss: 0.7937 - val_accuracy: 0.6554\n",
      "Epoch 113/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7410 - accuracy: 0.6719 - val_loss: 0.7950 - val_accuracy: 0.6517\n",
      "Epoch 114/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7404 - accuracy: 0.6710 - val_loss: 0.7952 - val_accuracy: 0.6486\n",
      "Epoch 115/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7404 - accuracy: 0.6708 - val_loss: 0.7949 - val_accuracy: 0.6532\n",
      "Epoch 116/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7401 - accuracy: 0.6727 - val_loss: 0.7960 - val_accuracy: 0.6486\n",
      "Epoch 117/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7410 - accuracy: 0.6703 - val_loss: 0.7962 - val_accuracy: 0.6479\n",
      "Epoch 118/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7412 - accuracy: 0.6710 - val_loss: 0.7941 - val_accuracy: 0.6517\n",
      "Epoch 119/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7395 - accuracy: 0.6730 - val_loss: 0.7956 - val_accuracy: 0.6509\n",
      "Epoch 120/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7401 - accuracy: 0.6718 - val_loss: 0.7947 - val_accuracy: 0.6592\n",
      "Epoch 121/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7394 - accuracy: 0.6719 - val_loss: 0.7950 - val_accuracy: 0.6502\n",
      "Epoch 122/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7393 - accuracy: 0.6719 - val_loss: 0.7951 - val_accuracy: 0.6539\n",
      "Epoch 123/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7393 - accuracy: 0.6719 - val_loss: 0.7950 - val_accuracy: 0.6517\n",
      "Epoch 124/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7392 - accuracy: 0.6728 - val_loss: 0.7957 - val_accuracy: 0.6569\n",
      "Epoch 125/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7401 - accuracy: 0.6717 - val_loss: 0.7962 - val_accuracy: 0.6547\n",
      "Epoch 126/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7392 - accuracy: 0.6719 - val_loss: 0.7961 - val_accuracy: 0.6532\n",
      "Epoch 127/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7388 - accuracy: 0.6731 - val_loss: 0.7984 - val_accuracy: 0.6502\n",
      "Epoch 128/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7386 - accuracy: 0.6743 - val_loss: 0.7950 - val_accuracy: 0.6532\n",
      "Epoch 129/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7386 - accuracy: 0.6725 - val_loss: 0.7961 - val_accuracy: 0.6539\n",
      "Epoch 130/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7384 - accuracy: 0.6722 - val_loss: 0.7962 - val_accuracy: 0.6532\n",
      "Epoch 131/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7381 - accuracy: 0.6733 - val_loss: 0.7962 - val_accuracy: 0.6539\n",
      "Epoch 132/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7379 - accuracy: 0.6710 - val_loss: 0.7958 - val_accuracy: 0.6547\n",
      "Epoch 133/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7378 - accuracy: 0.6720 - val_loss: 0.7964 - val_accuracy: 0.6532\n",
      "Epoch 134/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7379 - accuracy: 0.6734 - val_loss: 0.7968 - val_accuracy: 0.6502\n",
      "Epoch 135/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7378 - accuracy: 0.6726 - val_loss: 0.7975 - val_accuracy: 0.6494\n",
      "Epoch 136/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7390 - accuracy: 0.6707 - val_loss: 0.7984 - val_accuracy: 0.6502\n",
      "Epoch 137/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7389 - accuracy: 0.6712 - val_loss: 0.7987 - val_accuracy: 0.6486\n",
      "Epoch 138/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7376 - accuracy: 0.6740 - val_loss: 0.7974 - val_accuracy: 0.6456\n",
      "Epoch 139/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7382 - accuracy: 0.6720 - val_loss: 0.7966 - val_accuracy: 0.6494\n",
      "Epoch 140/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7369 - accuracy: 0.6730 - val_loss: 0.7976 - val_accuracy: 0.6517\n",
      "Epoch 141/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7375 - accuracy: 0.6716 - val_loss: 0.7983 - val_accuracy: 0.6509\n",
      "Epoch 142/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7371 - accuracy: 0.6739 - val_loss: 0.7966 - val_accuracy: 0.6554\n",
      "Epoch 143/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7373 - accuracy: 0.6741 - val_loss: 0.7976 - val_accuracy: 0.6577\n",
      "Epoch 144/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7369 - accuracy: 0.6724 - val_loss: 0.7984 - val_accuracy: 0.6486\n",
      "Epoch 145/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7369 - accuracy: 0.6727 - val_loss: 0.7971 - val_accuracy: 0.6562\n",
      "Epoch 146/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7370 - accuracy: 0.6748 - val_loss: 0.7981 - val_accuracy: 0.6547\n",
      "Epoch 147/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7372 - accuracy: 0.6716 - val_loss: 0.7976 - val_accuracy: 0.6554\n",
      "Epoch 148/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7370 - accuracy: 0.6733 - val_loss: 0.7973 - val_accuracy: 0.6502\n",
      "Epoch 149/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7368 - accuracy: 0.6732 - val_loss: 0.7982 - val_accuracy: 0.6479\n",
      "Epoch 150/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7372 - accuracy: 0.6724 - val_loss: 0.7979 - val_accuracy: 0.6539\n",
      "Epoch 151/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7358 - accuracy: 0.6713 - val_loss: 0.7973 - val_accuracy: 0.6502\n",
      "Epoch 152/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7359 - accuracy: 0.6729 - val_loss: 0.7972 - val_accuracy: 0.6494\n",
      "Epoch 153/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7361 - accuracy: 0.6717 - val_loss: 0.7979 - val_accuracy: 0.6502\n",
      "Epoch 154/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7355 - accuracy: 0.6735 - val_loss: 0.7976 - val_accuracy: 0.6509\n",
      "Epoch 155/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7361 - accuracy: 0.6733 - val_loss: 0.8002 - val_accuracy: 0.6502\n",
      "Epoch 156/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7349 - accuracy: 0.6752 - val_loss: 0.8002 - val_accuracy: 0.6532\n",
      "Epoch 157/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7370 - accuracy: 0.6731 - val_loss: 0.7993 - val_accuracy: 0.6524\n",
      "Epoch 158/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7364 - accuracy: 0.6758 - val_loss: 0.7982 - val_accuracy: 0.6539\n",
      "Epoch 159/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7360 - accuracy: 0.6732 - val_loss: 0.7988 - val_accuracy: 0.6547\n",
      "Epoch 160/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7356 - accuracy: 0.6724 - val_loss: 0.7997 - val_accuracy: 0.6486\n",
      "Epoch 161/200\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.7355 - accuracy: 0.6738 - val_loss: 0.8022 - val_accuracy: 0.6494\n",
      "Epoch 162/200\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.7355 - accuracy: 0.6711 - val_loss: 0.7993 - val_accuracy: 0.6517\n",
      "Epoch 163/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7350 - accuracy: 0.6736 - val_loss: 0.7985 - val_accuracy: 0.6502\n",
      "Epoch 164/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7354 - accuracy: 0.6729 - val_loss: 0.8012 - val_accuracy: 0.6562\n",
      "Epoch 165/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7368 - accuracy: 0.6741 - val_loss: 0.7986 - val_accuracy: 0.6569\n",
      "Epoch 166/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7366 - accuracy: 0.6717 - val_loss: 0.7984 - val_accuracy: 0.6494\n",
      "Epoch 167/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7348 - accuracy: 0.6735 - val_loss: 0.7981 - val_accuracy: 0.6547\n",
      "Epoch 168/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7353 - accuracy: 0.6728 - val_loss: 0.7986 - val_accuracy: 0.6479\n",
      "Epoch 169/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7343 - accuracy: 0.6753 - val_loss: 0.7993 - val_accuracy: 0.6524\n",
      "Epoch 170/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7352 - accuracy: 0.6750 - val_loss: 0.7995 - val_accuracy: 0.6547\n",
      "Epoch 171/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7344 - accuracy: 0.6750 - val_loss: 0.7985 - val_accuracy: 0.6509\n",
      "Epoch 172/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7344 - accuracy: 0.6752 - val_loss: 0.8001 - val_accuracy: 0.6517\n",
      "Epoch 173/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7344 - accuracy: 0.6758 - val_loss: 0.8002 - val_accuracy: 0.6471\n",
      "Epoch 174/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7351 - accuracy: 0.6740 - val_loss: 0.8009 - val_accuracy: 0.6464\n",
      "Epoch 175/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7366 - accuracy: 0.6709 - val_loss: 0.7993 - val_accuracy: 0.6554\n",
      "Epoch 176/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7344 - accuracy: 0.6732 - val_loss: 0.7993 - val_accuracy: 0.6494\n",
      "Epoch 177/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7344 - accuracy: 0.6747 - val_loss: 0.8030 - val_accuracy: 0.6471\n",
      "Epoch 178/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7341 - accuracy: 0.6741 - val_loss: 0.7993 - val_accuracy: 0.6547\n",
      "Epoch 179/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7341 - accuracy: 0.6739 - val_loss: 0.7999 - val_accuracy: 0.6554\n",
      "Epoch 180/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7340 - accuracy: 0.6751 - val_loss: 0.8007 - val_accuracy: 0.6502\n",
      "Epoch 181/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7338 - accuracy: 0.6745 - val_loss: 0.8013 - val_accuracy: 0.6509\n",
      "Epoch 182/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7339 - accuracy: 0.6748 - val_loss: 0.8002 - val_accuracy: 0.6517\n",
      "Epoch 183/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7340 - accuracy: 0.6754 - val_loss: 0.8036 - val_accuracy: 0.6479\n",
      "Epoch 184/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7342 - accuracy: 0.6732 - val_loss: 0.8000 - val_accuracy: 0.6539\n",
      "Epoch 185/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7337 - accuracy: 0.6758 - val_loss: 0.8017 - val_accuracy: 0.6471\n",
      "Epoch 186/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7332 - accuracy: 0.6741 - val_loss: 0.7994 - val_accuracy: 0.6554\n",
      "Epoch 187/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7333 - accuracy: 0.6764 - val_loss: 0.7999 - val_accuracy: 0.6569\n",
      "Epoch 188/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7336 - accuracy: 0.6740 - val_loss: 0.8003 - val_accuracy: 0.6539\n",
      "Epoch 189/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7341 - accuracy: 0.6750 - val_loss: 0.8005 - val_accuracy: 0.6494\n",
      "Epoch 190/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7337 - accuracy: 0.6748 - val_loss: 0.8010 - val_accuracy: 0.6494\n",
      "Epoch 191/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7336 - accuracy: 0.6741 - val_loss: 0.8062 - val_accuracy: 0.6464\n",
      "Epoch 192/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7335 - accuracy: 0.6726 - val_loss: 0.8012 - val_accuracy: 0.6479\n",
      "Epoch 193/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7345 - accuracy: 0.6730 - val_loss: 0.8010 - val_accuracy: 0.6486\n",
      "Epoch 194/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7338 - accuracy: 0.6758 - val_loss: 0.7997 - val_accuracy: 0.6517\n",
      "Epoch 195/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7339 - accuracy: 0.6723 - val_loss: 0.8015 - val_accuracy: 0.6486\n",
      "Epoch 196/200\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.7330 - accuracy: 0.6751 - val_loss: 0.8011 - val_accuracy: 0.6524\n",
      "Epoch 197/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7330 - accuracy: 0.6763 - val_loss: 0.8006 - val_accuracy: 0.6494\n",
      "Epoch 198/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7327 - accuracy: 0.6741 - val_loss: 0.8005 - val_accuracy: 0.6494\n",
      "Epoch 199/200\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.7325 - accuracy: 0.6746 - val_loss: 0.8029 - val_accuracy: 0.6471\n",
      "Epoch 200/200\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.7330 - accuracy: 0.6736 - val_loss: 0.8012 - val_accuracy: 0.6486\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.7822 - accuracy: 0.6527\n",
      "loss: 0.782159686088562 accuracy: 0.652694582939148\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "history31 = model31.fit([x_train_review_pad_glove, x_train_aspect_pad_glove], y_train, epochs=200, batch_size=512, validation_data=([x_dev_review_pad_glove,x_dev_aspect_pad_glove],y_dev), verbose=1)\n",
    "\n",
    "results31 = model31.evaluate([x_test_review_pad_glove,x_test_aspect_pad_glove],y_test)\n",
    "\n",
    "print(\"loss:\",results31[0],\"accuracy:\",results31[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0npTvFuVt5R"
   },
   "source": [
    "## Model 3-2 CNN or LSTM model with multiple-input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCp9khOn0D2-"
   },
   "source": [
    "Modify the previous CNN or LSTM model to be compatible with multiple-input, similar to model 3-1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1647369043731,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "Js5yZqmYywev",
    "outputId": "3d0dfc8c-6694-4039-dbc9-ca0d456c23c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 16)]         0           []                               \n",
      "                                                                                                  \n",
      " GloVe_Embeddings (Embedding)   multiple             120000300   ['input_7[0][0]',                \n",
      "                                                                  'input_8[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 123, 100)     180100      ['GloVe_Embeddings[4][0]']       \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 11, 100)      180100      ['GloVe_Embeddings[5][0]']       \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 100)         0           ['conv1d_2[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_2 (Global  (None, 100)         0           ['conv1d_3[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 200)          0           ['global_max_pooling1d_1[0][0]', \n",
      "                                                                  'global_max_pooling1d_2[0][0]'] \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 3)            603         ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 120,361,103\n",
      "Trainable params: 360,803\n",
      "Non-trainable params: 120,000,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import concatenate\n",
    "\n",
    "input1 = Input((128,))\n",
    "input2 = Input((16,))\n",
    "embed1 = embeddingLayer(input1)\n",
    "embed2 = embeddingLayer(input2)\n",
    "CNN1D1 = Conv1D(filters = 100, kernel_size = 6, activation = 'relu')(embed1)\n",
    "CNN1D2 = Conv1D(filters = 100, kernel_size = 6, activation = 'relu')(embed2)\n",
    "maxpool1 = GlobalMaxPooling1D()(CNN1D1)\n",
    "maxpool2 = GlobalMaxPooling1D()(CNN1D2)\n",
    "merged_maxpool = concatenate([maxpool1, maxpool2],axis=-1)\n",
    "Output = Dense(3, activation = 'softmax')(merged_maxpool)\n",
    "\n",
    "model32 = Model(inputs=[input1,input2],outputs=[Output])\n",
    "\n",
    "model32.summary()\n",
    "\n",
    "model32.compile(optimizer='adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 263547,
     "status": "ok",
     "timestamp": 1647369307277,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "jXJCzwVwZhIP",
    "outputId": "b0930f45-86e9-4e50-c794-97c46aa1c597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "22/22 [==============================] - 3s 75ms/step - loss: 0.9899 - acc: 0.5131 - val_loss: 0.8367 - val_acc: 0.6321\n",
      "Epoch 2/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.7845 - acc: 0.6547 - val_loss: 0.8042 - val_acc: 0.6524\n",
      "Epoch 3/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.7199 - acc: 0.6949 - val_loss: 0.7830 - val_acc: 0.6524\n",
      "Epoch 4/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.6740 - acc: 0.7205 - val_loss: 0.7905 - val_acc: 0.6539\n",
      "Epoch 5/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.6393 - acc: 0.7368 - val_loss: 0.7723 - val_acc: 0.6539\n",
      "Epoch 6/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.6079 - acc: 0.7525 - val_loss: 0.7722 - val_acc: 0.6689\n",
      "Epoch 7/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.5827 - acc: 0.7639 - val_loss: 0.7709 - val_acc: 0.6637\n",
      "Epoch 8/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.5556 - acc: 0.7773 - val_loss: 0.7733 - val_acc: 0.6749\n",
      "Epoch 9/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.5351 - acc: 0.7888 - val_loss: 0.7762 - val_acc: 0.6584\n",
      "Epoch 10/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.5176 - acc: 0.7985 - val_loss: 0.7791 - val_acc: 0.6607\n",
      "Epoch 11/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.4981 - acc: 0.8055 - val_loss: 0.7902 - val_acc: 0.6719\n",
      "Epoch 12/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.4813 - acc: 0.8121 - val_loss: 0.8159 - val_acc: 0.6652\n",
      "Epoch 13/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.4692 - acc: 0.8181 - val_loss: 0.8017 - val_acc: 0.6697\n",
      "Epoch 14/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.4554 - acc: 0.8280 - val_loss: 0.8100 - val_acc: 0.6599\n",
      "Epoch 15/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.4431 - acc: 0.8326 - val_loss: 0.8177 - val_acc: 0.6629\n",
      "Epoch 16/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.4356 - acc: 0.8331 - val_loss: 0.8351 - val_acc: 0.6584\n",
      "Epoch 17/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.4334 - acc: 0.8335 - val_loss: 0.8439 - val_acc: 0.6719\n",
      "Epoch 18/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.4195 - acc: 0.8433 - val_loss: 0.8436 - val_acc: 0.6667\n",
      "Epoch 19/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.4056 - acc: 0.8495 - val_loss: 0.8533 - val_acc: 0.6749\n",
      "Epoch 20/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.3977 - acc: 0.8537 - val_loss: 0.8559 - val_acc: 0.6659\n",
      "Epoch 21/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.3906 - acc: 0.8569 - val_loss: 0.8517 - val_acc: 0.6659\n",
      "Epoch 22/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.3877 - acc: 0.8563 - val_loss: 0.8670 - val_acc: 0.6644\n",
      "Epoch 23/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.3764 - acc: 0.8616 - val_loss: 0.8835 - val_acc: 0.6644\n",
      "Epoch 24/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.3768 - acc: 0.8593 - val_loss: 0.8909 - val_acc: 0.6629\n",
      "Epoch 25/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.3690 - acc: 0.8638 - val_loss: 0.8872 - val_acc: 0.6674\n",
      "Epoch 26/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.3624 - acc: 0.8680 - val_loss: 0.9122 - val_acc: 0.6652\n",
      "Epoch 27/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.3636 - acc: 0.8643 - val_loss: 0.9183 - val_acc: 0.6629\n",
      "Epoch 28/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.3627 - acc: 0.8651 - val_loss: 0.9214 - val_acc: 0.6749\n",
      "Epoch 29/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.3550 - acc: 0.8681 - val_loss: 0.9165 - val_acc: 0.6652\n",
      "Epoch 30/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.3490 - acc: 0.8721 - val_loss: 0.9474 - val_acc: 0.6712\n",
      "Epoch 31/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.3520 - acc: 0.8678 - val_loss: 0.9431 - val_acc: 0.6652\n",
      "Epoch 32/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.3408 - acc: 0.8748 - val_loss: 0.9458 - val_acc: 0.6629\n",
      "Epoch 33/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.3415 - acc: 0.8728 - val_loss: 0.9682 - val_acc: 0.6674\n",
      "Epoch 34/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.3446 - acc: 0.8703 - val_loss: 0.9770 - val_acc: 0.6629\n",
      "Epoch 35/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.3359 - acc: 0.8752 - val_loss: 0.9843 - val_acc: 0.6697\n",
      "Epoch 36/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.3375 - acc: 0.8713 - val_loss: 0.9726 - val_acc: 0.6592\n",
      "Epoch 37/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.3299 - acc: 0.8775 - val_loss: 0.9960 - val_acc: 0.6569\n",
      "Epoch 38/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.3278 - acc: 0.8779 - val_loss: 1.0060 - val_acc: 0.6532\n",
      "Epoch 39/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.3302 - acc: 0.8722 - val_loss: 1.0058 - val_acc: 0.6524\n",
      "Epoch 40/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.3271 - acc: 0.8739 - val_loss: 1.0071 - val_acc: 0.6584\n",
      "Epoch 41/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.3249 - acc: 0.8775 - val_loss: 1.0091 - val_acc: 0.6569\n",
      "Epoch 42/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.3221 - acc: 0.8762 - val_loss: 1.0083 - val_acc: 0.6622\n",
      "Epoch 43/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.3188 - acc: 0.8808 - val_loss: 1.0216 - val_acc: 0.6517\n",
      "Epoch 44/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.3169 - acc: 0.8788 - val_loss: 1.0226 - val_acc: 0.6607\n",
      "Epoch 45/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.3226 - acc: 0.8756 - val_loss: 1.0669 - val_acc: 0.6659\n",
      "Epoch 46/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.3156 - acc: 0.8790 - val_loss: 1.0458 - val_acc: 0.6539\n",
      "Epoch 47/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.3150 - acc: 0.8821 - val_loss: 1.0482 - val_acc: 0.6532\n",
      "Epoch 48/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.3156 - acc: 0.8797 - val_loss: 1.0552 - val_acc: 0.6547\n",
      "Epoch 49/200\n",
      "22/22 [==============================] - 1s 54ms/step - loss: 0.3142 - acc: 0.8805 - val_loss: 1.0441 - val_acc: 0.6456\n",
      "Epoch 50/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.3107 - acc: 0.8802 - val_loss: 1.0616 - val_acc: 0.6569\n",
      "Epoch 51/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.3106 - acc: 0.8813 - val_loss: 1.0629 - val_acc: 0.6584\n",
      "Epoch 52/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.3071 - acc: 0.8820 - val_loss: 1.1112 - val_acc: 0.6682\n",
      "Epoch 53/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.3120 - acc: 0.8796 - val_loss: 1.1002 - val_acc: 0.6584\n",
      "Epoch 54/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.3085 - acc: 0.8806 - val_loss: 1.1084 - val_acc: 0.6539\n",
      "Epoch 55/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.3081 - acc: 0.8819 - val_loss: 1.1000 - val_acc: 0.6562\n",
      "Epoch 56/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.3066 - acc: 0.8828 - val_loss: 1.0919 - val_acc: 0.6517\n",
      "Epoch 57/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.3017 - acc: 0.8849 - val_loss: 1.0938 - val_acc: 0.6554\n",
      "Epoch 58/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2975 - acc: 0.8890 - val_loss: 1.1014 - val_acc: 0.6562\n",
      "Epoch 59/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.3032 - acc: 0.8841 - val_loss: 1.1423 - val_acc: 0.6539\n",
      "Epoch 60/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.3020 - acc: 0.8851 - val_loss: 1.1212 - val_acc: 0.6554\n",
      "Epoch 61/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.3087 - acc: 0.8810 - val_loss: 1.1113 - val_acc: 0.6434\n",
      "Epoch 62/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.3041 - acc: 0.8825 - val_loss: 1.1350 - val_acc: 0.6532\n",
      "Epoch 63/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2969 - acc: 0.8882 - val_loss: 1.1371 - val_acc: 0.6562\n",
      "Epoch 64/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2984 - acc: 0.8832 - val_loss: 1.1457 - val_acc: 0.6539\n",
      "Epoch 65/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.3012 - acc: 0.8841 - val_loss: 1.1341 - val_acc: 0.6464\n",
      "Epoch 66/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.2964 - acc: 0.8855 - val_loss: 1.1364 - val_acc: 0.6441\n",
      "Epoch 67/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.2930 - acc: 0.8896 - val_loss: 1.1490 - val_acc: 0.6592\n",
      "Epoch 68/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.2988 - acc: 0.8820 - val_loss: 1.1691 - val_acc: 0.6554\n",
      "Epoch 69/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2940 - acc: 0.8856 - val_loss: 1.1856 - val_acc: 0.6584\n",
      "Epoch 70/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2956 - acc: 0.8843 - val_loss: 1.1647 - val_acc: 0.6524\n",
      "Epoch 71/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2946 - acc: 0.8874 - val_loss: 1.1683 - val_acc: 0.6509\n",
      "Epoch 72/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2952 - acc: 0.8855 - val_loss: 1.1832 - val_acc: 0.6486\n",
      "Epoch 73/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2931 - acc: 0.8870 - val_loss: 1.1635 - val_acc: 0.6577\n",
      "Epoch 74/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.2905 - acc: 0.8891 - val_loss: 1.2009 - val_acc: 0.6502\n",
      "Epoch 75/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.2938 - acc: 0.8871 - val_loss: 1.1914 - val_acc: 0.6532\n",
      "Epoch 76/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2969 - acc: 0.8855 - val_loss: 1.1687 - val_acc: 0.6344\n",
      "Epoch 77/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2966 - acc: 0.8845 - val_loss: 1.1698 - val_acc: 0.6494\n",
      "Epoch 78/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2936 - acc: 0.8837 - val_loss: 1.1940 - val_acc: 0.6321\n",
      "Epoch 79/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.2991 - acc: 0.8807 - val_loss: 1.2718 - val_acc: 0.6532\n",
      "Epoch 80/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.2933 - acc: 0.8871 - val_loss: 1.1807 - val_acc: 0.6471\n",
      "Epoch 81/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2897 - acc: 0.8854 - val_loss: 1.1957 - val_acc: 0.6494\n",
      "Epoch 82/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2910 - acc: 0.8871 - val_loss: 1.2088 - val_acc: 0.6456\n",
      "Epoch 83/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2885 - acc: 0.8843 - val_loss: 1.2411 - val_acc: 0.6479\n",
      "Epoch 84/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2876 - acc: 0.8860 - val_loss: 1.1890 - val_acc: 0.6389\n",
      "Epoch 85/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.2870 - acc: 0.8868 - val_loss: 1.2113 - val_acc: 0.6321\n",
      "Epoch 86/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.2882 - acc: 0.8885 - val_loss: 1.1915 - val_acc: 0.6479\n",
      "Epoch 87/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2904 - acc: 0.8841 - val_loss: 1.2067 - val_acc: 0.6411\n",
      "Epoch 88/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2866 - acc: 0.8887 - val_loss: 1.2063 - val_acc: 0.6396\n",
      "Epoch 89/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2872 - acc: 0.8891 - val_loss: 1.2407 - val_acc: 0.6389\n",
      "Epoch 90/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2862 - acc: 0.8888 - val_loss: 1.2403 - val_acc: 0.6509\n",
      "Epoch 91/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.2852 - acc: 0.8895 - val_loss: 1.2376 - val_acc: 0.6479\n",
      "Epoch 92/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2815 - acc: 0.8898 - val_loss: 1.2458 - val_acc: 0.6464\n",
      "Epoch 93/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.2807 - acc: 0.8902 - val_loss: 1.2234 - val_acc: 0.6434\n",
      "Epoch 94/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2855 - acc: 0.8872 - val_loss: 1.2681 - val_acc: 0.6426\n",
      "Epoch 95/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2884 - acc: 0.8844 - val_loss: 1.2368 - val_acc: 0.6434\n",
      "Epoch 96/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2875 - acc: 0.8866 - val_loss: 1.2578 - val_acc: 0.6562\n",
      "Epoch 97/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.2867 - acc: 0.8870 - val_loss: 1.2581 - val_acc: 0.6471\n",
      "Epoch 98/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2861 - acc: 0.8845 - val_loss: 1.2391 - val_acc: 0.6471\n",
      "Epoch 99/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2874 - acc: 0.8878 - val_loss: 1.2577 - val_acc: 0.6471\n",
      "Epoch 100/200\n",
      "22/22 [==============================] - 1s 54ms/step - loss: 0.2892 - acc: 0.8852 - val_loss: 1.2405 - val_acc: 0.6374\n",
      "Epoch 101/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.2886 - acc: 0.8858 - val_loss: 1.2735 - val_acc: 0.6464\n",
      "Epoch 102/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2850 - acc: 0.8864 - val_loss: 1.2433 - val_acc: 0.6464\n",
      "Epoch 103/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2815 - acc: 0.8898 - val_loss: 1.2580 - val_acc: 0.6441\n",
      "Epoch 104/200\n",
      "22/22 [==============================] - 1s 54ms/step - loss: 0.2786 - acc: 0.8918 - val_loss: 1.2685 - val_acc: 0.6471\n",
      "Epoch 105/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2828 - acc: 0.8879 - val_loss: 1.3235 - val_acc: 0.6396\n",
      "Epoch 106/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2896 - acc: 0.8833 - val_loss: 1.2660 - val_acc: 0.6374\n",
      "Epoch 107/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.2827 - acc: 0.8870 - val_loss: 1.2547 - val_acc: 0.6344\n",
      "Epoch 108/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2794 - acc: 0.8902 - val_loss: 1.2653 - val_acc: 0.6434\n",
      "Epoch 109/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2792 - acc: 0.8890 - val_loss: 1.2760 - val_acc: 0.6471\n",
      "Epoch 110/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2807 - acc: 0.8874 - val_loss: 1.2893 - val_acc: 0.6389\n",
      "Epoch 111/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.2816 - acc: 0.8896 - val_loss: 1.2975 - val_acc: 0.6314\n",
      "Epoch 112/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.2863 - acc: 0.8865 - val_loss: 1.2891 - val_acc: 0.6419\n",
      "Epoch 113/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2796 - acc: 0.8876 - val_loss: 1.2691 - val_acc: 0.6434\n",
      "Epoch 114/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.2777 - acc: 0.8867 - val_loss: 1.2794 - val_acc: 0.6502\n",
      "Epoch 115/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2797 - acc: 0.8895 - val_loss: 1.2781 - val_acc: 0.6456\n",
      "Epoch 116/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2805 - acc: 0.8880 - val_loss: 1.2834 - val_acc: 0.6441\n",
      "Epoch 117/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2765 - acc: 0.8908 - val_loss: 1.3484 - val_acc: 0.6464\n",
      "Epoch 118/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2809 - acc: 0.8903 - val_loss: 1.3067 - val_acc: 0.6404\n",
      "Epoch 119/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2758 - acc: 0.8908 - val_loss: 1.3128 - val_acc: 0.6344\n",
      "Epoch 120/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2749 - acc: 0.8908 - val_loss: 1.2883 - val_acc: 0.6441\n",
      "Epoch 121/200\n",
      "22/22 [==============================] - 1s 54ms/step - loss: 0.2815 - acc: 0.8893 - val_loss: 1.2987 - val_acc: 0.6471\n",
      "Epoch 122/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2807 - acc: 0.8856 - val_loss: 1.3428 - val_acc: 0.6374\n",
      "Epoch 123/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2776 - acc: 0.8885 - val_loss: 1.2913 - val_acc: 0.6411\n",
      "Epoch 124/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2792 - acc: 0.8890 - val_loss: 1.3153 - val_acc: 0.6456\n",
      "Epoch 125/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2787 - acc: 0.8906 - val_loss: 1.3072 - val_acc: 0.6411\n",
      "Epoch 126/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2824 - acc: 0.8869 - val_loss: 1.3299 - val_acc: 0.6449\n",
      "Epoch 127/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2826 - acc: 0.8876 - val_loss: 1.3182 - val_acc: 0.6389\n",
      "Epoch 128/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.2790 - acc: 0.8904 - val_loss: 1.3440 - val_acc: 0.6404\n",
      "Epoch 129/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2818 - acc: 0.8885 - val_loss: 1.3858 - val_acc: 0.6419\n",
      "Epoch 130/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2823 - acc: 0.8876 - val_loss: 1.3552 - val_acc: 0.6404\n",
      "Epoch 131/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2796 - acc: 0.8883 - val_loss: 1.3618 - val_acc: 0.6449\n",
      "Epoch 132/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2798 - acc: 0.8899 - val_loss: 1.3258 - val_acc: 0.6381\n",
      "Epoch 133/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2754 - acc: 0.8892 - val_loss: 1.3684 - val_acc: 0.6411\n",
      "Epoch 134/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2771 - acc: 0.8900 - val_loss: 1.3621 - val_acc: 0.6366\n",
      "Epoch 135/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2757 - acc: 0.8882 - val_loss: 1.3329 - val_acc: 0.6441\n",
      "Epoch 136/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2740 - acc: 0.8910 - val_loss: 1.3606 - val_acc: 0.6449\n",
      "Epoch 137/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2746 - acc: 0.8896 - val_loss: 1.3368 - val_acc: 0.6396\n",
      "Epoch 138/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.2773 - acc: 0.8871 - val_loss: 1.3488 - val_acc: 0.6381\n",
      "Epoch 139/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2752 - acc: 0.8875 - val_loss: 1.3654 - val_acc: 0.6389\n",
      "Epoch 140/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.2716 - acc: 0.8924 - val_loss: 1.3643 - val_acc: 0.6359\n",
      "Epoch 141/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.2701 - acc: 0.8916 - val_loss: 1.3450 - val_acc: 0.6344\n",
      "Epoch 142/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2722 - acc: 0.8894 - val_loss: 1.3479 - val_acc: 0.6374\n",
      "Epoch 143/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2801 - acc: 0.8842 - val_loss: 1.3682 - val_acc: 0.6411\n",
      "Epoch 144/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2783 - acc: 0.8908 - val_loss: 1.3399 - val_acc: 0.6441\n",
      "Epoch 145/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2748 - acc: 0.8917 - val_loss: 1.3886 - val_acc: 0.6419\n",
      "Epoch 146/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2738 - acc: 0.8891 - val_loss: 1.3669 - val_acc: 0.6404\n",
      "Epoch 147/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2750 - acc: 0.8921 - val_loss: 1.3791 - val_acc: 0.6404\n",
      "Epoch 148/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2776 - acc: 0.8874 - val_loss: 1.3899 - val_acc: 0.6321\n",
      "Epoch 149/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2824 - acc: 0.8872 - val_loss: 1.3647 - val_acc: 0.6404\n",
      "Epoch 150/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2786 - acc: 0.8882 - val_loss: 1.3684 - val_acc: 0.6426\n",
      "Epoch 151/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.2714 - acc: 0.8909 - val_loss: 1.3974 - val_acc: 0.6374\n",
      "Epoch 152/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.2731 - acc: 0.8893 - val_loss: 1.3641 - val_acc: 0.6396\n",
      "Epoch 153/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2714 - acc: 0.8916 - val_loss: 1.3628 - val_acc: 0.6441\n",
      "Epoch 154/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2722 - acc: 0.8912 - val_loss: 1.3800 - val_acc: 0.6314\n",
      "Epoch 155/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.2717 - acc: 0.8900 - val_loss: 1.3784 - val_acc: 0.6404\n",
      "Epoch 156/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.2742 - acc: 0.8885 - val_loss: 1.3687 - val_acc: 0.6374\n",
      "Epoch 157/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2702 - acc: 0.8916 - val_loss: 1.3798 - val_acc: 0.6404\n",
      "Epoch 158/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2744 - acc: 0.8886 - val_loss: 1.3618 - val_acc: 0.6479\n",
      "Epoch 159/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2742 - acc: 0.8889 - val_loss: 1.3855 - val_acc: 0.6359\n",
      "Epoch 160/200\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.2711 - acc: 0.8924 - val_loss: 1.3870 - val_acc: 0.6464\n",
      "Epoch 161/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2729 - acc: 0.8914 - val_loss: 1.3653 - val_acc: 0.6441\n",
      "Epoch 162/200\n",
      "22/22 [==============================] - 1s 56ms/step - loss: 0.2722 - acc: 0.8901 - val_loss: 1.3678 - val_acc: 0.6441\n",
      "Epoch 163/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2723 - acc: 0.8914 - val_loss: 1.3802 - val_acc: 0.6456\n",
      "Epoch 164/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2716 - acc: 0.8892 - val_loss: 1.3693 - val_acc: 0.6456\n",
      "Epoch 165/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2716 - acc: 0.8911 - val_loss: 1.3979 - val_acc: 0.6329\n",
      "Epoch 166/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2702 - acc: 0.8923 - val_loss: 1.3668 - val_acc: 0.6456\n",
      "Epoch 167/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2704 - acc: 0.8941 - val_loss: 1.4004 - val_acc: 0.6464\n",
      "Epoch 168/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2727 - acc: 0.8916 - val_loss: 1.3748 - val_acc: 0.6299\n",
      "Epoch 169/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2749 - acc: 0.8878 - val_loss: 1.4267 - val_acc: 0.6434\n",
      "Epoch 170/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2738 - acc: 0.8889 - val_loss: 1.3968 - val_acc: 0.6419\n",
      "Epoch 171/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2703 - acc: 0.8900 - val_loss: 1.3832 - val_acc: 0.6419\n",
      "Epoch 172/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.2714 - acc: 0.8897 - val_loss: 1.3957 - val_acc: 0.6426\n",
      "Epoch 173/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2686 - acc: 0.8914 - val_loss: 1.3886 - val_acc: 0.6479\n",
      "Epoch 174/200\n",
      "22/22 [==============================] - 1s 54ms/step - loss: 0.2727 - acc: 0.8892 - val_loss: 1.3865 - val_acc: 0.6426\n",
      "Epoch 175/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2677 - acc: 0.8908 - val_loss: 1.3907 - val_acc: 0.6366\n",
      "Epoch 176/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2662 - acc: 0.8940 - val_loss: 1.3877 - val_acc: 0.6351\n",
      "Epoch 177/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2692 - acc: 0.8904 - val_loss: 1.4304 - val_acc: 0.6411\n",
      "Epoch 178/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2775 - acc: 0.8869 - val_loss: 1.4320 - val_acc: 0.6449\n",
      "Epoch 179/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2813 - acc: 0.8860 - val_loss: 1.3967 - val_acc: 0.6426\n",
      "Epoch 180/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2729 - acc: 0.8875 - val_loss: 1.4072 - val_acc: 0.6299\n",
      "Epoch 181/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2719 - acc: 0.8908 - val_loss: 1.3977 - val_acc: 0.6434\n",
      "Epoch 182/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2680 - acc: 0.8928 - val_loss: 1.3919 - val_acc: 0.6306\n",
      "Epoch 183/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2712 - acc: 0.8898 - val_loss: 1.3817 - val_acc: 0.6381\n",
      "Epoch 184/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.2710 - acc: 0.8871 - val_loss: 1.3798 - val_acc: 0.6434\n",
      "Epoch 185/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2719 - acc: 0.8890 - val_loss: 1.4039 - val_acc: 0.6404\n",
      "Epoch 186/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2699 - acc: 0.8911 - val_loss: 1.3995 - val_acc: 0.6374\n",
      "Epoch 187/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2742 - acc: 0.8875 - val_loss: 1.4003 - val_acc: 0.6456\n",
      "Epoch 188/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2727 - acc: 0.8888 - val_loss: 1.4075 - val_acc: 0.6359\n",
      "Epoch 189/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.2714 - acc: 0.8887 - val_loss: 1.4091 - val_acc: 0.6351\n",
      "Epoch 190/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.2822 - acc: 0.8850 - val_loss: 1.4256 - val_acc: 0.6366\n",
      "Epoch 191/200\n",
      "22/22 [==============================] - 1s 54ms/step - loss: 0.2709 - acc: 0.8909 - val_loss: 1.4209 - val_acc: 0.6411\n",
      "Epoch 192/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.2685 - acc: 0.8893 - val_loss: 1.4131 - val_acc: 0.6374\n",
      "Epoch 193/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2677 - acc: 0.8922 - val_loss: 1.3976 - val_acc: 0.6426\n",
      "Epoch 194/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.2699 - acc: 0.8896 - val_loss: 1.3941 - val_acc: 0.6389\n",
      "Epoch 195/200\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.2688 - acc: 0.8891 - val_loss: 1.4098 - val_acc: 0.6434\n",
      "Epoch 196/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2705 - acc: 0.8916 - val_loss: 1.4042 - val_acc: 0.6336\n",
      "Epoch 197/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2684 - acc: 0.8893 - val_loss: 1.4099 - val_acc: 0.6396\n",
      "Epoch 198/200\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2684 - acc: 0.8893 - val_loss: 1.4322 - val_acc: 0.6426\n",
      "Epoch 199/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2731 - acc: 0.8879 - val_loss: 1.4436 - val_acc: 0.6366\n",
      "Epoch 200/200\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.2769 - acc: 0.8870 - val_loss: 1.3928 - val_acc: 0.6359\n",
      "42/42 [==============================] - 0s 7ms/step - loss: 1.2963 - acc: 0.6445\n",
      "loss: 1.2963393926620483 accuracy: 0.6444610953330994\n"
     ]
    }
   ],
   "source": [
    "#Compile model. \n",
    "model32.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"acc\"])\n",
    "\n",
    "history = model32.fit([x_train_review_pad_glove, x_train_aspect_pad_glove], y_train,\n",
    "                    epochs = 200,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = ([x_dev_review_pad_glove, x_dev_aspect_pad_glove], y_dev),\n",
    "                    verbose = 1)\n",
    "results32 = model32.evaluate([x_test_review_pad_glove, x_test_aspect_pad_glove],y_test)\n",
    "\n",
    "print(\"loss:\",results32[0],\"accuracy:\",results32[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_PPzd6R6gt8"
   },
   "source": [
    "#  Model 4: Another LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IC85cIumM9V"
   },
   "source": [
    "If you study the data carefully, you can find that every aspect appears in the review sentence, which means we can extract the aspect information from the sentence. In most cases, the polarity of the aspect is determined by the content near it. Therefore, an LSTM can transfer the information of adjacent context to the aspect. We only need to extract the aspect vector to calculate its polarity, without analyzing the whole sentence.\n",
    "\n",
    "![image.png](data:image/png;base64,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)\n",
    "\n",
    "(This is a bit like a simplified version of the \"unknown target\" sequence tagging models discussed in lectures - but we're only trying to tag the one part of the sequence that we know corresponds to the aspect mention.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1647369307553,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "pcxqucKN66sy",
    "outputId": "021b9a53-41a7-4f00-a517-774dcc09d2c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation error:\n",
      "['i love the food here, and although it is pricey, the entree comes with rice, naan, dal, and salad, which makes it worthwhile.', 'd al', 'neutral', '24', '28']\n",
      "\n",
      "\n",
      "\n",
      "['the decor is not special at all but their food and amazing prices make up for it.', 'decor', 'negative', '4', '9']\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['the decor is not special at all but their food and amazing prices make up for it.', 'food', 'positive', '42', '46']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "['the decor is not special at all but their food and amazing prices make up for it.', 'prices', 'positive', '59', '65']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "['when tables opened up, the manager sat another party before us.', 'tables', 'neutral', '5', '11']\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['the scene there are two distinct personalities to the place: the loud, seemingly always-crowded bar with hanging paper decorations and dim lighting, and the two main dining areas, where the noise level and decor is notably more subdued.', 'noise level', 'negative', '190', '201']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def aspect_mask(reviews, aspects, dataset):\n",
    "  mask = []\n",
    "  for review,aspect,data in zip(reviews, aspects, dataset):\n",
    "    find_aspect = False\n",
    "    for j in range(5):\n",
    "      aspect_num = len(aspect)\n",
    "      aspect_str = \" \".join(aspect)\n",
    "      aspect_len = int(len(aspect_str) - 1) * (j+1)\n",
    "      offset = 0\n",
    "      for i,r in enumerate(review):\n",
    "        if i + aspect_num <= len(review):\n",
    "          r_context = \" \".join(review[i:i+aspect_num])\n",
    "          if r_context == aspect_str and offset + aspect_len >  int(data[3]) and offset + aspect_len <  int(data[4]):\n",
    "            find_aspect = True\n",
    "            sentence_mask = [0] * len(review)\n",
    "            sentence_mask[i:i+aspect_num] = [1] * aspect_num\n",
    "            mask.append(sentence_mask)\n",
    "            break\n",
    "          else:\n",
    "            offset += (len(r) + 1)\n",
    "      if find_aspect:\n",
    "        break\n",
    "\n",
    "    if not find_aspect:\n",
    "      for j in range(5):\n",
    "        aspect_len = int(len(aspect_str) - 1) * (j+1)\n",
    "        offset = 0\n",
    "        for i,r in enumerate(review):\n",
    "          if i + aspect_num <= len(review):\n",
    "            r_context = \" \".join(review[i:i+aspect_num])\n",
    "            if r_context.startswith(aspect_str) and offset + aspect_len >  int(data[3]) and offset + aspect_len <  int(data[4]):\n",
    "              find_aspect = True\n",
    "              sentence_mask = [0] * len(review)\n",
    "              sentence_mask[i:i+aspect_num] = [1] * aspect_num\n",
    "              mask.append(sentence_mask)\n",
    "              break\n",
    "            else:\n",
    "              offset += (len(r) + 1)\n",
    "        if find_aspect:\n",
    "          break\n",
    "\n",
    "    if not find_aspect:\n",
    "      for j in range(5):\n",
    "        aspect_len = int(len(aspect_str) - 1) * (j+1)\n",
    "        offset = 0\n",
    "        for i,r in enumerate(review):\n",
    "          if i + aspect_num <= len(review):\n",
    "            r_context = \" \".join(review[i:i+aspect_num])\n",
    "            if r_context.endswith(aspect_str) and offset + aspect_len >  int(data[3]) and offset + aspect_len <  int(data[4]):\n",
    "              find_aspect = True\n",
    "              sentence_mask = [0] * len(review)\n",
    "              sentence_mask[i:i+aspect_num] = [1] * aspect_num\n",
    "              mask.append(sentence_mask)\n",
    "              break\n",
    "            else:\n",
    "              offset += (len(r) + 1)\n",
    "        if find_aspect:\n",
    "          break\n",
    "\n",
    "    if not find_aspect:\n",
    "      print(\"annotation error:\")\n",
    "      print(data)\n",
    "      sentence_mask = [0] * len(review)\n",
    "      sentence_mask[16] = 1\n",
    "      mask.append(sentence_mask)\n",
    "\n",
    "    # if aspect_num > 1:\n",
    "    #   print(mask[-1])\n",
    "\n",
    "  return mask\n",
    "x_train_aspect_mask = aspect_mask(x_train_review, x_train_aspect, train)\n",
    "x_dev_aspect_mask = aspect_mask(x_dev_review, x_dev_aspect, val)\n",
    "x_test_aspect_mask = aspect_mask(x_test_review, x_test_aspect, test)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "assert len(x_train_aspect_mask) == len(train)\n",
    "assert len(x_test_aspect_mask) == len(x_test_aspect)\n",
    "\n",
    "print(train[0])\n",
    "print(x_train_aspect_mask[0])\n",
    "print(train[1])\n",
    "print(x_train_aspect_mask[1])\n",
    "print(train[2])\n",
    "print(x_train_aspect_mask[2])\n",
    "print(train[3])\n",
    "print(x_train_aspect_mask[3])\n",
    "print(train[10319])\n",
    "print(x_train_aspect_mask[10319])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1647369307553,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "PJK16El2yBQ0",
    "outputId": "90153393-7114-4b8d-f61c-31347c7c1cf7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_aspect_mask_pad = keras.preprocessing.sequence.pad_sequences(x_train_aspect_mask,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=128)\n",
    "x_dev_aspect_mask_pad = keras.preprocessing.sequence.pad_sequences(x_dev_aspect_mask,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=128)\n",
    "x_test_aspect_mask_pad = keras.preprocessing.sequence.pad_sequences(x_test_aspect_mask,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=128)\n",
    "x_train_aspect_mask_pad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 988,
     "status": "ok",
     "timestamp": 1647369308540,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "ahUWMflW65jM",
    "outputId": "eae4311e-4420-4660-f665-9ded96337013"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " GloVe_Embeddings (Embedding)   multiple             120000300   ['input_9[0][0]']                \n",
      "                                                                                                  \n",
      " BiLSTM (Bidirectional)         (None, 128, 200)     320800      ['GloVe_Embeddings[6][0]']       \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 200)          0           ['BiLSTM[0][0]',                 \n",
      "                                                                  'input_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 16)           3216        ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 3)            51          ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 120,324,367\n",
      "Trainable params: 324,067\n",
      "Non-trainable params: 120,000,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dot\n",
    "\n",
    "# your code goes here\n",
    "# Tips: \n",
    "# 1) LSTM layer: Bidirectional(LSTM(100, return_sequences=True),name = 'BiLSTM')\n",
    "# 2) Dot  layer: tf.keras.layers.Dot(axes=1)\n",
    "# 3) Document of the Dot layer: https://keras.io/api/layers/merging_layers/dot/\n",
    "# 4) dtype of the aspect input layer is float32\n",
    "\n",
    "input3 = Input((128,))\n",
    "embed1 = embeddingLayer(input3)\n",
    "BiLSTM = Bidirectional(LSTM(100, return_sequences=True),name = 'BiLSTM')(embed1)\n",
    "input4 = Input((128,), dtype='float32')\n",
    "dot = Dot(axes=1)([BiLSTM, input4])\n",
    "dense2 = Dense(16)(dot)\n",
    "Output = Dense(3, activation='softmax')(dense2)\n",
    "\n",
    "model4 = Model(inputs=[input3, input4],outputs=[Output])\n",
    "\n",
    "model4.summary()\n",
    "\n",
    "model4.compile(optimizer='adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1647369308842,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "f90SrYb4YHj3",
    "outputId": "4ed93ffa-9f6e-4aeb-9381-3f9c671c943c"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"392pt\" viewBox=\"0.00 0.00 786.00 470.00\" width=\"655pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(.8333 .8333) rotate(0) translate(4 466)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-466 782,-466 782,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140320479251536 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140320479251536</title>\n",
       "<polygon fill=\"none\" points=\"50,-415.5 50,-461.5 380,-461.5 380,-415.5 50,-415.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"90\" y=\"-446.3\">input_9</text>\n",
       "<polyline fill=\"none\" points=\"50,-438.5 130,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"90\" y=\"-423.3\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"130,-415.5 130,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"130,-438.5 188,-438.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"159\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"188,-415.5 188,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"236\" y=\"-434.8\">[(None, 128)]</text>\n",
       "<polyline fill=\"none\" points=\"284,-415.5 284,-461.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"332\" y=\"-434.8\">[(None, 128)]</text>\n",
       "</g>\n",
       "<!-- 140316056250640 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140316056250640</title>\n",
       "<polygon fill=\"none\" points=\"96,-332.5 96,-378.5 334,-378.5 334,-332.5 96,-332.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-363.3\">GloVe_Embeddings</text>\n",
       "<polyline fill=\"none\" points=\"96,-355.5 230,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163\" y=\"-340.3\">Embedding</text>\n",
       "<polyline fill=\"none\" points=\"230,-332.5 230,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"259\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"230,-355.5 288,-355.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"259\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"288,-332.5 288,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"299.5\" y=\"-351.8\">?</text>\n",
       "<polyline fill=\"none\" points=\"311,-332.5 311,-378.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.5\" y=\"-351.8\">?</text>\n",
       "</g>\n",
       "<!-- 140320479251536&#45;&gt;140316056250640 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140320479251536-&gt;140316056250640</title>\n",
       "<path d=\"M215,-415.3799C215,-407.1745 215,-397.7679 215,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"218.5001,-388.784 215,-378.784 211.5001,-388.784 218.5001,-388.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140320479399312 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140320479399312</title>\n",
       "<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 430,-295.5 430,-249.5 0,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"69\" y=\"-280.3\">BiLSTM(lstm)</text>\n",
       "<polyline fill=\"none\" points=\"0,-272.5 138,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"69\" y=\"-257.3\">Bidirectional(LSTM)</text>\n",
       "<polyline fill=\"none\" points=\"138,-249.5 138,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"138,-272.5 196,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"167\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"196,-249.5 196,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"254.5\" y=\"-268.8\">(None, 128, 300)</text>\n",
       "<polyline fill=\"none\" points=\"313,-249.5 313,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"371.5\" y=\"-268.8\">(None, 128, 200)</text>\n",
       "</g>\n",
       "<!-- 140316056250640&#45;&gt;140320479399312 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140316056250640-&gt;140320479399312</title>\n",
       "<path d=\"M215,-332.3799C215,-324.1745 215,-314.7679 215,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"218.5001,-305.784 215,-295.784 211.5001,-305.784 218.5001,-305.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140320479647568 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140320479647568</title>\n",
       "<polygon fill=\"none\" points=\"220.5,-166.5 220.5,-212.5 607.5,-212.5 607.5,-166.5 220.5,-166.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.5\" y=\"-197.3\">dot</text>\n",
       "<polyline fill=\"none\" points=\"220.5,-189.5 258.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.5\" y=\"-174.3\">Dot</text>\n",
       "<polyline fill=\"none\" points=\"258.5,-166.5 258.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"287.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"258.5,-189.5 316.5,-189.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"287.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"316.5,-166.5 316.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"418.5\" y=\"-185.8\">[(None, 128, 200), (None, 128)]</text>\n",
       "<polyline fill=\"none\" points=\"520.5,-166.5 520.5,-212.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"564\" y=\"-185.8\">(None, 200)</text>\n",
       "</g>\n",
       "<!-- 140320479399312&#45;&gt;140320479647568 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140320479399312-&gt;140320479647568</title>\n",
       "<path d=\"M270.1683,-249.4901C294.755,-239.2353 323.8812,-227.0872 349.4713,-216.414\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"350.8423,-219.6345 358.7244,-212.5547 348.1476,-213.1739 350.8423,-219.6345\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140320479250064 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140320479250064</title>\n",
       "<polygon fill=\"none\" points=\"448,-249.5 448,-295.5 778,-295.5 778,-249.5 448,-249.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"488\" y=\"-280.3\">input_10</text>\n",
       "<polyline fill=\"none\" points=\"448,-272.5 528,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"488\" y=\"-257.3\">InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"528,-249.5 528,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"557\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"528,-272.5 586,-272.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"557\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"586,-249.5 586,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"634\" y=\"-268.8\">[(None, 128)]</text>\n",
       "<polyline fill=\"none\" points=\"682,-249.5 682,-295.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"730\" y=\"-268.8\">[(None, 128)]</text>\n",
       "</g>\n",
       "<!-- 140320479250064&#45;&gt;140320479647568 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140320479250064-&gt;140320479647568</title>\n",
       "<path d=\"M557.8317,-249.4901C533.245,-239.2353 504.1188,-227.0872 478.5287,-216.414\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"479.8524,-213.1739 469.2756,-212.5547 477.1577,-219.6345 479.8524,-213.1739\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140314059726352 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140314059726352</title>\n",
       "<polygon fill=\"none\" points=\"266,-83.5 266,-129.5 562,-129.5 562,-83.5 266,-83.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"301.5\" y=\"-114.3\">dense_10</text>\n",
       "<polyline fill=\"none\" points=\"266,-106.5 337,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"301.5\" y=\"-91.3\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"337,-83.5 337,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"337,-106.5 395,-106.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"395,-83.5 395,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"438.5\" y=\"-102.8\">(None, 200)</text>\n",
       "<polyline fill=\"none\" points=\"482,-83.5 482,-129.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"522\" y=\"-102.8\">(None, 16)</text>\n",
       "</g>\n",
       "<!-- 140320479647568&#45;&gt;140314059726352 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140320479647568-&gt;140314059726352</title>\n",
       "<path d=\"M414,-166.3799C414,-158.1745 414,-148.7679 414,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"417.5001,-139.784 414,-129.784 410.5001,-139.784 417.5001,-139.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140314060030160 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140314060030160</title>\n",
       "<polygon fill=\"none\" points=\"273.5,-.5 273.5,-46.5 554.5,-46.5 554.5,-.5 273.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"309\" y=\"-31.3\">dense_11</text>\n",
       "<polyline fill=\"none\" points=\"273.5,-23.5 344.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"309\" y=\"-8.3\">Dense</text>\n",
       "<polyline fill=\"none\" points=\"344.5,-.5 344.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"344.5,-23.5 402.5,-23.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"373.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"402.5,-.5 402.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"442.5\" y=\"-19.8\">(None, 16)</text>\n",
       "<polyline fill=\"none\" points=\"482.5,-.5 482.5,-46.5 \" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"518.5\" y=\"-19.8\">(None, 3)</text>\n",
       "</g>\n",
       "<!-- 140314059726352&#45;&gt;140314060030160 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140314059726352-&gt;140314060030160</title>\n",
       "<path d=\"M414,-83.3799C414,-75.1745 414,-65.7679 414,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"417.5001,-56.784 414,-46.784 410.5001,-56.784 417.5001,-56.784\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils import vis_utils\n",
    "SVG(vis_utils.model_to_dot(model4, show_shapes=True, show_layer_names=True, dpi=60).create(prog='dot', format='svg'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59191,
     "status": "ok",
     "timestamp": 1647369368029,
     "user": {
      "displayName": "Animesh Chourey",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16521180785802099913"
     },
     "user_tz": 0
    },
    "id": "MF_x9s6mYbkv",
    "outputId": "9aa1dd77-98df-4158-fa48-0357578d41df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "22/22 [==============================] - 9s 197ms/step - loss: 0.9254 - acc: 0.5757 - val_loss: 0.8176 - val_acc: 0.6539\n",
      "Epoch 2/16\n",
      "22/22 [==============================] - 3s 150ms/step - loss: 0.7730 - acc: 0.6699 - val_loss: 0.7584 - val_acc: 0.6652\n",
      "Epoch 3/16\n",
      "22/22 [==============================] - 3s 150ms/step - loss: 0.6977 - acc: 0.7090 - val_loss: 0.7099 - val_acc: 0.7125\n",
      "Epoch 4/16\n",
      "22/22 [==============================] - 3s 151ms/step - loss: 0.6378 - acc: 0.7306 - val_loss: 0.6964 - val_acc: 0.7230\n",
      "Epoch 5/16\n",
      "22/22 [==============================] - 3s 151ms/step - loss: 0.5874 - acc: 0.7606 - val_loss: 0.6851 - val_acc: 0.7245\n",
      "Epoch 6/16\n",
      "22/22 [==============================] - 3s 151ms/step - loss: 0.5424 - acc: 0.7779 - val_loss: 0.6745 - val_acc: 0.7275\n",
      "Epoch 7/16\n",
      "22/22 [==============================] - 3s 149ms/step - loss: 0.4968 - acc: 0.8031 - val_loss: 0.6928 - val_acc: 0.7312\n",
      "Epoch 8/16\n",
      "22/22 [==============================] - 3s 150ms/step - loss: 0.4549 - acc: 0.8211 - val_loss: 0.7120 - val_acc: 0.7305\n",
      "Epoch 9/16\n",
      "22/22 [==============================] - 3s 151ms/step - loss: 0.4148 - acc: 0.8359 - val_loss: 0.7475 - val_acc: 0.7260\n",
      "Epoch 10/16\n",
      "22/22 [==============================] - 3s 150ms/step - loss: 0.3692 - acc: 0.8598 - val_loss: 0.7813 - val_acc: 0.7342\n",
      "Epoch 11/16\n",
      "22/22 [==============================] - 3s 150ms/step - loss: 0.3326 - acc: 0.8733 - val_loss: 0.8182 - val_acc: 0.7290\n",
      "Epoch 12/16\n",
      "22/22 [==============================] - 3s 151ms/step - loss: 0.2929 - acc: 0.8910 - val_loss: 0.8337 - val_acc: 0.7230\n",
      "Epoch 13/16\n",
      "22/22 [==============================] - 3s 150ms/step - loss: 0.2444 - acc: 0.9116 - val_loss: 0.9261 - val_acc: 0.7185\n",
      "Epoch 14/16\n",
      "22/22 [==============================] - 3s 151ms/step - loss: 0.2139 - acc: 0.9262 - val_loss: 1.0343 - val_acc: 0.7095\n",
      "Epoch 15/16\n",
      "22/22 [==============================] - 3s 152ms/step - loss: 0.1861 - acc: 0.9369 - val_loss: 1.0800 - val_acc: 0.7140\n",
      "Epoch 16/16\n",
      "22/22 [==============================] - 3s 150ms/step - loss: 0.1423 - acc: 0.9556 - val_loss: 1.1356 - val_acc: 0.7170\n",
      "42/42 [==============================] - 1s 18ms/step - loss: 1.1583 - acc: 0.7111\n",
      "[1.1583406925201416, 0.711077868938446]\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "model4.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"acc\"])\n",
    "\n",
    "history = model4.fit([x_train_review_pad_glove, x_train_aspect_mask_pad], y_train,\n",
    "                    epochs = 16,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = ([x_dev_review_pad_glove, x_dev_aspect_mask_pad], y_dev),\n",
    "                    verbose = 1)\n",
    "\n",
    "results = model4.evaluate([x_test_review_pad_glove,x_test_aspect_mask_pad], y_test)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "7001_2021_22_lab4_Aspect_Based_Sentiment_Analysis_without_answers_(1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
